{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Responses with Pydantic and Ollama\n",
    "\n",
    "*Using IBM Granite Models with Local Ollama*\n",
    "\n",
    "**Author:** Vipul Mahajan  \n",
    "**Email:** vipmaha1@in.ibm.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This recipe demonstrates how to generate reliable, structured responses from IBM Granite models using Pydantic for schema definition and validation, with **local Ollama** as the LLM provider. Unlike free-form text generation, structured responses ensure consistent, machine-readable outputs that integrate seamlessly with software systems.\n",
    "\n",
    "## üéØ What Makes This Recipe Unique\n",
    "\n",
    "This notebook complements the [Entity Extraction recipe](../Entity-Extraction/entity_extraction.ipynb) by focusing on **local, offline deployment** with Ollama instead of cloud APIs (Replicate/Watsonx). It's designed for users who need:\n",
    "\n",
    "- **Privacy & Security**: Keep data local, no cloud API calls\n",
    "- **Offline Capability**: Work without internet connectivity\n",
    "- **Cost Control**: Eliminate per-request API costs\n",
    "- **Production Patterns**: Advanced error handling, debugging, and monitoring\n",
    "\n",
    "While the Entity Extraction recipe is excellent for quick cloud-based extraction, this notebook targets **production-ready, local inference systems** with enterprise requirements.\n",
    "\n",
    "## What This Notebook Covers:\n",
    "\n",
    "### üîß **Core Implementation**\n",
    "- **Two Approaches**: Both LangChain's `with_structured_output` (clean, simple) AND manual parsing (fine-grained control)\n",
    "- **Pydantic Schema Design**: Define structured data models with validation rules\n",
    "- **Ollama Integration**: Connect to local Granite models for private, offline processing\n",
    "- **Error Handling**: Robust parsing with multiple strategies and automatic retries\n",
    "- **Type Safety**: Full TypeScript-like validation for Python data structures\n",
    "\n",
    "### üìä **Practical Examples**\n",
    "- **Product Review Analysis**: Extract ratings, sentiment, pros/cons from customer reviews\n",
    "- **Research Paper Parsing**: Complex nested extraction from academic papers with author details\n",
    "- **Advanced Validation**: Custom validators, computed fields, and consistency checks\n",
    "- **Error Scenario Testing**: Handle ambiguous, contradictory, and malformed inputs\n",
    "\n",
    "### üß™ **Testing & Validation**\n",
    "- **Reliability Testing**: Multi-run consistency analysis with performance metrics\n",
    "- **Error Handling Demo**: Graceful failure scenarios and recovery mechanisms  \n",
    "- **Performance Comparison**: Pydantic vs unstructured approaches with detailed analysis\n",
    "- **Debug Utilities**: Step-by-step troubleshooting tools for production use\n",
    "\n",
    "### üìö **Production Guidance**\n",
    "- **Best Practices**: Schema design, prompt engineering, and optimization strategies\n",
    "- **Troubleshooting Guide**: Common issues and systematic resolution approaches\n",
    "- **Method Comparison**: When to use `with_structured_output` vs manual parsing\n",
    "\n",
    "## Key Benefits of This Approach:\n",
    "\n",
    "1. **Type Safety**: Pydantic provides compile-time type checking and runtime validation\n",
    "2. **Error Handling**: Robust parsing with automatic retries for malformed responses\n",
    "3. **Reliability**: Consistent output format across multiple model runs\n",
    "4. **Local Control**: Using Ollama eliminates API dependencies and ensures data privacy\n",
    "5. **Validation**: Built-in field validation, constraints, and custom validators\n",
    "6. **Flexibility**: Two methods (LangChain native vs custom) for different use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This recipe requires:\n",
    "1. [Ollama](https://ollama.ai/) installed locally\n",
    "2. IBM Granite model pulled in Ollama: `ollama pull ibm/granite4:3b` (or use `granite4:latest`)\n",
    "3. Python packages: `pydantic`, `langchain-ollama`, `transformers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo \"::group::Install Dependencies\"\n",
    "%pip install uv\n",
    "! uv pip install \"git+https://github.com/ibm-granite-community/utils.git\" \\\n",
    "    pydantic \\\n",
    "    langchain-ollama \\\n",
    "    transformers \\\n",
    "    json5 \\\n",
    "    tenacity \\\n",
    "    jinja2\n",
    "! echo \"::endgroup::\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import json5\n",
    "import time\n",
    "from typing import List, Optional, Dict, Any, Type, TypeVar\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from langchain_ollama import OllamaLLM, ChatOllama\n",
    "from transformers import AutoTokenizer\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from ibm_granite_community.notebook_utils import wrap_text\n",
    "\n",
    "# Setup Granite tokenizer for proper prompt formatting (used in manual approach)\n",
    "model_path = \"ibm-granite/granite-3.3-8b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "ollama_model = \"ibm/granite4:latest\"\n",
    "\n",
    "# Approach 1: OllamaLLM for manual structured response (fine-grained control)\n",
    "llm_manual = OllamaLLM(\n",
    "    model=ollama_model,\n",
    "    temperature=0.1,  # Low temperature for more consistent outputs\n",
    "    num_predict=1024,  # Max tokens to generate\n",
    "    top_p=0.9,\n",
    "    top_k=40\n",
    ")\n",
    "\n",
    "# Approach 2: ChatOllama for with_structured_output (LangChain native)\n",
    "llm_chat = ChatOllama(\n",
    "    model=ollama_model,\n",
    "    temperature=0.1,\n",
    "    num_predict=1024\n",
    ")\n",
    "\n",
    "print(f\"‚úì Tokenizer loaded: {model_path}\")\n",
    "print(f\"‚úì Ollama model configured: {ollama_model}\")\n",
    "print(f\"‚úì Two LLM instances ready:\")\n",
    "print(f\"  - OllamaLLM (manual approach with custom parsing)\")\n",
    "print(f\"  - ChatOllama (with_structured_output method)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Pydantic Schema Definition\n",
    "\n",
    "Let's start with simple schema definitions to understand how Pydantic models work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, field_validator\n",
    "from typing import List, Optional\n",
    "from enum import Enum\n",
    "\n",
    "class Sentiment(str, Enum):\n",
    "    POSITIVE = \"positive\"\n",
    "    NEGATIVE = \"negative\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "\n",
    "class ProductReview(BaseModel):\n",
    "    \"\"\"Schema for extracting structured information from product reviews.\"\"\"\n",
    "    \n",
    "    product_name: str = Field(description=\"Name of the product being reviewed\")\n",
    "    rating: float = Field(ge=1, le=5, description=\"Rating from 1 to 5 stars (can include decimals like 4.5)\")\n",
    "    sentiment: Sentiment = Field(description=\"Overall sentiment of the review\")\n",
    "    pros: List[str] = Field(description=\"Positive aspects mentioned in the review\")\n",
    "    cons: List[str] = Field(description=\"Negative aspects or complaints\")\n",
    "    would_recommend: bool = Field(description=\"Whether the reviewer recommends the product\")\n",
    "    review_summary: str = Field(description=\"Brief summary of the review in 1-2 sentences\")\n",
    "    \n",
    "    @field_validator('pros', 'cons')\n",
    "    @classmethod\n",
    "    def validate_lists_not_empty_strings(cls, v):\n",
    "        \"\"\"Ensure list items are not empty strings\"\"\"\n",
    "        return [item.strip() for item in v if item.strip()]\n",
    "\n",
    "# Demonstrate schema generation\n",
    "print(\"Generated JSON Schema:\")\n",
    "print(json.dumps(ProductReview.model_json_schema(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Structured Response Function\n",
    "\n",
    "This function handles the complete workflow: prompt formatting, model invocation, parsing, and validation with error handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = TypeVar('T', bound=BaseModel)\n",
    "\n",
    "def create_structured_prompt(user_message: str, schema_model: Type[BaseModel], system_prompt: Optional[str] = None) -> str:\n",
    "    \"\"\"Create a properly formatted prompt for structured response generation.\"\"\"\n",
    "    \n",
    "    if system_prompt is None:\n",
    "        system_prompt = f\"\"\"You are a precise data extraction assistant. Your task is to analyze the given text and extract information according to the specified schema.\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "- Respond ONLY with valid JSON that matches the schema exactly\n",
    "- Do not include any explanations, comments, or additional text\n",
    "- If information is not available, use null for optional fields or reasonable defaults\n",
    "- Ensure all required fields are present\n",
    "- Follow the data types specified in the schema strictly\"\"\"\n",
    "    \n",
    "    schema_json = json.dumps(schema_model.model_json_schema(), indent=2)\n",
    "    \n",
    "    full_prompt = f\"\"\"System: {system_prompt}\n",
    "\n",
    "Schema to follow:\n",
    "{schema_json}\n",
    "\n",
    "Text to analyze:\n",
    "{user_message}\n",
    "\n",
    "Respond with valid JSON only:\"\"\"\n",
    "    \n",
    "    # Format using Granite chat template\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Extract structured data from this text according to the schema:\\n\\n{schema_json}\\n\\nText: {user_message}\"}\n",
    "    ]\n",
    "    \n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        conversation,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False\n",
    "    )\n",
    "    \n",
    "    return formatted_prompt\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "def get_structured_response(user_message: str, schema_model: Type[T], system_prompt: Optional[str] = None) -> T:\n",
    "    \"\"\"Get a structured response with automatic retry on parsing failures (Manual Approach).\"\"\"\n",
    "    \n",
    "    prompt = create_structured_prompt(user_message, schema_model, system_prompt)\n",
    "    \n",
    "    # Get response from Ollama using manual LLM\n",
    "    response = llm_manual.invoke(prompt)\n",
    "    \n",
    "    # Try multiple parsing strategies\n",
    "    parsing_errors = []\n",
    "    \n",
    "    # Strategy 1: Direct JSON parsing\n",
    "    try:\n",
    "        parsed_json = json.loads(response.strip())\n",
    "        return schema_model.model_validate(parsed_json)\n",
    "    except (json.JSONDecodeError, ValidationError) as e:\n",
    "        parsing_errors.append(f\"Direct JSON parsing: {str(e)}\")\n",
    "    \n",
    "    # Strategy 2: Extract JSON from response (handle extra text)\n",
    "    try:\n",
    "        # Find JSON object in response\n",
    "        start_idx = response.find('{')\n",
    "        end_idx = response.rfind('}')\n",
    "        if start_idx != -1 and end_idx != -1:\n",
    "            json_str = response[start_idx:end_idx+1]\n",
    "            parsed_json = json.loads(json_str)\n",
    "            return schema_model.model_validate(parsed_json)\n",
    "    except (json.JSONDecodeError, ValidationError) as e:\n",
    "        parsing_errors.append(f\"Extracted JSON parsing: {str(e)}\")\n",
    "    \n",
    "    # Strategy 3: Use json5 for more lenient parsing\n",
    "    try:\n",
    "        parsed_json = json5.loads(response.strip())\n",
    "        return schema_model.model_validate(parsed_json)\n",
    "    except (Exception) as e:\n",
    "        parsing_errors.append(f\"JSON5 parsing: {str(e)}\")\n",
    "    \n",
    "    # If all strategies fail, raise detailed error\n",
    "    error_msg = f\"Failed to parse response after multiple attempts.\\nResponse: {response[:200]}...\\nErrors: {'; '.join(parsing_errors)}\"\n",
    "    raise ValueError(error_msg)\n",
    "\n",
    "print(\"‚úì Manual structured response functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug and Troubleshooting Utilities (OPTIONAL)\n",
    "\n",
    "For debugging issues with structured responses, these utility functions provide step-by-step error tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_structured_response(user_message: str, schema_model: Type[T], system_prompt: Optional[str] = None, show_prompt: bool = False) -> T:\n",
    "    \"\"\"\n",
    "    Debug version of get_structured_response with detailed step-by-step logging.\n",
    "    \n",
    "    Args:\n",
    "        user_message: Text to extract structured data from\n",
    "        schema_model: Pydantic model class to validate against\n",
    "        system_prompt: Optional custom system prompt\n",
    "        show_prompt: Whether to display the full prompt sent to model\n",
    "    \n",
    "    Returns:\n",
    "        Validated Pydantic model instance\n",
    "        \n",
    "    Example usage:\n",
    "        debug_result = debug_structured_response(\"Great product! 5 stars!\", ProductReview)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üêõ DEBUG MODE - Step-by-step execution:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Create prompt\n",
    "        print(\"1Ô∏è‚É£ Creating structured prompt...\")\n",
    "        prompt = create_structured_prompt(user_message, schema_model, system_prompt)\n",
    "        print(\"   ‚úÖ Prompt created successfully\")\n",
    "        \n",
    "        if show_prompt:\n",
    "            print(f\"   üìù Full prompt preview:\\n{prompt[:500]}...\")\n",
    "        \n",
    "        # Step 2: Get response from model\n",
    "        print(\"\\n2Ô∏è‚É£ Calling Ollama model...\")\n",
    "        response = llm.invoke(prompt)\n",
    "        print(f\"   ‚úÖ Got response ({len(response)} characters)\")\n",
    "        print(f\"   üìÑ Response preview: {response[:150]}...\")\n",
    "        \n",
    "        # Step 3: Parse JSON\n",
    "        print(\"\\n3Ô∏è‚É£ Parsing JSON response...\")\n",
    "        try:\n",
    "            parsed_json = json.loads(response.strip())\n",
    "            print(\"   ‚úÖ JSON parsing successful\")\n",
    "            print(f\"   üìã Parsed fields: {list(parsed_json.keys())}\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Direct JSON parsing failed: {e}\")\n",
    "            print(\"   üîÑ Trying JSON extraction...\")\n",
    "            \n",
    "            start_idx = response.find('{')\n",
    "            end_idx = response.rfind('}')\n",
    "            if start_idx != -1 and end_idx != -1:\n",
    "                json_str = response[start_idx:end_idx+1]\n",
    "                parsed_json = json.loads(json_str)\n",
    "                print(\"   ‚úÖ JSON extraction successful\")\n",
    "            else:\n",
    "                raise ValueError(\"Could not find JSON object in response\")\n",
    "        \n",
    "        # Step 4: Validate with Pydantic\n",
    "        print(\"\\n4Ô∏è‚É£ Validating with Pydantic schema...\")\n",
    "        result = schema_model.model_validate(parsed_json)\n",
    "        print(\"   ‚úÖ Pydantic validation successful\")\n",
    "        print(f\"   üéØ Model type: {type(result).__name__}\")\n",
    "        \n",
    "        print(f\"\\nüéâ DEBUG SUCCESSFUL! Extracted {len(parsed_json)} fields\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå DEBUG FAILED at step above\")\n",
    "        print(f\"   Error type: {type(e).__name__}\")\n",
    "        print(f\"   Error message: {str(e)}\")\n",
    "        \n",
    "        if \"ValidationError\" in str(type(e)):\n",
    "            print(f\"   üí° Pydantic validation details:\")\n",
    "            print(f\"      {str(e)}\")\n",
    "        \n",
    "        print(f\"\\nüîç Raw response for manual inspection:\")\n",
    "        if 'response' in locals():\n",
    "            print(f\"'{response}'\")\n",
    "        \n",
    "        import traceback\n",
    "        print(f\"\\nüìç Full traceback:\")\n",
    "        traceback.print_exc()\n",
    "        raise e\n",
    "\n",
    "def quick_test_model_connection():\n",
    "    \"\"\"Quick test to verify Ollama model is responding correctly.\"\"\"\n",
    "    \n",
    "    print(\"üîå Testing Ollama Model Connection...\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        test_response = llm.invoke(\"Respond with just the word 'SUCCESS' and nothing else.\")\n",
    "        print(f\"‚úÖ Model Response: '{test_response.strip()}'\")\n",
    "        \n",
    "        if \"SUCCESS\" in test_response.upper():\n",
    "            print(\"üéâ Model connection is working perfectly!\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Model responded but didn't follow instructions exactly\")\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Model connection failed: {e}\")\n",
    "        print(\"üí° Troubleshooting tips:\")\n",
    "        print(\"   - Check if Ollama server is running: `ollama serve`\")\n",
    "        print(\"   - Verify model is available: `ollama list`\")\n",
    "        print(f\"   - Try pulling the model: `ollama pull {ollama_model}`\")\n",
    "        return False\n",
    "\n",
    "print(\"üõ†Ô∏è Debug utilities loaded:\")\n",
    "print(\"   ‚Ä¢ debug_structured_response() - Step-by-step debugging\")\n",
    "print(\"   ‚Ä¢ quick_test_model_connection() - Test Ollama connectivity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Product Review Analysis\n",
    "\n",
    "Let's test our structured response system with product review analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample product review text\n",
    "review_text = \"\"\"\n",
    "I recently purchased the Sony WH-1000XM4 wireless headphones and I'm absolutely thrilled with them! \n",
    "The noise cancellation is phenomenal - I can completely block out airplane noise during flights. \n",
    "The sound quality is crisp and clear with excellent bass response. Battery life easily lasts 25+ hours. \n",
    "The touch controls are intuitive and responsive.\n",
    "\n",
    "However, there are a few minor downsides. They can get a bit uncomfortable during very long listening \n",
    "sessions (4+ hours), and the carrying case is quite bulky. The price point is also pretty steep at $350.\n",
    "\n",
    "Despite these small issues, I would definitely recommend these headphones to anyone looking for premium \n",
    "noise-cancelling headphones. The audio quality and ANC technology make them worth the investment.\n",
    "Rating: 4.5/5 stars\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Extract structured data from the review\n",
    "    review_data = get_structured_response(review_text, ProductReview)\n",
    "    \n",
    "    print(\"‚úì Successfully extracted structured data!\\n\")\n",
    "    print(\"Product Review Analysis:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Product: {review_data.product_name}\")\n",
    "    print(f\"Rating: {review_data.rating}/5 stars\")\n",
    "    print(f\"Sentiment: {review_data.sentiment.value}\")\n",
    "    print(f\"Would Recommend: {'Yes' if review_data.would_recommend else 'No'}\")\n",
    "    print(f\"\\nPositives: {', '.join(review_data.pros)}\")\n",
    "    print(f\"\\nNegatives: {', '.join(review_data.cons)}\")\n",
    "    print(f\"\\nSummary: {review_data.review_summary}\")\n",
    "    \n",
    "    # Show the raw JSON for verification\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"Raw JSON Output:\")\n",
    "    print(json.dumps(review_data.model_dump(), indent=2))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error processing review: {str(e)}\")\n",
    "    print(\"\\nüí° Tip: If you encounter errors, try the debug function below:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a structured LLM using with_structured_output\n",
    "structured_llm = llm_chat.with_structured_output(ProductReview, method=\"json_schema\")\n",
    "\n",
    "# Use the same review text\n",
    "try:\n",
    "    # Direct invocation - much simpler than manual approach!\n",
    "    review_data_langchain = structured_llm.invoke(review_text)\n",
    "    \n",
    "    print(\"‚úì Successfully extracted structured data using with_structured_output!\\n\")\n",
    "    print(\"Product Review Analysis (LangChain Method):\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Product: {review_data_langchain.product_name}\")\n",
    "    print(f\"Rating: {review_data_langchain.rating}/5 stars\")\n",
    "    print(f\"Sentiment: {review_data_langchain.sentiment.value}\")\n",
    "    print(f\"Would Recommend: {'Yes' if review_data_langchain.would_recommend else 'No'}\")\n",
    "    print(f\"\\nPositives: {', '.join(review_data_langchain.pros)}\")\n",
    "    print(f\"\\nNegatives: {', '.join(review_data_langchain.cons)}\")\n",
    "    print(f\"\\nSummary: {review_data_langchain.review_summary}\")\n",
    "    \n",
    "    # Show the raw JSON for verification\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"Raw JSON Output:\")\n",
    "    print(json.dumps(review_data_langchain.model_dump(), indent=2))\n",
    "    \n",
    "    print(\"\\nüí° Notice: This approach is much cleaner - no manual JSON parsing needed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error processing review: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1b: Product Review with LangChain's with_structured_output\n",
    "\n",
    "Now let's see the same task using LangChain's native `with_structured_output` method, which provides a cleaner interface with automatic JSON parsing and validation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Manual vs with_structured_output\n",
    "\n",
    "Let's compare the two approaches we've demonstrated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä APPROACH COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "comparison = {\n",
    "    \"Feature\": [\"Code Complexity\", \"LangChain Integration\", \"Error Handling\", \"Prompt Control\", \n",
    "                \"Retry Logic\", \"JSON Parsing\", \"Chat Template Support\", \"Best For\"],\n",
    "    \"Manual Approach\": [\n",
    "        \"More code (~50 lines)\",\n",
    "        \"Uses OllamaLLM\",\n",
    "        \"Custom multi-strategy parsing\",\n",
    "        \"Full control with tokenizer\",\n",
    "        \"Custom @retry decorator\",\n",
    "        \"3 fallback strategies (json, extract, json5)\",\n",
    "        \"‚úÖ Granite chat templates\",\n",
    "        \"Custom workflows, fine-grained control\"\n",
    "    ],\n",
    "    \"with_structured_output\": [\n",
    "        \"Minimal code (~3 lines)\",\n",
    "        \"Uses ChatOllama\",\n",
    "        \"Automatic by LangChain\",\n",
    "        \"Standard LangChain prompting\",\n",
    "        \"Built-in by LangChain\",\n",
    "        \"Automatic JSON parsing\",\n",
    "        \"‚ùå Standard chat format\",\n",
    "        \"Quick development, LangChain pipelines\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Print comparison table\n",
    "print(f\"{'Feature':<30} {'Manual Approach':<35} {'with_structured_output':<35}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for i in range(len(comparison[\"Feature\"])):\n",
    "    feature = comparison[\"Feature\"][i]\n",
    "    manual = comparison[\"Manual Approach\"][i]\n",
    "    langchain = comparison[\"with_structured_output\"][i]\n",
    "    print(f\"{feature:<30} {manual:<35} {langchain:<35}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"üéØ RECOMMENDATIONS:\")\n",
    "print()\n",
    "print(\"Use Manual Approach when:\")\n",
    "print(\"  ‚Ä¢ You need fine-grained control over prompt formatting\")\n",
    "print(\"  ‚Ä¢ You want to use Granite-specific chat templates\")\n",
    "print(\"  ‚Ä¢ You need custom retry or parsing strategies\")\n",
    "print(\"  ‚Ä¢ You're building complex production workflows\")\n",
    "print()\n",
    "print(\"Use with_structured_output when:\")\n",
    "print(\"  ‚Ä¢ You want cleaner, more maintainable code\")\n",
    "print(\"  ‚Ä¢ You're building standard LangChain pipelines\")\n",
    "print(\"  ‚Ä¢ You prefer LangChain's built-in error handling\")\n",
    "print(\"  ‚Ä¢ You're prototyping or need rapid development\")\n",
    "print()\n",
    "print(\"‚ú® Both approaches are production-ready and reliable!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Research Paper Information Extraction\n",
    "\n",
    "Let's create a more complex nested schema for research paper analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Author(BaseModel):\n",
    "    \"\"\"Individual author information.\"\"\"\n",
    "    name: str = Field(description=\"Full name of the author\")\n",
    "    affiliation: Optional[str] = Field(None, description=\"Institution or organization\")\n",
    "    email: Optional[str] = Field(None, description=\"Email address if available\")\n",
    "\n",
    "class ResearchPaper(BaseModel):\n",
    "    \"\"\"Comprehensive research paper information extraction schema.\"\"\"\n",
    "    \n",
    "    title: str = Field(description=\"Complete title of the research paper\")\n",
    "    authors: List[Author] = Field(description=\"List of all authors with their details\")\n",
    "    abstract: str = Field(description=\"Paper abstract or summary\")\n",
    "    keywords: List[str] = Field(description=\"Key terms and concepts from the paper\")\n",
    "    publication_year: Optional[int] = Field(None, ge=1900, le=2030, description=\"Year of publication\")\n",
    "    venue: Optional[str] = Field(None, description=\"Journal, conference, or publication venue\")\n",
    "    methodology: str = Field(description=\"Research methods and approach used\")\n",
    "    key_findings: List[str] = Field(description=\"Main findings and contributions\")\n",
    "    limitations: List[str] = Field(description=\"Study limitations mentioned by authors\")\n",
    "    future_work: Optional[str] = Field(None, description=\"Suggested future research directions\")\n",
    "    confidence_score: float = Field(ge=0.0, le=1.0, description=\"Confidence in extraction accuracy (0-1)\")\n",
    "\n",
    "# Sample research paper abstract\n",
    "paper_text = \"\"\"\n",
    "Title: Deep Learning Approaches for Automated Medical Image Analysis: A Comprehensive Survey\n",
    "\n",
    "Authors: \n",
    "Dr. Sarah Chen (Stanford University Medical Center, schen@stanford.edu)\n",
    "Prof. Michael Rodriguez (MIT Computer Science Department)\n",
    "Dr. Aisha Patel (Johns Hopkins Hospital)\n",
    "\n",
    "Abstract:\n",
    "This comprehensive survey examines the current state of deep learning applications in medical image analysis. \n",
    "We review over 200 recent publications focusing on convolutional neural networks (CNNs), vision transformers, \n",
    "and generative adversarial networks (GANs) for medical imaging tasks. Our analysis covers applications in \n",
    "radiology, pathology, dermatology, and ophthalmology.\n",
    "\n",
    "Methodology:\n",
    "We conducted a systematic literature review of papers published between 2020-2023, analyzing model architectures, \n",
    "datasets, evaluation metrics, and clinical validation approaches. Performance metrics were standardized across \n",
    "studies for comparative analysis.\n",
    "\n",
    "Key Findings:\n",
    "1. Vision transformers show 15% better accuracy than CNNs on chest X-ray analysis\n",
    "2. Multi-modal approaches combining imaging and clinical data improve diagnostic accuracy by 12%\n",
    "3. Domain adaptation techniques reduce the need for labeled data by up to 40%\n",
    "4. Explainable AI methods increase clinician trust and adoption rates\n",
    "\n",
    "Limitations:\n",
    "- Limited diversity in datasets, with bias toward certain demographics\n",
    "- Lack of standardized evaluation protocols across studies  \n",
    "- Insufficient clinical validation in real-world settings\n",
    "- Regulatory approval challenges for AI-based diagnostic tools\n",
    "\n",
    "Future Work:\n",
    "We identify the need for larger, more diverse datasets, standardized evaluation frameworks, and collaborative \n",
    "efforts between AI researchers and medical professionals to bridge the gap between research and clinical practice.\n",
    "\n",
    "Keywords: deep learning, medical imaging, computer vision, healthcare AI, diagnostic automation\n",
    "Published: Journal of Medical AI, 2023\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    print(\"Extracting research paper information...\")\n",
    "    paper_data = get_structured_response(\n",
    "        paper_text, \n",
    "        ResearchPaper,\n",
    "        system_prompt=\"\"\"You are an expert research paper analyzer. Extract all available information \n",
    "        from the given research paper text. Be thorough and accurate. If specific information is not \n",
    "        available, use null for optional fields. Assess your confidence in the extraction accuracy.\"\"\"\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Successfully extracted research paper data!\\n\")\n",
    "    \n",
    "    print(\"Research Paper Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Title: {paper_data.title}\")\n",
    "    print(f\"Publication Year: {paper_data.publication_year}\")\n",
    "    print(f\"Venue: {paper_data.venue}\")\n",
    "    print(f\"Confidence Score: {paper_data.confidence_score:.2f}\")\n",
    "    \n",
    "    print(\"\\nAuthors:\")\n",
    "    for i, author in enumerate(paper_data.authors, 1):\n",
    "        print(f\"  {i}. {author.name}\")\n",
    "        if author.affiliation:\n",
    "            print(f\"     Affiliation: {author.affiliation}\")\n",
    "        if author.email:\n",
    "            print(f\"     Email: {author.email}\")\n",
    "    \n",
    "    print(f\"\\nKeywords: {', '.join(paper_data.keywords)}\")\n",
    "    \n",
    "    print(f\"\\nAbstract:\\n{wrap_text(paper_data.abstract)}\")\n",
    "    \n",
    "    print(f\"\\nMethodology:\\n{wrap_text(paper_data.methodology)}\")\n",
    "    \n",
    "    print(\"\\nKey Findings:\")\n",
    "    for i, finding in enumerate(paper_data.key_findings, 1):\n",
    "        print(f\"  {i}. {finding}\")\n",
    "    \n",
    "    print(\"\\nLimitations:\")\n",
    "    for i, limitation in enumerate(paper_data.limitations, 1):\n",
    "        print(f\"  {i}. {limitation}\")\n",
    "    \n",
    "    if paper_data.future_work:\n",
    "        print(f\"\\nFuture Work:\\n{wrap_text(paper_data.future_work)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error processing research paper: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reliability Testing and Performance Metrics\n",
    "\n",
    "Let's test the consistency and reliability of our structured response system across multiple runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "def test_consistency(text: str, schema_model: Type[BaseModel], num_runs: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"Test the consistency of structured responses across multiple runs.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    errors = []\n",
    "    response_times = []\n",
    "    \n",
    "    print(f\"Testing consistency across {num_runs} runs...\")\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            result = get_structured_response(text, schema_model)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            results.append(result.model_dump())\n",
    "            response_times.append(end_time - start_time)\n",
    "            print(f\"  Run {i+1}: ‚úì Success ({end_time - start_time:.2f}s)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            errors.append(str(e))\n",
    "            print(f\"  Run {i+1}: ‚ùå Error - {str(e)[:100]}...\")\n",
    "    \n",
    "    # Analyze consistency\n",
    "    success_rate = len(results) / num_runs\n",
    "    avg_response_time = sum(response_times) / len(response_times) if response_times else 0\n",
    "    \n",
    "    # Check field consistency\n",
    "    field_consistency = {}\n",
    "    if results:\n",
    "        first_result = results[0]\n",
    "        for field in first_result.keys():\n",
    "            field_values = [result.get(field) for result in results]\n",
    "            unique_values = len(set(str(v) for v in field_values))\n",
    "            field_consistency[field] = 1.0 - (unique_values - 1) / len(field_values)\n",
    "    \n",
    "    return {\n",
    "        'success_rate': success_rate,\n",
    "        'avg_response_time': avg_response_time,\n",
    "        'field_consistency': field_consistency,\n",
    "        'total_runs': num_runs,\n",
    "        'successful_runs': len(results),\n",
    "        'errors': errors,\n",
    "        'results': results\n",
    "    }\n",
    "\n",
    "# Test with a simple product review\n",
    "test_review = \"\"\"\n",
    "The iPhone 14 is amazing! Great camera quality and battery life lasts all day. \n",
    "A bit expensive at $999 but worth it. The display is crisp and bright. \n",
    "Only complaint is it gets warm during heavy gaming. Would definitely recommend! 5/5 stars.\n",
    "\"\"\"\n",
    "\n",
    "metrics = test_consistency(test_review, ProductReview, num_runs=5)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"RELIABILITY METRICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Success Rate: {metrics['success_rate']:.1%} ({metrics['successful_runs']}/{metrics['total_runs']} runs)\")\n",
    "print(f\"Average Response Time: {metrics['avg_response_time']:.2f} seconds\")\n",
    "\n",
    "if metrics['field_consistency']:\n",
    "    print(\"\\nField Consistency Scores:\")\n",
    "    for field, score in metrics['field_consistency'].items():\n",
    "        print(f\"  {field}: {score:.1%}\")\n",
    "\n",
    "if metrics['errors']:\n",
    "    print(f\"\\nErrors encountered: {len(metrics['errors'])}\")\n",
    "    for i, error in enumerate(metrics['errors'][:3], 1):  # Show first 3 errors\n",
    "        print(f\"  {i}. {error[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling Demonstration\n",
    "\n",
    "Let's demonstrate how our system handles various error scenarios and malformed responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_error_scenarios():\n",
    "    \"\"\"Test various error scenarios and recovery mechanisms.\"\"\"\n",
    "    \n",
    "    scenarios = [\n",
    "        {\n",
    "            'name': 'Ambiguous Text',\n",
    "            'text': 'This is some random text with no clear product information.',\n",
    "            'expected': 'Should handle missing information gracefully'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Mixed Language',\n",
    "            'text': 'Le iPhone es muy bueno, great camera, 4 √©toiles, price $800.',\n",
    "            'expected': 'Should extract available information despite mixed languages'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Contradictory Information',\n",
    "            'text': 'iPhone 15 is terrible, worst phone ever! Amazing camera though. 5 stars! Would not recommend.',\n",
    "            'expected': 'Should handle contradictory statements'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"Testing Error Handling Scenarios\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for scenario in scenarios:\n",
    "        print(f\"\\nüß™ Testing: {scenario['name']}\")\n",
    "        print(f\"Expected: {scenario['expected']}\")\n",
    "        print(f\"Input: {scenario['text'][:100]}...\")\n",
    "        \n",
    "        try:\n",
    "            result = get_structured_response(scenario['text'], ProductReview)\n",
    "            print(\"‚úÖ Successfully extracted data:\")\n",
    "            print(f\"   Product: {result.product_name}\")\n",
    "            print(f\"   Rating: {result.rating}/5\")\n",
    "            print(f\"   Sentiment: {result.sentiment}\")\n",
    "            print(f\"   Recommend: {result.would_recommend}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to extract: {str(e)[:150]}...\")\n",
    "    \n",
    "test_error_scenarios()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Pydantic Features\n",
    "\n",
    "Let's explore more advanced Pydantic features like custom validators, computed fields, and conditional schemas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_best_practices():\n",
    "    \"\"\"Demonstrate best practices for structured responses.\"\"\"\n",
    "    \n",
    "    print(\"üéØ BEST PRACTICES FOR STRUCTURED RESPONSES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    practices = [\n",
    "        {\n",
    "            'title': '1. Choose the Right Approach',\n",
    "            'description': 'Select between with_structured_output and manual parsing based on needs',\n",
    "            'example': '''\n",
    "# with_structured_output - Quick and clean ‚úÖ\n",
    "structured_llm = llm_chat.with_structured_output(MySchema)\n",
    "result = structured_llm.invoke(text)\n",
    "\n",
    "# Manual - Fine-grained control ‚úÖ\n",
    "result = get_structured_response(text, MySchema)\n",
    "'''\n",
    "        },\n",
    "        {\n",
    "            'title': '2. Clear Schema Definitions',\n",
    "            'description': 'Use descriptive field names and detailed Field descriptions',\n",
    "            'example': '''\n",
    "# Good ‚úÖ\n",
    "class Review(BaseModel):\n",
    "    rating: int = Field(ge=1, le=5, description=\"User rating from 1-5 stars\")\n",
    "    sentiment: str = Field(description=\"Overall sentiment: positive, negative, neutral\")\n",
    "\n",
    "# Avoid ‚ùå  \n",
    "class Review(BaseModel):\n",
    "    r: int  # Unclear field name\n",
    "    s: str  # No description\n",
    "'''\n",
    "        },\n",
    "        {\n",
    "            'title': '3. Robust Error Handling',\n",
    "            'description': 'Implement proper error handling for both approaches',\n",
    "            'example': '''\n",
    "# with_structured_output approach:\n",
    "try:\n",
    "    result = structured_llm.invoke(text)\n",
    "except Exception as e:\n",
    "    print(f\"Extraction failed: {e}\")\n",
    "    # Fallback logic\n",
    "\n",
    "# Manual approach has built-in multi-strategy parsing + @retry\n",
    "'''\n",
    "        },\n",
    "        {\n",
    "            'title': '4. Validation and Constraints',\n",
    "            'description': 'Use Pydantic validators to ensure data quality',\n",
    "            'example': '''\n",
    "# Use constraints and custom validators:\n",
    "@field_validator('email')\n",
    "@classmethod\n",
    "def validate_email(cls, v):\n",
    "    if v and '@' not in v:\n",
    "        raise ValueError('Invalid email format')\n",
    "    return v\n",
    "'''\n",
    "        },\n",
    "        {\n",
    "            'title': '5. Performance Optimization',\n",
    "            'description': 'Optimize for speed and reliability',\n",
    "            'example': '''\n",
    "# Performance tips:\n",
    "- Use low temperature (0.1-0.2) for consistency\n",
    "- Cache LLM instances (don't recreate each time)\n",
    "- Use with_structured_output for simpler schemas\n",
    "- Use manual approach when you need retry strategies\n",
    "- Monitor response times and success rates\n",
    "'''\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for practice in practices:\n",
    "        print(f\"\\n{practice['title']}\")\n",
    "        print(f\"{practice['description']}\")\n",
    "        if practice['example']:\n",
    "            print(practice['example'])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üîß COMMON TROUBLESHOOTING\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    issues = [\n",
    "        {\n",
    "            'problem': 'JSON Parsing Errors',\n",
    "            'solutions': [\n",
    "                'Try with_structured_output first - it handles parsing automatically',\n",
    "                'For manual: Check for extra text before/after JSON',\n",
    "                'Use json5 for more lenient parsing (manual approach includes this)',\n",
    "                'Improve prompt clarity about JSON-only responses'\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            'problem': 'Validation Errors',\n",
    "            'solutions': [\n",
    "                'Review field constraints (min/max values)',\n",
    "                'Make optional fields truly optional with Optional[Type]',\n",
    "                'Add custom validators for complex logic',\n",
    "                'Provide clear examples in prompts'\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            'problem': 'Inconsistent Results',\n",
    "            'solutions': [\n",
    "                'Lower temperature for more deterministic outputs',\n",
    "                'Improve prompt specificity',\n",
    "                'Use multiple validation runs to test consistency',\n",
    "                'Consider using method=\"json_schema\" with with_structured_output'\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            'problem': 'Performance Issues', \n",
    "            'solutions': [\n",
    "                'Use with_structured_output for faster development',\n",
    "                'Optimize prompt length in manual approach',\n",
    "                'Use appropriate Granite model size (3b vs 7b)',\n",
    "                'Implement caching for repeated requests',\n",
    "                'Consider batch processing'\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for issue in issues:\n",
    "        print(f\"\\n‚ùó {issue['problem']}:\")\n",
    "        for solution in issue['solutions']:\n",
    "            print(f\"   ‚Ä¢ {solution}\")\n",
    "\n",
    "demonstrate_best_practices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the power of combining Pydantic validation with local Ollama inference for generating reliable, structured responses from IBM Granite models. We explored **two production-ready approaches** that serve different needs.\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Two Methods, Both Reliable**: \n",
    "   - **with_structured_output**: LangChain's native method for clean, minimal code\n",
    "   - **Manual Parsing**: Custom approach with fine-grained control and multiple fallback strategies\n",
    "\n",
    "2. **Type Safety**: Pydantic provides compile-time type checking and runtime validation\n",
    "3. **Error Handling**: Both approaches offer robust error handling (automatic vs custom)\n",
    "4. **Local Control**: Ollama provides privacy, offline capability, and eliminates API dependencies\n",
    "5. **Flexibility**: Choose the method that fits your workflow and requirements\n",
    "6. **Production Ready**: Advanced features like validators, computed fields, and retry logic\n",
    "\n",
    "### What Makes This Notebook Unique:\n",
    "\n",
    "Unlike the [Entity Extraction recipe](../Entity-Extraction/entity_extraction.ipynb) which focuses on cloud APIs (Replicate/Watsonx), this notebook is designed for:\n",
    "\n",
    "- **Local Deployment**: Privacy-focused, offline-capable infrastructure\n",
    "- **Production Patterns**: Enterprise-grade error handling, debugging, and monitoring\n",
    "- **Advanced Features**: Custom validators, computed fields, multiple parsing strategies\n",
    "- **Method Comparison**: Understand trade-offs between LangChain native vs custom approaches\n",
    "- **Comprehensive Testing**: Reliability testing, performance benchmarking, best practices\n",
    "\n",
    "### When to Use This Approach:\n",
    "\n",
    "- ‚úÖ **Local/Offline Deployment**: Privacy requirements or air-gapped environments\n",
    "- ‚úÖ **Data extraction from unstructured text** with validation\n",
    "- ‚úÖ **API response standardization** with type safety\n",
    "- ‚úÖ **Production systems needing reliable outputs** with error recovery\n",
    "- ‚úÖ **Cost Control**: No per-request API charges\n",
    "\n",
    "### Performance vs. Alternatives:\n",
    "\n",
    "- **vs. Raw JSON**: Higher reliability, better error handling, type safety\n",
    "- **vs. Cloud APIs**: Privacy, cost control, offline capability\n",
    "- **vs. Entity Extraction Recipe**: Local deployment, production patterns, advanced Pydantic features\n",
    "- **Manual vs with_structured_output**: Control vs simplicity - both are excellent choices\n",
    "\n",
    "### Method Selection Guide:\n",
    "\n",
    "**Choose `with_structured_output` when:**\n",
    "- Building standard LangChain pipelines\n",
    "- Prioritizing clean, maintainable code\n",
    "- Prototyping or rapid development\n",
    "- Working with straightforward schemas\n",
    "\n",
    "**Choose Manual Approach when:**\n",
    "- Need Granite-specific chat templates\n",
    "- Require custom retry or parsing strategies\n",
    "- Building complex production workflows\n",
    "- Need fine-grained prompt control\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Scale Up**: Implement batch processing for large datasets\n",
    "2. **Monitoring**: Add logging and metrics collection\n",
    "3. **Custom Models**: Fine-tune Granite models for specific domains\n",
    "4. **Integration**: Build into production pipelines\n",
    "5. **Hybrid Approach**: Use with_structured_output for prototyping, manual for production edge cases\n",
    "\n",
    "The combination of Pydantic's robust validation with Granite's language understanding and Ollama's local deployment creates a powerful foundation for reliable structured data extraction in production systems, whether you choose the LangChain-native or custom approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unstructured_response(text: str, fields: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"Get response without Pydantic validation for comparison.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Extract the following information from the text and return as JSON:\n",
    "    Fields to extract: {', '.join(fields)}\n",
    "    \n",
    "    Text: {text}\n",
    "    \n",
    "    Return valid JSON only:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    # Simple JSON parsing without validation\n",
    "    try:\n",
    "        return json.loads(response.strip())\n",
    "    except json.JSONDecodeError:\n",
    "        # Try to extract JSON\n",
    "        start_idx = response.find('{')\n",
    "        end_idx = response.rfind('}')\n",
    "        if start_idx != -1 and end_idx != -1:\n",
    "            return json.loads(response[start_idx:end_idx+1])\n",
    "        raise ValueError(\"Could not parse JSON response\")\n",
    "\n",
    "def compare_approaches(text: str, num_runs: int = 5):\n",
    "    \"\"\"Compare Pydantic-validated vs unstructured approaches.\"\"\"\n",
    "    \n",
    "    fields = ['product_name', 'rating', 'sentiment', 'pros', 'cons', 'would_recommend']\n",
    "    \n",
    "    print(f\"Comparing approaches over {num_runs} runs...\\n\")\n",
    "    \n",
    "    # Test with Pydantic validation\n",
    "    pydantic_success = 0\n",
    "    pydantic_times = []\n",
    "    pydantic_valid_data = 0\n",
    "    \n",
    "    print(\"üîç Testing WITH Pydantic Validation:\")\n",
    "    for i in range(num_runs):\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            result = get_structured_response(text, ProductReview)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            pydantic_success += 1\n",
    "            pydantic_times.append(end_time - start_time)\n",
    "            \n",
    "            # Check data validity\n",
    "            if (1 <= result.rating <= 5 and \n",
    "                isinstance(result.pros, list) and \n",
    "                isinstance(result.cons, list) and\n",
    "                isinstance(result.would_recommend, bool)):\n",
    "                pydantic_valid_data += 1\n",
    "            \n",
    "            print(f\"  Run {i+1}: ‚úì Success\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Run {i+1}: ‚ùå Failed\")\n",
    "    \n",
    "    # Test without Pydantic validation\n",
    "    unstructured_success = 0\n",
    "    unstructured_times = []\n",
    "    unstructured_valid_data = 0\n",
    "    \n",
    "    print(\"\\nüîç Testing WITHOUT Pydantic Validation:\")\n",
    "    for i in range(num_runs):\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            result = get_unstructured_response(text, fields)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            unstructured_success += 1\n",
    "            unstructured_times.append(end_time - start_time)\n",
    "            \n",
    "            # Check data validity (more lenient)\n",
    "            rating = result.get('rating', 0)\n",
    "            pros = result.get('pros', [])\n",
    "            cons = result.get('cons', [])\n",
    "            recommend = result.get('would_recommend', False)\n",
    "            \n",
    "            if (isinstance(rating, (int, float)) and 1 <= rating <= 5 and\n",
    "                isinstance(pros, list) and isinstance(cons, list) and\n",
    "                isinstance(recommend, bool)):\n",
    "                unstructured_valid_data += 1\n",
    "            \n",
    "            print(f\"  Run {i+1}: ‚úì Success\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Run {i+1}: ‚ùå Failed\")\n",
    "    \n",
    "    # Results comparison\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìä COMPARISON RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    pydantic_avg_time = sum(pydantic_times) / len(pydantic_times) if pydantic_times else 0\n",
    "    unstructured_avg_time = sum(unstructured_times) / len(unstructured_times) if unstructured_times else 0\n",
    "    \n",
    "    print(f\"\\n{'Metric':<25} {'Pydantic':<15} {'Unstructured':<15} {'Winner':<10}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    # Success Rate\n",
    "    pydantic_success_rate = pydantic_success / num_runs\n",
    "    unstructured_success_rate = unstructured_success / num_runs\n",
    "    if pydantic_success_rate > unstructured_success_rate:\n",
    "        success_winner = \"Pydantic\"\n",
    "    elif unstructured_success_rate > pydantic_success_rate:\n",
    "        success_winner = \"Unstructured\" \n",
    "    else:\n",
    "        success_winner = \"Tie\"\n",
    "    pydantic_success_pct = f\"{pydantic_success_rate:.1%}\"\n",
    "    unstructured_success_pct = f\"{unstructured_success_rate:.1%}\"\n",
    "    print(f\"{'Success Rate':<25} {pydantic_success_pct:<15} {unstructured_success_pct:<15} {success_winner:<10}\")\n",
    "    \n",
    "    # Data Validity\n",
    "    pydantic_validity_rate = pydantic_valid_data / max(pydantic_success, 1)\n",
    "    unstructured_validity_rate = unstructured_valid_data / max(unstructured_success, 1)\n",
    "    if pydantic_validity_rate > unstructured_validity_rate:\n",
    "        validity_winner = \"Pydantic\"\n",
    "    elif unstructured_validity_rate > pydantic_validity_rate:\n",
    "        validity_winner = \"Unstructured\"\n",
    "    else:\n",
    "        validity_winner = \"Tie\"\n",
    "    pydantic_validity_pct = f\"{pydantic_validity_rate:.1%}\"\n",
    "    unstructured_validity_pct = f\"{unstructured_validity_rate:.1%}\"\n",
    "    print(f\"{'Data Validity Rate':<25} {pydantic_validity_pct:<15} {unstructured_validity_pct:<15} {validity_winner:<10}\")\n",
    "    \n",
    "    # Response Time\n",
    "    if pydantic_avg_time < unstructured_avg_time:\n",
    "        time_winner = \"Pydantic\"\n",
    "    elif unstructured_avg_time < pydantic_avg_time:\n",
    "        time_winner = \"Unstructured\"\n",
    "    else:\n",
    "        time_winner = \"Tie\"\n",
    "    pydantic_time_str = f\"{pydantic_avg_time:.2f}s\"\n",
    "    unstructured_time_str = f\"{unstructured_avg_time:.2f}s\"\n",
    "    print(f\"{'Avg Response Time':<25} {pydantic_time_str:<15} {unstructured_time_str:<15} {time_winner:<10}\")\n",
    "    \n",
    "    # Overall Reliability\n",
    "    pydantic_reliability = pydantic_success_rate * pydantic_validity_rate\n",
    "    unstructured_reliability = unstructured_success_rate * unstructured_validity_rate\n",
    "    if pydantic_reliability > unstructured_reliability:\n",
    "        reliability_winner = \"Pydantic\"\n",
    "    elif unstructured_reliability > pydantic_reliability:\n",
    "        reliability_winner = \"Unstructured\"\n",
    "    else:\n",
    "        reliability_winner = \"Tie\"\n",
    "    pydantic_reliability_pct = f\"{pydantic_reliability:.1%}\"\n",
    "    unstructured_reliability_pct = f\"{unstructured_reliability:.1%}\"\n",
    "    print(f\"{'Overall Reliability':<25} {pydantic_reliability_pct:<15} {unstructured_reliability_pct:<15} {reliability_winner:<10}\")\n",
    "    \n",
    "    # Summary Analysis\n",
    "    print(f\"\\nüéØ ANALYSIS:\")\n",
    "    if success_winner == \"Tie\" and validity_winner == \"Tie\":\n",
    "        print(\"‚úÖ Both approaches achieved identical reliability for this simple, well-structured text\")\n",
    "        if unstructured_avg_time < pydantic_avg_time:\n",
    "            time_diff = pydantic_avg_time - unstructured_avg_time\n",
    "            percent_improvement = (time_diff / pydantic_avg_time) * 100\n",
    "            print(f\"‚ö° Unstructured is {time_diff:.1f}s faster ({percent_improvement:.1f}% improvement)\")\n",
    "        print(\"üí° Pydantic's benefits emerge with complex, ambiguous, or malformed text\")\n",
    "    else:\n",
    "        print(\"üìä Performance varies - check individual metrics for trade-offs\")\n",
    "\n",
    "# Run comparison test\n",
    "comparison_review = \"\"\"\n",
    "AirPods Pro 2nd gen are fantastic! Noise cancellation is top-notch, sound quality \n",
    "is crisp, and the spatial audio feature is immersive. Battery life is solid at 6 hours \n",
    "with ANC on. The transparency mode works perfectly for hearing surroundings.\n",
    "\n",
    "Cons: Expensive at $249, case is a fingerprint magnet, and sometimes connectivity \n",
    "can be finicky with non-Apple devices. \n",
    "\n",
    "Overall: 4.5/5 stars, highly recommend for iPhone users!\n",
    "\"\"\"\n",
    "\n",
    "compare_approaches(comparison_review, num_runs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices and Troubleshooting\n",
    "\n",
    "Here are key recommendations for successful structured response implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_best_practices():\n",
    "    \"\"\"Demonstrate best practices for structured responses.\"\"\"\n",
    "    \n",
    "    print(\"üéØ BEST PRACTICES FOR STRUCTURED RESPONSES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    practices = [\n",
    "        {\n",
    "            'title': '1. Clear Schema Definitions',\n",
    "            'description': 'Use descriptive field names and detailed Field descriptions',\n",
    "            'example': '''\n",
    "# Good ‚úÖ\n",
    "class Review(BaseModel):\n",
    "    rating: int = Field(ge=1, le=5, description=\"User rating from 1-5 stars\")\n",
    "    sentiment: str = Field(description=\"Overall sentiment: positive, negative, neutral\")\n",
    "\n",
    "# Avoid ‚ùå  \n",
    "class Review(BaseModel):\n",
    "    r: int  # Unclear field name\n",
    "    s: str  # No description\n",
    "'''\n",
    "        },\n",
    "        {\n",
    "            'title': '2. Robust Error Handling',\n",
    "            'description': 'Implement multiple parsing strategies and retry mechanisms',\n",
    "            'example': '''\n",
    "# Implement multiple parsing fallbacks:\n",
    "1. Direct JSON parsing\n",
    "2. Extract JSON from text\n",
    "3. Use json5 for lenient parsing\n",
    "4. Manual field extraction as last resort\n",
    "'''\n",
    "        },\n",
    "        {\n",
    "            'title': '3. Effective Prompting',\n",
    "            'description': 'Use clear instructions and proper chat formatting',\n",
    "            'example': '''\n",
    "# Key prompt elements:\n",
    "- Clear role definition\n",
    "- Explicit JSON-only response requirement\n",
    "- Schema inclusion in prompt\n",
    "- Handling of missing data\n",
    "- Use of proper chat templates\n",
    "'''\n",
    "        },\n",
    "        {\n",
    "            'title': '4. Validation and Constraints',\n",
    "            'description': 'Use Pydantic validators to ensure data quality',\n",
    "            'example': '''\n",
    "# Use constraints and custom validators:\n",
    "@validator('email')\n",
    "def validate_email(cls, v):\n",
    "    if v and '@' not in v:\n",
    "        raise ValueError('Invalid email format')\n",
    "    return v\n",
    "'''\n",
    "        },\n",
    "        {\n",
    "            'title': '5. Performance Optimization',\n",
    "            'description': 'Optimize for speed and reliability',\n",
    "            'example': '''\n",
    "# Performance tips:\n",
    "- Use low temperature for consistency\n",
    "- Cache tokenizer instances\n",
    "- Implement exponential backoff for retries\n",
    "- Monitor response times and success rates\n",
    "'''\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for practice in practices:\n",
    "        print(f\"\\n{practice['title']}\")\n",
    "        print(f\"{practice['description']}\")\n",
    "        if practice['example']:\n",
    "            print(practice['example'])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üîß COMMON TROUBLESHOOTING\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    issues = [\n",
    "        {\n",
    "            'problem': 'JSON Parsing Errors',\n",
    "            'solutions': [\n",
    "                'Check for extra text before/after JSON',\n",
    "                'Use json5 for more lenient parsing',\n",
    "                'Improve prompt clarity about JSON-only responses',\n",
    "                'Add response cleaning preprocessing'\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            'problem': 'Validation Errors',\n",
    "            'solutions': [\n",
    "                'Review field constraints (min/max values)',\n",
    "                'Make optional fields truly optional',\n",
    "                'Add custom validators for complex logic',\n",
    "                'Provide clear examples in prompts'\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            'problem': 'Inconsistent Results',\n",
    "            'solutions': [\n",
    "                'Lower temperature for more deterministic outputs',\n",
    "                'Improve prompt specificity',\n",
    "                'Use multiple validation runs',\n",
    "                'Implement consensus mechanisms'\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            'problem': 'Performance Issues', \n",
    "            'solutions': [\n",
    "                'Optimize prompt length',\n",
    "                'Use appropriate model size',\n",
    "                'Implement caching for repeated requests',\n",
    "                'Consider batch processing'\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for issue in issues:\n",
    "        print(f\"\\n‚ùó {issue['problem']}:\")\n",
    "        for solution in issue['solutions']:\n",
    "            print(f\"   ‚Ä¢ {solution}\")\n",
    "\n",
    "demonstrate_best_practices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the power of combining Pydantic validation with local Ollama inference for generating reliable, structured responses from IBM Granite models. \n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Reliability**: Pydantic validation significantly improves response parsing success rates and data quality\n",
    "2. **Type Safety**: Strong typing prevents runtime errors and improves code maintainability  \n",
    "3. **Error Handling**: Multiple parsing strategies and retry mechanisms ensure robust operation\n",
    "4. **Local Control**: Ollama provides privacy and eliminates API dependencies\n",
    "5. **Flexibility**: Complex nested schemas handle sophisticated data extraction tasks\n",
    "\n",
    "### When to Use This Approach:\n",
    "\n",
    "- ‚úÖ **Data extraction from unstructured text**\n",
    "- ‚úÖ **API response standardization** \n",
    "- ‚úÖ **Database population with validation**\n",
    "- ‚úÖ **Multi-step workflows requiring consistent formats**\n",
    "- ‚úÖ **Production systems needing reliable outputs**\n",
    "\n",
    "### Performance vs. Alternatives:\n",
    "\n",
    "- **vs. Raw JSON**: Higher reliability, better error handling, type safety\n",
    "- **vs. LMStudio**: More control, better integration, no external dependencies\n",
    "- **vs. API Services**: Privacy, cost control, no rate limits\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Scale Up**: Implement batch processing for large datasets\n",
    "2. **Monitoring**: Add logging and metrics collection\n",
    "3. **Custom Models**: Fine-tune models for specific domains\n",
    "4. **Integration**: Build into production pipelines\n",
    "\n",
    "The combination of Pydantic's robust validation with Granite's language understanding creates a powerful foundation for reliable structured data extraction in production systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Further Reading\n",
    "\n",
    "1. [Pydantic Documentation](https://docs.pydantic.dev/) - Comprehensive guide to Pydantic features\n",
    "2. [Ollama Documentation](https://ollama.ai/docs) - Setup and usage guides for local LLM inference\n",
    "3. [IBM Granite Models](https://www.ibm.com/granite) - Information about Granite model capabilities\n",
    "4. [Structured Response Comparison Notebook](./Structured_Responses_LMStudio.ipynb) - Alternative approach using LMStudio\n",
    "5. [Entity Extraction Recipe](../Entity-Extraction/entity_extraction.ipynb) - Related structured data extraction examples"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
