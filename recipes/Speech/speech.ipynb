{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Building a Voice Assistant with Granite Models\n",
    "\n",
    "In this notebook, we'll walk through the process of building a voice assistant using the **Granite series of models**, specifically focusing on the **Granite Speech-to-Text** model for transcription.\n",
    "\n",
    "The core of our assistant will be powered by two main components:\n",
    "1.  **Granite Speech 3.3 8B**: To accurately transcribe spoken audio from the microphone into text in real-time.\n",
    "2.  **Granite Instruct 3.3 8B**: A large language model (LLM) that will understand the transcribed text and generate intelligent, conversational responses.\n",
    "\n",
    "## 1. Setup and Installation\n",
    "\n",
    "First, we need to install the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/ibm-granite-community/utils replicate soundfile sounddevice numpy webrtcvad scipy langchain-community langchain-core datasets ipywidgets ipywidgets torch transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 2. A Quick Test: Transcribing a File\n",
    "\n",
    "Before we start building the voice assistant, let's begin with a simpler task: transcribing a pre-existing audio file. This allows us to test our transcription function in a controlled way.\n",
    "\n",
    "We will use the Hugging Face `datasets` library to download a sample from the `fixie-ai/llama-questions` dataset, which contains short audio recordings of spoken questions. We'll then extract the audio data and save it as a standard `.wav` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "import soundfile as sf, io\n",
    "\n",
    "# 1) Load dataset\n",
    "ds = load_dataset(\"fixie-ai/llama-questions\", split=\"test\")\n",
    "\n",
    "# 2) Grab raw bytes for the first clip\n",
    "ds = ds.cast_column(\"audio\", Audio(decode=False))\n",
    "audio_bytes = ds[0][\"audio\"][\"bytes\"]\n",
    "\n",
    "# 3) Decode with soundfile\n",
    "waveform, sr = sf.read(io.BytesIO(audio_bytes))\n",
    "\n",
    "# 4) Save to a file\n",
    "sf.write(\"sample.wav\", waveform, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 3. Transcribing with Granite Speech\n",
    "\n",
    "With our sample audio file ready, we can now use the Granite Speech model to convert it to text.\n",
    "\n",
    "### How it Works\n",
    "The process involves two helper functions:\n",
    "\n",
    "1.  `encode_audio(filepath)`: The Replicate API expects the audio data to be sent as a base64-encoded [Data URI](https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/Data_URLs). This function reads our `.wav` file, encodes it, and formats it into the required string format.\n",
    "\n",
    "2.  `transcribe(data_uri)`: This is the core function for speech-to-text. It uses the `langchain-community` integration to call the `ibm-granite/granite-speech-3.3-8b` model on Replicate. We pass the encoded audio and a simple prompt.\n",
    "\n",
    "Let's call these functions and print the transcription of our sample file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import mimetypes\n",
    "from langchain.llms import Replicate\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "\n",
    "def encode_audio(filepath: str) -> str:\n",
    "    mime_type, _ = mimetypes.guess_type(filepath)\n",
    "    if not mime_type or not mime_type.startswith(\"audio\"):\n",
    "        raise ValueError(f\"Unsupported audio type for {filepath}\")\n",
    "\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        encoded = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "        return f\"data:{mime_type};base64,{encoded}\"\n",
    "\n",
    "def transcribe(data_uri: str) -> str:\n",
    "\n",
    "    stt = Replicate(\n",
    "        model=\"ibm-granite/granite-speech-3.3-8b\",\n",
    "        model_kwargs={\n",
    "            \"audio\": [data_uri],\n",
    "            \"top_k\": 50,\n",
    "            \"top_p\": 0.9,\n",
    "            \"prompt\": \"Transcribe the speech into written form.\",\n",
    "            \"max_tokens\": 512,\n",
    "            \"min_tokens\": 0,\n",
    "            \"temperature\": 0.6,\n",
    "            \"presence_penalty\": 0,\n",
    "            \"frequency_penalty\": 0\n",
    "        },\n",
    "        replicate_api_token=get_env_var(\"REPLICATE_API_TOKEN\")\n",
    "    )\n",
    "\n",
    "    return stt.invoke(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_uri = encode_audio(\"sample.wav\")\n",
    "transcription = transcribe(data_uri)\n",
    "\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 4. Real-time Recording with Voice Activity Detection (VAD)\n",
    "\n",
    "Transcribing a file is useful, but a true voice assistant needs to listen in real-time. A key challenge here is knowing when the user has finished speaking. \n",
    "\n",
    "To solve this, we'll implement **Voice Activity Detection (VAD)**. VAD is a technique used to distinguish between human speech and silence in an audio stream. We'll use the `webrtcvad` library, a fast and effective VAD implementation.\n",
    "\n",
    "### The `record_segment` function\n",
    "The code below defines a function `record_segment` that listens to the microphone and captures a single segment of speech. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import webrtcvad\n",
    "import collections\n",
    "import io\n",
    "import wave\n",
    "import base64\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "FRAME_DURATION_MS = 30\n",
    "FRAME_SIZE = int(SAMPLE_RATE * FRAME_DURATION_MS / 1000)\n",
    "MAX_SEGMENT_SEC = 10\n",
    "VAD_SILENCE_FRAMES = int(0.75 * 1000 / FRAME_DURATION_MS)\n",
    "\n",
    "def record_segment():\n",
    "    vad          = webrtcvad.Vad(3)\n",
    "    voiced_frames = []                      \n",
    "    ring_buffer   = collections.deque(maxlen=VAD_SILENCE_FRAMES)\n",
    "    recording_done, stop_reason = False, \"timeout\"\n",
    "\n",
    "    def callback(indata, frames, time, status):\n",
    "        nonlocal recording_done, stop_reason\n",
    "        pcm = (indata[:, 0] * 32767).astype(np.int16).tobytes()\n",
    "\n",
    "        is_speech = vad.is_speech(pcm, SAMPLE_RATE)\n",
    "        ring_buffer.append(is_speech)\n",
    "\n",
    "        if is_speech:                       # keep only speech frames\n",
    "            voiced_frames.append(pcm)\n",
    "\n",
    "        # 0.5 s of silence ‚Üí stop\n",
    "        if len(ring_buffer) == VAD_SILENCE_FRAMES and not any(ring_buffer):\n",
    "            stop_reason, recording_done = \"vad\", True\n",
    "\n",
    "    with sd.InputStream(samplerate=SAMPLE_RATE,\n",
    "                        channels=1,\n",
    "                        dtype='float32',\n",
    "                        blocksize=FRAME_SIZE,\n",
    "                        callback=callback):\n",
    "        elapsed = 0\n",
    "        while elapsed < MAX_SEGMENT_SEC and not recording_done:\n",
    "            sd.sleep(FRAME_DURATION_MS)\n",
    "            elapsed += FRAME_DURATION_MS / 1000\n",
    "\n",
    "    # ---- emit ONE base64 chunk for the whole segment ----\n",
    "    wav_bytes = b''.join(voiced_frames)\n",
    "    buffer = io.BytesIO()\n",
    "    with wave.open(buffer, 'wb') as wf:\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(2)\n",
    "        wf.setframerate(SAMPLE_RATE)\n",
    "        wf.writeframes(wav_bytes)\n",
    "\n",
    "    return buffer.getvalue(), stop_reason\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 5. Generating Responses with Granite LLM\n",
    "\n",
    "Now that we can convert speech to text, we need a text-to-text language model for our system to respond to the transcribed user inputs. For this, we'll use the `ibm-granite/granite-3.3-8b-instruct`. \n",
    "\n",
    "The `chat` function below handles the conversation logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Replicate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import BaseMessage\n",
    "from transformers import AutoTokenizer\n",
    "from ibm_granite_community.langchain import TokenizerChatPromptTemplate\n",
    "\n",
    "def chat(\n",
    "    messages: list[BaseMessage],\n",
    "    replicate_model = \"ibm-granite/granite-3.3-8b-instruct\",\n",
    ") -> str:\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(replicate_model)\n",
    "\n",
    "    # Format conversation with tokenizer-backed chat prompt\n",
    "    prompt = TokenizerChatPromptTemplate.from_messages(\n",
    "        messages=messages,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Granite LLM via Replicate\n",
    "    llm = Replicate(\n",
    "        model=replicate_model,\n",
    "        replicate_api_token=get_env_var(\"REPLICATE_API_TOKEN\"),\n",
    "        model_kwargs={\n",
    "            \"max_tokens\": 512,\n",
    "            \"temperature\": 0.6,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 50\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Chain and run\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    return chain.invoke({})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 6. The Main Conversation Loop\n",
    "\n",
    "All the core components are now in place:\n",
    "*   `record_segment`: Records a user's speech segment from the microphone.\n",
    "*   `transcribe`: Converts audio to text using the Granite Speech model.\n",
    "*   `chat`: Generates a text response using the Granite LLM.\n",
    "\n",
    "Now, we just need to put them together in a continuous loop to create our interactive voice assistant. The `start_conversation_loop` function below orchestrates this entire process.\n",
    "\n",
    "The assistant runs with a **background listener** that continuously records speech segments and puts them into a queue. The main loop processes each segment as it arrives, so the assistant is always ready to capture your speech‚Äîeven if you start talking while it's responding.\n",
    "\n",
    "Here‚Äôs how each turn works:\n",
    "1.  **Prompt**: The notebook prints `User (Speak now...):` and accumulates your transcribed speech as you talk.\n",
    "2.  **Listen**: The background listener captures your speech and adds it to the queue.\n",
    "3.  **Transcribe**: Each segment is transcribed and appended to the growing user input line.\n",
    "4.  **Update History**: When you finish speaking, the full user message is added to the conversation history.\n",
    "5.  **Get Response**: The complete conversation history is sent to the `chat` function to generate the assistant's reply.\n",
    "6.  **Print Response**: The assistant's reply is printed below your message.\n",
    "7.  **Repeat**: The loop starts over, ready to capture your next input as soon as you start speaking.\n",
    "\n",
    "This design ensures the assistant never misses your speech and provides a natural, conversational experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio, base64, io, wave\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage\n",
    "\n",
    "MIN_MS = 150  \n",
    "\n",
    "def has_payload(wav_bytes: bytes, min_ms: int = MIN_MS) -> bool:\n",
    "    \"\"\"True if WAV has >0 frames and duration >= min_ms.\"\"\"\n",
    "    if not wav_bytes:\n",
    "        return False\n",
    "    try:\n",
    "        with wave.open(io.BytesIO(wav_bytes), \"rb\") as wf:\n",
    "            frames = wf.getnframes()\n",
    "            sr = wf.getframerate() or 1\n",
    "        dur_ms = frames * 1000 / sr\n",
    "        return frames > 0 and dur_ms >= min_ms\n",
    "    except wave.Error:\n",
    "        return False\n",
    "\n",
    "async def background_listener(queue: asyncio.Queue):\n",
    "    \"\"\"Continuously record segments and enqueue only real audio.\"\"\"\n",
    "    while True:\n",
    "        audio_bytes, stop_reason = await asyncio.to_thread(record_segment)\n",
    "        if has_payload(audio_bytes):\n",
    "            await queue.put((audio_bytes, stop_reason))\n",
    "\n",
    "async def start_conversation_loop() -> None:\n",
    "    print(\"Conversation started. Speak to begin...\")\n",
    "    history: list[BaseMessage] = [SystemMessage(content=\"You are a helpful assistant.\")]\n",
    "    queue: asyncio.Queue = asyncio.Queue()\n",
    "    asyncio.create_task(background_listener(queue))\n",
    "\n",
    "    current_utterance = \"\"\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            audio_bytes, stop_reason = await queue.get()\n",
    "\n",
    "            # --- 1) Transcribe safely ---\n",
    "            data_uri = \"data:audio/wav;base64,\" + base64.b64encode(audio_bytes).decode()\n",
    "            try:\n",
    "                segment_text = await asyncio.to_thread(transcribe, data_uri)\n",
    "            except Exception as e:  # catch ValueError/RuntimeError/etc.\n",
    "                print(\"‚ö†Ô∏è STT error:\", repr(e))\n",
    "                continue\n",
    "\n",
    "            segment_text = segment_text.strip()\n",
    "            if not segment_text:\n",
    "                continue\n",
    "\n",
    "            # --- 2) Accumulate text ---\n",
    "            current_utterance = (current_utterance + \" \" + segment_text).strip()\n",
    "\n",
    "            # --- 3) Turn boundary (VAD silence) ---\n",
    "            if stop_reason == \"vad\" and current_utterance:\n",
    "                print(f\"\\nüó£Ô∏è User: {current_utterance}\")\n",
    "                history.append(HumanMessage(content=current_utterance))\n",
    "\n",
    "                try:\n",
    "                    response = await asyncio.to_thread(chat, history)\n",
    "                except Exception as e:\n",
    "                    print(\"‚ö†Ô∏è LLM error:\", repr(e))\n",
    "                    response = \"[error generating response]\"\n",
    "                print(f\"\\nü§ñ Assistant: {response}\\n{'_'*72}\\nüé§ Speak now...\")\n",
    "\n",
    "                history.append(AIMessage(content=response))\n",
    "                current_utterance = \"\"  # reset\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nConversation ended by keyboard interrupt.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 7. Run the Voice Assistant!\n",
    "\n",
    "It's time to run our application. Executing the cell below will start the conversation. The program will print \"Speak now...\" when it's ready.\n",
    "\n",
    "Go ahead and start chatting!\n",
    "\n",
    "To stop the assistant, simply interrupt the kernel (the \"Stop\" button in most notebook environments) or press `Ctrl-C` if running as a script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "await start_conversation_loop()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
