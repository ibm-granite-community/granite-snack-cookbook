{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Retrieval Augmented Generation (RAG) with LlamaIndex\n",
    "*Using IBM Granite Models*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Recipe Overview\n",
    "\n",
    "Welcome to this Granite Recipe!\n",
    "\n",
    "In this notebook you will learn to implement Retrieval Augumented Generation (RAG) using LlamaIndex orchestration framework. \n",
    "\n",
    "RAG is an architecture that optimizes the performance of language models by connecting it to knowledge bases. By doing so, the language models are capable of recalling factual information from the knowledge base and customizing this information to respond to the user query. \n",
    "\n",
    "The major components of RAG architecture are:\n",
    "1. Knowledge Base - Data repository for the system\n",
    "2. Retriever - A language model that gathers context from the knowledge base that is relevant to the user query\n",
    "3. Generator - A language model that generates response to the augmented query that contains the user query and the context identified by the retriever\n",
    "4. Integration Layer - A layer that co-ordinates and brings together the functionality of all the components\n",
    "\n",
    "Advantages of RAG architecture include access to domain-specific information, cost efficient AI implementation/scaling, reduced risk of hallucinations, greater data security etc. Some use cases of RAG are:\n",
    "- Customer service: Answering questions about a product or service using facts from the product documentation.\n",
    "- Specialized chatbot: Exploring a specialized domain (e.g., finance) using facts from papers or articles in the knowledge base.\n",
    "- News chat: Chatting about current events by calling up relevant recent news articles.\n",
    "\n",
    "[![Open YouTube video](https://img.youtube.com/vi/T-D1OfcDW1M/0.jpg)](https://www.youtube.com/watch?v=T-D1OfcDW1M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Environment Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/ibm-granite-community/utils \\\n",
    "    transformers \\\n",
    "    llama-index \\\n",
    "    llama-index-embeddings-huggingface \\\n",
    "    llama-index-vector-stores-chroma \\\n",
    "    wget \\\n",
    "    chromadb \\\n",
    "    llama-index-llms-replicate \\\n",
    "    replicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Components Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Model Selection (Retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the embedding model and the tokenizer for the architecture. The embedding model generates vector representations of the user query and knowledge base, enabling retrieval of semantically relevant context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "embeddings_model_path = \"ibm-granite/granite-embedding-30m-english\"\n",
    "embeddings_model = HuggingFaceEmbedding(\n",
    "    model_name=embeddings_model_path,\n",
    ")\n",
    "embeddings_tokenizer = AutoTokenizer.from_pretrained(embeddings_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Selection (Generator)\n",
    "\n",
    "The LLM will be the generator component that answers the user query, given the retrieved context. For this recipe, we connect to Granite 3.3 8B model using LlamaIndex-Replicate client.\n",
    "\n",
    "You can select other Granite models from the [`ibm-granite`](https://replicate.com/ibm-granite) org on Replicate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.replicate import Replicate\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "\n",
    "model_path = \"ibm-granite/granite-3.3-8b-instruct\"\n",
    "get_env_var('REPLICATE_API_TOKEN')\n",
    "\n",
    "model = Replicate(\n",
    "    model=model_path\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting the global parameters, we ensure consistency across the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = model\n",
    "Settings.embed_model = embeddings_model\n",
    "Settings.chunk_size = embeddings_tokenizer.max_len_single_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Database Selection (Knowledge Base)\n",
    "\n",
    "Identify the database to store and retrieve embedding vectors.\n",
    "In this recipe, we select ChromaDB to store our Knowledge Base. The storage of knowledge base in the form of vectors help in efficient similarity computation and relevant context retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"granite_rag_collection\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Building the Vector Database\n",
    "\n",
    "In this recipe, we take the State of the Union speech text, split it into chunks, derive embedding vectors using the embedding model, and load it into the vector database for querying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the document\n",
    "\n",
    "Here we use President Biden's State of the Union address from March 1, 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "\n",
    "filename = 'state_of_the_union.txt'\n",
    "url = 'https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/data/foundation_models/state_of_the_union.txt'\n",
    "\n",
    "if not os.path.isfile(filename):\n",
    "  wget.download(url, out=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the document into chunks\n",
    "\n",
    "Split the document into text segments that can fit into the model's context window.\n",
    "\n",
    "Please note that the chunk size is set to model's context window under the Global Settings section and is implicitly passed to SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files=[filename]).load_data()\n",
    "sentence_splitter = SentenceSplitter(chunk_overlap=0)\n",
    "\n",
    "nodes = sentence_splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "for idx, node in enumerate(nodes):\n",
    "    node.metadata[\"doc_id\"] = idx\n",
    "\n",
    "print(f\"{len(nodes)} text document chunks created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Populate the vector database\n",
    "\n",
    "NOTE: Population of the vector database may take over a minute depending on your embedding model and service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex(\n",
    "    nodes=nodes,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embeddings_model,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval from Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conduct a similarity search\n",
    "\n",
    "Search the knowledge base for similar documents by calculating the proximity of embedded vectors of the query and the documents in the vector space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What did the president say about Fortune 500 Corporations?\"\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=3)\n",
    "retrieval_results = retriever.retrieve(query)\n",
    "print(f\"{len(retrieval_results)} documents returned\")\n",
    "for i, node in enumerate(retrieval_results):\n",
    "    print(f\"\\nDocument {i+1} :\")\n",
    "    print(f\"\\nDocument ID : {node.metadata['doc_id']}\")\n",
    "    print(f\"\\nScore {i+1} : {node.score:.2f}\")\n",
    "    print(f\"\\nText:\\n {node.text}\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom RAG Query Engine\n",
    "\n",
    "This section outlines the process of building a custom LlamaIndex query engine for RAG using Granite models. The custom query engine operates in the following steps:\n",
    "\n",
    "1. **Document Retrieval** - The retriever identifies and fetches relevant documents based on the input query.\n",
    "\n",
    "2. **Prompt Construction** - The Granite chat prompt template is utilized to create a system prompt, integrating both the original query and the retrieved documents.\n",
    "\n",
    "3. **Response Generation** - An LLM (Granite in this recipe) generates a response by processing the formatted prompt, utilizing the context from the retrieved documents.\n",
    "\n",
    "The engine returns a Response object, which contains the generated output from the LLM, with the source nodes (documents used) incorporated into the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import CustomQueryEngine\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "from llama_index.core.llms import LLM\n",
    "from llama_index.core.base.response.schema import Response\n",
    "\n",
    "class RAGGraniteQueryEngine(CustomQueryEngine):\n",
    "    retriever: BaseRetriever\n",
    "    llm: LLM\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "\n",
    "    def custom_query(self, query_str: str):\n",
    "        docs = self.retriever.retrieve(query_str)\n",
    "\n",
    "        formatted_prompt = self.tokenizer.apply_chat_template(\n",
    "            conversation=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query_str\n",
    "            }],\n",
    "            documents=[{\n",
    "                \"doc_id\": node.metadata.get(\"doc_id\", \"\"),\n",
    "                \"text\": node.text,\n",
    "            } for node in docs],\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False\n",
    "        )\n",
    "\n",
    "        llm_response = self.llm.complete(formatted_prompt)\n",
    "        return Response(response=llm_response.text, source_nodes=docs)\n",
    "\n",
    "\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=3)\n",
    "\n",
    "query_engine = RAGGraniteQueryEngine(\n",
    "    retriever=retriever,\n",
    "    llm=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###  Query Engine Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The query is submitted to the query engine, and the resulting response is captured. This response contains both the response generated by the LLM and the relevant documents retrieved in relation to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_granite_community.notebook_utils import wrap_text\n",
    "\n",
    "query = \"What was said about Ketanji Brown Jackson's nomination to the Supreme Court?\"\n",
    "answer = query_engine.query(query)\n",
    "\n",
    "print(\"=== RAG Response ===\")\n",
    "print(wrap_text(answer.response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source documents that were identified as relevant context can be observed using the below code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n=== Source Documents ===\")\n",
    "for i, source_node in enumerate(answer.source_nodes):\n",
    "    doc_id = source_node.metadata.get('doc_id', 'N/A')\n",
    "    print(f\"\\nDocument {i+1} :\")\n",
    "    print(f\"\\nDocument ID : {doc_id}\")\n",
    "    print(f\"\\nScore {i+1} : {source_node.score:.2f}\")\n",
    "    print(f\"\\nText:\\n {source_node.text}\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries outside the scope of Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queries beyond the scope of the knowledge base will not be answered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"When was the last time Ferrari won the Formula 1 World Championship?\"\n",
    "answer = query_engine.query(query)\n",
    "\n",
    "print(\"=== RAG Response ===\")\n",
    "print(wrap_text(answer.response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In conclusion, this recipe demonstrates the implementation of a simple RAG architecture using the LlamaIndex orchestration layer and a knowledge base stored in ChromaDB. We utilized the LlamaIndex-Replicate client for the Granite language model and the LlamaIndex-HuggingFace client for Granite tokenizers. Additionally, we explored the process of building a customized query engine and leveraging the Granite prompt template to generate responses for RAG queries.\n",
    "\n",
    "For more recipes on RAG architectures, please refer [here](https://github.com/ibm-granite-community/granite-snack-cookbook/tree/main/recipes/RAG). You can also explore more on Agentic RAG in this [recipe](https://github.com/ibm-granite-community/granite-snack-cookbook/blob/main/recipes/AI-Agents/Agentic_RAG.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## References\n",
    "1. “What is retrieval-augmented generation?”. 2023. IBM Research Blog. https://research.ibm.com/blog/retrieval-augmented-generation-RAG.\n",
    "2. \"Basic Chat Template Examples\". 2025. IBM Granite Documentation. https://www.ibm.com/granite/docs/models/granite/#basic-chat-template-example. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
