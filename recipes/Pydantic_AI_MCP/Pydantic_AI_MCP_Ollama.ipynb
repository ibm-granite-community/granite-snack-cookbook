{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pydantic AI with MCP and Ollama using Granite Models\n",
    "\n",
    "**Author**: Vipul Mahajan(vipmaha1@in.ibm.com)\n",
    "\n",
    "This notebook demonstrates how to integrate [Pydantic AI](https://ai.pydantic.dev), [Model Context Protocol (MCP)](https://modelcontextprotocol.io), and [Ollama](https://ollama.ai) with IBM Granite models to build agents that use external tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "### Pydantic AI\n",
    "A Python agent framework for building production-grade AI applications with:\n",
    "- Type-safe development with Pydantic models\n",
    "- Structured, validated outputs\n",
    "- Model-agnostic design\n",
    "\n",
    "### Model Context Protocol (MCP)\n",
    "An open standard for connecting AI applications to external tools and data:\n",
    "- Standardized communication between models and services\n",
    "- Dynamic tool discovery\n",
    "- Reusable MCP servers across applications\n",
    "\n",
    "### Ollama\n",
    "Run LLMs locally on your machine:\n",
    "- Complete privacy (data never leaves your machine)\n",
    "- No API costs\n",
    "- Offline capability\n",
    "\n",
    "### What We'll Build\n",
    "1. MCP servers exposing tools (calculator, text analyzer, statistics)\n",
    "2. Pydantic AI agents that connect to MCP servers\n",
    "3. Single-tool and multi-tool agentic workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this notebook\n",
    "\n",
    "This notebook addresses a key challenge in building production-grade AI applications: how to enable language models to access external tools and data sources in a standardized, reusable way. \n",
    "\n",
    "Traditional approaches to tool calling often result in tightly-coupled code that's difficult to maintain and share across projects. The Model Context Protocol (MCP) solves this by providing an open standard for connecting AI applications to external services, while Pydantic AI ensures type-safe, validated interactions throughout your agent workflows.\n",
    "\n",
    "In this notebook, you'll learn how to:\n",
    "- Create MCP servers that expose tools (calculator, text analyzer, weather) using FastMCP\n",
    "- Build Pydantic AI agents that connect to MCP servers and use their tools\n",
    "- Implement both single-tool and multi-tool agentic workflows\n",
    "- Run everything locally using Ollama with IBM Granite models for complete privacy and no API costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "\n",
    "### 1. Ollama Installed and Running\n",
    "\n",
    "Install Ollama from [ollama.ai](https://ollama.ai) and pull the Granite model:\n",
    "\n",
    "```bash\n",
    "# Install Ollama (macOS/Linux)\n",
    "curl -fsSL https://ollama.ai/install.sh | sh\n",
    "\n",
    "# Pull IBM Granite model\n",
    "ollama pull granite3.3:8b\n",
    "\n",
    "# Verify it's running\n",
    "ollama list\n",
    "```\n",
    "\n",
    "### 2. Python Dependencies\n",
    "\n",
    "We'll install the required packages in the next cell:\n",
    "\n",
    "- `pydantic-ai-slim[mcp]` - Pydantic AI with MCP support\n",
    "- `fastmcp` - For creating MCP servers\n",
    "- `ollama` - Python Ollama client\n",
    "- `httpx` - Async HTTP client for MCP\n",
    "- Additional utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Let's install all required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"::group::Pip Install Dependencies\"\n",
    "%pip install \"git+https://github.com/ibm-granite-community/utils.git\" \\\n",
    "    \"pydantic-ai-slim[openai]\" \\\n",
    "    fastmcp \\\n",
    "    ollama \\\n",
    "    httpx \\\n",
    "    python-dotenv \\\n",
    "    mcp\n",
    "!echo \"::endgroup::\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "Let's set up our imports and verify Ollama connectivity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from typing import Optional, List, Dict, Any\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic_ai import Agent\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "import ollama\n",
    "import subprocess\n",
    "import sys\n",
    "from ibm_granite_community.notebook_utils import wrap_text\n",
    "\n",
    "print(\"üîç Detecting available Ollama models...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Get list of all models from Ollama\n",
    "    models_response = ollama.list()\n",
    "    \n",
    "    # Extract model names\n",
    "    all_models = []\n",
    "    granite_models = []\n",
    "    \n",
    "    model_list = models_response.get('models', []) if hasattr(models_response, 'get') else getattr(models_response, 'models', [])\n",
    "    \n",
    "    for model_obj in model_list:\n",
    "        \n",
    "        model_name = getattr(model_obj, 'model', None) or (model_obj.get('model') if isinstance(model_obj, dict) else str(model_obj))\n",
    "        \n",
    "        if model_name:\n",
    "            all_models.append(model_name)\n",
    "            if 'granite' in model_name.lower():\n",
    "                granite_models.append(model_name)\n",
    "    \n",
    "    # Select model\n",
    "    if granite_models:\n",
    "        print(f\"‚úÖ Ollama is running\")\n",
    "        print(f\"‚úÖ Found {len(granite_models)} Granite model(s):\")\n",
    "        for i, model in enumerate(granite_models):\n",
    "            print(f\"   [{i}] {model}\")\n",
    "        \n",
    "        # üëá CHANGE THIS INDEX TO SELECT A DIFFERENT MODEL\n",
    "        GRANITE_MODEL = granite_models[0]  # 0 = first model, 1 = second, etc.\n",
    "        \n",
    "        print(f\"\\nüéØ Using: {GRANITE_MODEL}\")\n",
    "        if len(granite_models) > 1:\n",
    "            print(f\"   (To change: edit cell and modify granite_models[0])\")\n",
    "        \n",
    "    elif all_models:\n",
    "        print(f\"‚ö†Ô∏è  No Granite models found\")\n",
    "        print(f\"‚úÖ Found {len(all_models)} other model(s):\")\n",
    "        for i, model in enumerate(all_models):\n",
    "            print(f\"   [{i}] {model}\")\n",
    "        \n",
    "        # üëá CHANGE THIS INDEX TO SELECT A DIFFERENT MODEL\n",
    "        GRANITE_MODEL = all_models[0]  # 0 = first model, 1 = second, etc.\n",
    "        \n",
    "        print(f\"\\nüéØ Using: {GRANITE_MODEL}\")\n",
    "        print(f\"   üí° Install Granite: ollama pull granite3.3:8b\")\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"No models found in Ollama\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"\\nüîß QUICK FIX:\")\n",
    "    print(\"   1. Open terminal ‚Üí Run: ollama serve\")\n",
    "    print(\"   2. Pull a model ‚Üí Run: ollama pull granite3.3:8b\")\n",
    "    print(\"   3. Re-run this cell\")\n",
    "    \n",
    "    # Check if models exist but server isn't running\n",
    "    try:\n",
    "        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, timeout=5)\n",
    "        if result.returncode == 0 and result.stdout:\n",
    "            print(\"\\n‚úÖ Models installed (server not running):\")\n",
    "            print(result.stdout)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    GRANITE_MODEL = \"granite3.3:latest\"  # Fallback\n",
    "    print(f\"\\n‚ö†Ô∏è  Using fallback: {GRANITE_MODEL}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Creating an MCP Server\n",
    "\n",
    "Let's start by creating a simple MCP server that exposes a few useful tools. We'll use FastMCP, which makes it easy to create MCP servers in Python.\n",
    "\n",
    "Our server will provide three tools:\n",
    "1. **Calculator** - Basic arithmetic operations\n",
    "2. **Text Analyzer** - Count words, characters, and analyze sentiment\n",
    "3. **Weather Info** - Simulated weather data (mock implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple MCP server using FastMCP\n",
    "# This will be saved as a separate Python file that runs as a subprocess\n",
    "\n",
    "import os\n",
    "\n",
    "# Get the directory where this notebook is located\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\")) if \"__file__\" in dir() else os.getcwd()\n",
    "if \"Pydantic_AI_MCP\" not in notebook_dir:\n",
    "    # If we're not in the recipe directory, navigate to it\n",
    "    notebook_dir = os.path.join(os.getcwd(), \"recipes\", \"Pydantic_AI_MCP\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(notebook_dir, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Working directory: {notebook_dir}\")\n",
    "\n",
    "mcp_server_code = '''\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "import math\n",
    "from typing import Literal\n",
    "\n",
    "# Initialize MCP server\n",
    "mcp = FastMCP(\"Granite Tools Server\")\n",
    "\n",
    "@mcp.tool()\n",
    "def calculator(operation: Literal[\"add\", \"subtract\", \"multiply\", \"divide\", \"power\", \"sqrt\"], a: float, b: float = 0) -> str:\n",
    "    \"\"\"Perform basic mathematical calculations.\n",
    "    \n",
    "    Args:\n",
    "        operation: The mathematical operation to perform\n",
    "        a: First number\n",
    "        b: Second number (not used for sqrt)\n",
    "    \n",
    "    Returns:\n",
    "        The result of the calculation as a string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if operation == \"add\":\n",
    "            result = a + b\n",
    "        elif operation == \"subtract\":\n",
    "            result = a - b\n",
    "        elif operation == \"multiply\":\n",
    "            result = a * b\n",
    "        elif operation == \"divide\":\n",
    "            if b == 0:\n",
    "                return \"Error: Division by zero\"\n",
    "            result = a / b\n",
    "        elif operation == \"power\":\n",
    "            result = a ** b\n",
    "        elif operation == \"sqrt\":\n",
    "            if a < 0:\n",
    "                return \"Error: Cannot take square root of negative number\"\n",
    "            result = math.sqrt(a)\n",
    "        else:\n",
    "            return f\"Error: Unknown operation {operation}\"\n",
    "        \n",
    "        return f\"Result: {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "@mcp.tool()\n",
    "def text_analyzer(text: str) -> str:\n",
    "    \"\"\"Analyze text and return statistics.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to analyze\n",
    "    \n",
    "    Returns:\n",
    "        A formatted string with text statistics\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chars = len(text)\n",
    "    chars_no_space = len(text.replace(\" \", \"\"))\n",
    "    sentences = text.count(\".\") + text.count(\"!\") + text.count(\"?\")\n",
    "    \n",
    "    # Simple sentiment analysis based on keywords\n",
    "    positive_words = [\"good\", \"great\", \"excellent\", \"amazing\", \"wonderful\", \"fantastic\", \"love\", \"best\"]\n",
    "    negative_words = [\"bad\", \"terrible\", \"awful\", \"horrible\", \"worst\", \"hate\", \"poor\"]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    positive_count = sum(1 for word in positive_words if word in text_lower)\n",
    "    negative_count = sum(1 for word in negative_words if word in text_lower)\n",
    "    \n",
    "    if positive_count > negative_count:\n",
    "        sentiment = \"Positive\"\n",
    "    elif negative_count > positive_count:\n",
    "        sentiment = \"Negative\"\n",
    "    else:\n",
    "        sentiment = \"Neutral\"\n",
    "    \n",
    "    return f\"\"\"Text Analysis:\n",
    "- Word count: {len(words)}\n",
    "- Character count: {chars}\n",
    "- Characters (no spaces): {chars_no_space}\n",
    "- Sentence count: {sentences}\n",
    "- Sentiment: {sentiment} (Positive: {positive_count}, Negative: {negative_count})\"\"\"\n",
    "\n",
    "@mcp.tool()\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get current weather information for a city (simulated).\n",
    "    \n",
    "    Args:\n",
    "        city: The name of the city\n",
    "    \n",
    "    Returns:\n",
    "        Weather information as a formatted string\n",
    "    \"\"\"\n",
    "    # This is a mock implementation - in production, you'd call a real weather API\n",
    "    import random\n",
    "    \n",
    "    conditions = [\"Sunny\", \"Cloudy\", \"Rainy\", \"Partly Cloudy\", \"Windy\"]\n",
    "    condition = random.choice(conditions)\n",
    "    temp = random.randint(50, 85)\n",
    "    humidity = random.randint(30, 80)\n",
    "    \n",
    "    return f\"\"\"Weather for {city}:\n",
    "- Condition: {condition}\n",
    "- Temperature: {temp}¬∞F\n",
    "- Humidity: {humidity}%\n",
    "- Note: This is simulated data for demonstration purposes\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run(transport=\"stdio\")\n",
    "'''\n",
    "\n",
    "# Save the MCP server code to a file\n",
    "server_path = os.path.join(notebook_dir, \"mcp_server.py\")\n",
    "with open(server_path, \"w\") as f:\n",
    "    f.write(mcp_server_code)\n",
    "\n",
    "print(f\"‚úÖ MCP server code saved to: {server_path}\")\n",
    "print(\"\\nServer provides the following tools:\")\n",
    "print(\"  1. calculator - Perform mathematical operations\")\n",
    "print(\"  2. text_analyzer - Analyze text statistics and sentiment\")\n",
    "print(\"  3. get_weather - Get weather information (simulated)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building a Pydantic AI Agent as MCP Client\n",
    "\n",
    "Now let's create a Pydantic AI agent that connects to our MCP server and uses its tools. \n",
    "\n",
    "**What you'll learn in this section:**\n",
    "- How to configure Pydantic AI with Ollama's OpenAI-compatible endpoint\n",
    "- How to connect an agent to an MCP server\n",
    "- How to implement a basic agentic loop with tool calling\n",
    "- How to handle single-tool workflows\n",
    "\n",
    "Pydantic AI doesn't have native Ollama support, but we can use Ollama's OpenAI-compatible API endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Ollama to work with Pydantic AI\n",
    "# Pydantic AI can use Ollama through OpenAI-compatible API\n",
    "import httpx\n",
    "from pydantic_ai.models import Model, KnownModelName\n",
    "import os\n",
    "\n",
    "# Set Ollama endpoint as OpenAI base URL\n",
    "os.environ['OPENAI_BASE_URL'] = 'http://localhost:11434/v1'\n",
    "os.environ['OPENAI_API_KEY'] = 'ollama'  # Dummy key for Ollama\n",
    "\n",
    "# Use openai: prefix which will use the custom base URL\n",
    "MODEL_NAME = f'openai:{GRANITE_MODEL}'\n",
    "\n",
    "print(f\"‚úÖ Configured model: {MODEL_NAME}\")\n",
    "print(f\"‚úÖ Using Ollama endpoint: http://localhost:11434/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper class to manage MCP server connection\n",
    "class MCPServerManager:\n",
    "    def __init__(self, server_script_path: str):\n",
    "        self.server_script_path = server_script_path\n",
    "        self.session = None\n",
    "        self.read_stream = None\n",
    "        self.write_stream = None\n",
    "        self._client_context = None\n",
    "    \n",
    "    async def start(self):\n",
    "        \"\"\"Start the MCP server and initialize connection.\"\"\"\n",
    "        server_params = StdioServerParameters(\n",
    "            command=sys.executable,  # Python executable\n",
    "            args=[self.server_script_path],\n",
    "        )\n",
    "        \n",
    "        self._client_context = stdio_client(server_params)\n",
    "        self.read_stream, self.write_stream = await self._client_context.__aenter__()\n",
    "        \n",
    "        self.session = ClientSession(self.read_stream, self.write_stream)\n",
    "        await self.session.__aenter__()\n",
    "        await self.session.initialize()\n",
    "        \n",
    "        # List available tools\n",
    "        tools_result = await self.session.list_tools()\n",
    "        tool_names = [tool.name for tool in tools_result.tools]\n",
    "        print(f\"üîß Connected to MCP server. Available tools: {tool_names}\")\n",
    "        \n",
    "        return tools_result.tools\n",
    "    \n",
    "    async def call_tool(self, tool_name: str, arguments: dict) -> str:\n",
    "        \"\"\"Call a tool on the MCP server.\"\"\"\n",
    "        result = await self.session.call_tool(tool_name, arguments)\n",
    "        \n",
    "        # Extract result text\n",
    "        if result.content:\n",
    "            if isinstance(result.content, list):\n",
    "                return \"\\n\".join(\n",
    "                    x.text if hasattr(x, \"text\") else str(x)\n",
    "                    for x in result.content\n",
    "                )\n",
    "            return str(result.content)\n",
    "        return \"Tool executed successfully\"\n",
    "    \n",
    "    async def close(self):\n",
    "        \"\"\"Clean up MCP server connection.\"\"\"\n",
    "        if self.session:\n",
    "            await self.session.__aexit__(None, None, None)\n",
    "        if self._client_context:\n",
    "            await self._client_context.__aexit__(None, None, None)\n",
    "        print(\"üîå Disconnected from MCP server\")\n",
    "\n",
    "print(\"‚úÖ MCPServerManager class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_agent_with_mcp_tools(user_query: str, max_iterations: int = 3):\n",
    "    \"\"\"\n",
    "    Simple agent that uses MCP tools to answer user queries.\n",
    "    \n",
    "    This is a minimal example showing the complete workflow:\n",
    "    1. Connect to MCP server and discover available tools\n",
    "    2. Create agent with system prompt describing tools\n",
    "    3. Parse agent responses for tool calls\n",
    "    4. Execute tools and feed results back to agent\n",
    "    5. Return final answer to user\n",
    "    \n",
    "    üí° Copy this function as a starting point for your own MCP agents!\n",
    "    \n",
    "    Args:\n",
    "        user_query: The question or task for the agent\n",
    "        max_iterations: Maximum number of agent turns (prevents infinite loops)\n",
    "    \n",
    "    Returns:\n",
    "        Final answer from the agent\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üîç Query: {user_query}\")\n",
    "    print(f\"üîç Using model: {GRANITE_MODEL}\\n\")\n",
    "    \n",
    "    # Step 1: Connect to MCP server\n",
    "    server_path = os.path.join(notebook_dir, \"mcp_server.py\")\n",
    "    mcp_manager = MCPServerManager(server_path)\n",
    "    tools = await mcp_manager.start()\n",
    "    \n",
    "    # Step 2: Build tool descriptions for the agent\n",
    "    tool_descriptions = []\n",
    "    for tool in tools:\n",
    "        # Extract parameter info from the tool schema\n",
    "        params_desc = []\n",
    "        if 'properties' in tool.inputSchema:\n",
    "            for param, details in tool.inputSchema['properties'].items():\n",
    "                param_type = details.get('type', 'any')\n",
    "                param_desc = details.get('description', '')\n",
    "                params_desc.append(f\"  - {param} ({param_type}): {param_desc}\")\n",
    "        \n",
    "        tool_desc = f\"**{tool.name}**: {tool.description}\\n\" + \"\\n\".join(params_desc)\n",
    "        tool_descriptions.append(tool_desc)\n",
    "    \n",
    "    tools_info = \"\\n\\n\".join(tool_descriptions)\n",
    "    \n",
    "    # Step 3: Create agent with improved system prompt\n",
    "    system_prompt = f\"\"\"You are a helpful assistant with access to tools.\n",
    "\n",
    "Available tools:\n",
    "{tools_info}\n",
    "\n",
    "MANDATORY RESPONSE FORMATS - You MUST follow these EXACTLY:\n",
    "\n",
    "FORMAT 1 - When calling a tool (use plain text, no markdown):\n",
    "TOOL: <tool_name>\n",
    "ARGS: <json_object>\n",
    "\n",
    "FORMAT 2 - When providing final answer:\n",
    "ANSWER: <your response>\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. NEVER mix TOOL/ARGS with ANSWER in the same response\n",
    "2. If a tool can help, respond with ONLY \"TOOL:\" and \"ARGS:\" lines (nothing else)\n",
    "3. ARGS must be a valid JSON object with curly braces: {{\"param\": \"value\"}}\n",
    "4. After you call a tool, you will receive its result in the next message\n",
    "5. When you have the tool's result, respond with ONLY the \"ANSWER:\" line\n",
    "6. Do NOT use markdown: write \"TOOL:\" not \"**TOOL**:\"\n",
    "7. Do NOT use markdown: write \"ANSWER:\" not \"**ANSWER**:\"\n",
    "8. Do NOT provide your own calculations or analysis - ALWAYS use the tool first\n",
    "\n",
    "WORKFLOW EXAMPLE (calculator):\n",
    "\n",
    "Turn 1 - User asks: \"What is 5 to the power of 4?\"\n",
    "Your response:\n",
    "TOOL: calculator\n",
    "ARGS: {{\"operation\": \"power\", \"a\": 5, \"b\": 4}}\n",
    "\n",
    "[System executes tool and returns: \"Result: 625\"]\n",
    "\n",
    "Turn 2 - You receive tool result\n",
    "Your response:\n",
    "ANSWER: 5 raised to the power of 4 equals 625\n",
    "\n",
    "WORKFLOW EXAMPLE (text_analyzer):\n",
    "\n",
    "Turn 1 - User asks: \"Analyze this text: 'Hello world'\"\n",
    "Your response:\n",
    "TOOL: text_analyzer\n",
    "ARGS: {{\"text\": \"Hello world\"}}\n",
    "\n",
    "[System executes tool and returns analysis]\n",
    "\n",
    "Turn 2 - You receive tool result  \n",
    "Your response:\n",
    "ANSWER: The text contains 2 words and 11 characters\n",
    "\n",
    "KEY POINTS:\n",
    "- In Turn 1: ONLY \"TOOL:\" and \"ARGS:\" (no answer, no explanation)\n",
    "- In Turn 2: ONLY \"ANSWER:\" (no tool call, no formatting)\n",
    "- Never guess the result - wait for the actual tool output\"\"\"\n",
    "\n",
    "    agent = Agent(MODEL_NAME, system_prompt=system_prompt)\n",
    "    \n",
    "    # Step 4: Agent loop\n",
    "    conversation_context = user_query\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        print(f\"--- Iteration {iteration + 1} ---\")\n",
    "        \n",
    "        # Get agent response\n",
    "        result = await agent.run(conversation_context)\n",
    "        \n",
    "        # Extract response text (handle different result formats)\n",
    "        if hasattr(result, 'data'):\n",
    "            response = result.data\n",
    "        elif hasattr(result, 'output'):\n",
    "            response = result.output  \n",
    "        elif hasattr(result, 'message'):\n",
    "            response = result.message\n",
    "        else:\n",
    "            response = str(result)\n",
    "        \n",
    "        print(f\"Agent: {response}\\n\")\n",
    "        \n",
    "        # Step 5: Clean up markdown formatting if present\n",
    "        # Handle both **TOOL**: and **ARGS**: with potential spaces/newlines\n",
    "        clean_response = response.replace(\"**TOOL:**\", \"TOOL:\").replace(\"**TOOL**:\", \"TOOL:\")\n",
    "        clean_response = clean_response.replace(\"**ARGS:**\", \"ARGS:\").replace(\"**ARGS**:\", \"ARGS:\")\n",
    "        clean_response = clean_response.replace(\"**ANSWER:**\", \"ANSWER:\").replace(\"**ANSWER**:\", \"ANSWER:\")\n",
    "        \n",
    "        # Check for mixed response (both TOOL and ANSWER in iteration 1)\n",
    "        if \"TOOL:\" in clean_response and \"ANSWER:\" in clean_response:\n",
    "            # Agent tried to provide answer before calling tool - extract only tool call\n",
    "            print(\"‚ö†Ô∏è  Agent included answer with tool call - extracting TOOL call only (will execute tool first)\")\n",
    "            clean_response = clean_response.split(\"ANSWER:\")[0]\n",
    "        \n",
    "        # Parse response for tool call or answer\n",
    "        if \"TOOL:\" in clean_response and \"ARGS:\" in clean_response:\n",
    "            # Extract tool call\n",
    "            lines = clean_response.strip().split('\\n')\n",
    "            tool_name = None\n",
    "            args_json = None\n",
    "            \n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if line.startswith(\"TOOL:\"):\n",
    "                    tool_name = line.replace(\"TOOL:\", \"\").strip()\n",
    "                elif line.startswith(\"ARGS:\"):\n",
    "                    args_json = line.replace(\"ARGS:\", \"\").strip()\n",
    "            \n",
    "            if tool_name and args_json:\n",
    "                try:\n",
    "                    args = json.loads(args_json)\n",
    "                    print(f\"üîß Calling tool: {tool_name}\")\n",
    "                    print(f\"üìã Arguments: {json.dumps(args, indent=2)}\")\n",
    "                    \n",
    "                    # Execute tool via MCP\n",
    "                    tool_result = await mcp_manager.call_tool(tool_name, args)\n",
    "                    print(f\"‚úÖ Tool Result: {tool_result}\\n\")\n",
    "                    \n",
    "                    # Clearer follow-up instruction with strict format requirement\n",
    "                    conversation_context = f\"\"\"The user asked: {user_query}\n",
    "\n",
    "You called the {tool_name} tool and received this result:\n",
    "{tool_result}\n",
    "\n",
    "Now respond with ONLY this format (nothing else):\n",
    "ANSWER: <your answer based on the tool result>\n",
    "\n",
    "Do NOT include any other text, formatting, or explanations. Just the single ANSWER: line.\"\"\"\n",
    "                    \n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"‚ùå Failed to parse tool arguments: {e}\")\n",
    "                    print(f\"   Raw args_json: {args_json}\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Tool execution failed: {e}\")\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"‚ùå Could not extract tool name or arguments\")\n",
    "                print(f\"   Tool: {tool_name}, Args: {args_json}\")\n",
    "                break\n",
    "                \n",
    "        elif \"ANSWER:\" in clean_response:\n",
    "            # Final answer received\n",
    "            answer = clean_response.split(\"ANSWER:\", 1)[1].strip()\n",
    "            print(f\"{'='*50}\")\n",
    "            print(f\"‚úÖ Final Answer: {answer}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            await mcp_manager.close()\n",
    "            return answer\n",
    "            \n",
    "        else:\n",
    "            # Response doesn't match expected format\n",
    "            print(f\"‚ö†Ô∏è  Agent response doesn't match expected format\")\n",
    "            print(f\"   Expected: 'TOOL: <name>' + 'ARGS: <json>' OR 'ANSWER: <text>'\")\n",
    "            print(f\"   Got: {clean_response[:200]}...\")\n",
    "            \n",
    "            # Try to extract answer if it's prose without \"ANSWER:\" prefix\n",
    "            if iteration > 0:  # Only in iteration 2+\n",
    "                print(f\"‚ÑπÔ∏è  Attempting to extract answer from prose response...\")\n",
    "                # Treat the whole response as the answer\n",
    "                print(f\"{'='*50}\")\n",
    "                print(f\"‚úÖ Final Answer: {clean_response.strip()}\")\n",
    "                print(f\"{'='*50}\")\n",
    "                await mcp_manager.close()\n",
    "                return clean_response.strip()\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    # Max iterations reached\n",
    "    print(f\"\\n‚ö†Ô∏è  Reached maximum iterations ({max_iterations})\")\n",
    "    await mcp_manager.close()\n",
    "    return \"Agent did not provide a final answer within iteration limit.\"\n",
    "\n",
    "print(\"‚úÖ run_agent_with_mcp_tools() function defined\")\n",
    "print(\"üí° This function handles single or multi-tool workflows\")\n",
    "print(\"üí° Copy this code to your projects and customize as needed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent with MCP Tools\n",
    "\n",
    "Now let's create an agent that can use our MCP server's tools. We'll manually handle tool calling since Pydantic AI's native MCP client support is still evolving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Calculator tool\n",
    "await run_agent_with_mcp_tools(\"What is 9 raised to the power of 3?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Text analyzer tool\n",
    "await run_agent_with_mcp_tools(\n",
    "    \"Analyze this text: 'IBM Granite models are fantastic! They provide excellent performance and amazing capabilities.'\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Weather tool\n",
    "await run_agent_with_mcp_tools(\"What's the weather like in San Francisco?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Advanced Multi-Tool Agent\n",
    "\n",
    "Now let's build an advanced agent that can coordinate multiple tools to accomplish complex tasks. This demonstrates a production-ready pattern where the agent:\n",
    "\n",
    "1. Analyzes the user's request\n",
    "2. Breaks it into sub-tasks (e.g., data analysis + formatting)\n",
    "3. Calls multiple tools sequentially\n",
    "4. Synthesizes the final answer\n",
    "\n",
    "### Example Workflow:\n",
    "```\n",
    "User: \"Analyze these sales numbers: 120, 450, 380, 290, 510 and format as markdown\"\n",
    "\n",
    "Step 1: Agent calls data_statistics ‚Üí Gets mean, median, etc.\n",
    "Step 2: Agent calls format_data ‚Üí Formats results as markdown\n",
    "Step 3: Agent provides formatted business report\n",
    "```\n",
    "\n",
    "Let's create the advanced MCP server with data analysis and formatting tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create advanced MCP server with data analysis and formatting tools\n",
    "advanced_mcp_server_code = '''\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "import statistics\n",
    "from typing import Literal\n",
    "\n",
    "# Initialize MCP server\n",
    "mcp = FastMCP(\"Advanced Analytics Server\")\n",
    "\n",
    "@mcp.tool()\n",
    "def data_statistics(numbers: str) -> str:\n",
    "    \"\"\"Calculate statistical measures for a list of numbers.\n",
    "    \n",
    "    Args:\n",
    "        numbers: Comma-separated list of numbers (e.g., \"10, 20, 30, 40\")\n",
    "    \n",
    "    Returns:\n",
    "        Statistical analysis including mean, median, mode, stdev\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse numbers\n",
    "        num_list = [float(x.strip()) for x in numbers.split(',')]\n",
    "        \n",
    "        if len(num_list) == 0:\n",
    "            return \"Error: No numbers provided\"\n",
    "        \n",
    "        # Calculate statistics\n",
    "        mean = statistics.mean(num_list)\n",
    "        median = statistics.median(num_list)\n",
    "        \n",
    "        # Mode (handle potential StatisticsError)\n",
    "        try:\n",
    "            mode = statistics.mode(num_list)\n",
    "        except statistics.StatisticsError:\n",
    "            mode = \"No unique mode\"\n",
    "        \n",
    "        # Standard deviation (requires at least 2 values)\n",
    "        if len(num_list) >= 2:\n",
    "            stdev = statistics.stdev(num_list)\n",
    "        else:\n",
    "            stdev = \"N/A (need 2+ values)\"\n",
    "        \n",
    "        # Min and max\n",
    "        minimum = min(num_list)\n",
    "        maximum = max(num_list)\n",
    "        \n",
    "        return f\"\"\"Statistics for {len(num_list)} numbers:\n",
    "- Mean (average): {mean}\n",
    "- Median: {median}\n",
    "- Mode: {mode}\n",
    "- Standard Deviation: {stdev}\n",
    "- Minimum: {minimum}\n",
    "- Maximum: {maximum}\n",
    "- Range: {maximum - minimum}\"\"\"\n",
    "        \n",
    "    except ValueError as e:\n",
    "        return f\"Error parsing numbers: {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "@mcp.tool()\n",
    "def format_data(data: str, format_type: Literal[\"markdown\", \"json\", \"csv\", \"plain\"]) -> str:\n",
    "    \"\"\"Format data in different output formats.\n",
    "    \n",
    "    Args:\n",
    "        data: The data to format (text with key-value pairs or statistics)\n",
    "        format_type: Output format (markdown, json, csv, or plain)\n",
    "    \n",
    "    Returns:\n",
    "        Formatted data string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if format_type == \"markdown\":\n",
    "            # Convert to markdown table or formatted text\n",
    "            lines = data.strip().split('\\\\n')\n",
    "            result = \"## Data Analysis Results\\\\n\\\\n\"\n",
    "            \n",
    "            for line in lines:\n",
    "                if ':' in line:\n",
    "                    key, value = line.split(':', 1)\n",
    "                    result += f\"**{key.strip()}:** {value.strip()}\\\\n\"\n",
    "                else:\n",
    "                    result += f\"{line}\\\\n\"\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        elif format_type == \"json\":\n",
    "            # Convert to JSON format\n",
    "            import json\n",
    "            lines = data.strip().split('\\\\n')\n",
    "            result_dict = {}\n",
    "            \n",
    "            for line in lines:\n",
    "                if ':' in line:\n",
    "                    key, value = line.split(':', 1)\n",
    "                    # Try to convert to number\n",
    "                    try:\n",
    "                        result_dict[key.strip()] = float(value.strip())\n",
    "                    except:\n",
    "                        result_dict[key.strip()] = value.strip()\n",
    "            \n",
    "            return json.dumps(result_dict, indent=2)\n",
    "            \n",
    "        elif format_type == \"csv\":\n",
    "            # Convert to CSV format\n",
    "            lines = data.strip().split('\\\\n')\n",
    "            result = \"Metric,Value\\\\n\"\n",
    "            \n",
    "            for line in lines:\n",
    "                if ':' in line:\n",
    "                    key, value = line.split(':', 1)\n",
    "                    result += f\"{key.strip()},{value.strip()}\\\\n\"\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        else:  # plain\n",
    "            return data\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Error formatting data: {e}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run(transport=\"stdio\")\n",
    "'''\n",
    "\n",
    "# Save the advanced MCP server\n",
    "advanced_server_path = os.path.join(notebook_dir, \"advanced_mcp_server.py\")\n",
    "with open(advanced_server_path, \"w\") as f:\n",
    "    f.write(advanced_mcp_server_code)\n",
    "\n",
    "print(f\"‚úÖ Advanced MCP server created: {advanced_server_path}\")\n",
    "print(\"\\\\nServer provides the following tools:\")\n",
    "print(\"  1. data_statistics - Calculate mean, median, mode, stdev, etc.\")\n",
    "print(\"  2. format_data - Format data as markdown, JSON, CSV, or plain text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the advanced agent with a complex query requiring multiple tools\n",
    "complex_query = \"\"\"I have these sales numbers: 120, 450, 380, 290, 510. \n",
    "Please analyze them and format the results as markdown.\"\"\"\n",
    "\n",
    "async def run_advanced_agent(user_query: str, max_iterations: int = 5):\n",
    "    \"\"\"\n",
    "    Advanced agent that coordinates multiple tools to accomplish complex tasks.\n",
    "    \n",
    "    This demonstrates a production-ready pattern:\n",
    "    1. Agent analyzes the query\n",
    "    2. Breaks it into sub-tasks (data analysis, formatting)\n",
    "    3. Calls multiple tools in sequence\n",
    "    4. Synthesizes final answer\n",
    "    \n",
    "    üí° This pattern works for: research agents, data pipelines, multi-step workflows\n",
    "    \n",
    "    Args:\n",
    "        user_query: Complex query requiring multiple tool calls\n",
    "        max_iterations: Maximum loop iterations (prevents runaway agents)\n",
    "    \n",
    "    Returns:\n",
    "        Final answer from the agent\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üîç Using model: {GRANITE_MODEL}\")\n",
    "    print(f\"üîç Model name configured: {MODEL_NAME}\\n\")\n",
    "    \n",
    "    # Step 1: Connect to MCP server\n",
    "    advanced_server_path = os.path.join(notebook_dir, \"advanced_mcp_server.py\")\n",
    "    mcp_manager = MCPServerManager(advanced_server_path)\n",
    "    tools = await mcp_manager.start()\n",
    "    \n",
    "    # Step 2: Build tool descriptions\n",
    "    tool_descriptions = []\n",
    "    for tool in tools:\n",
    "        params_desc = []\n",
    "        if 'properties' in tool.inputSchema:\n",
    "            for param, details in tool.inputSchema['properties'].items():\n",
    "                param_type = details.get('type', 'any')\n",
    "                param_desc = details.get('description', '')\n",
    "                params_desc.append(f\"  - {param} ({param_type}): {param_desc}\")\n",
    "        \n",
    "        tool_desc = f\"**{tool.name}**: {tool.description}\\n\" + \"\\n\".join(params_desc)\n",
    "        tool_descriptions.append(tool_desc)\n",
    "    \n",
    "    tools_info = \"\\n\\n\".join(tool_descriptions)\n",
    "    \n",
    "    # Step 3: Create system prompt\n",
    "    system_prompt = f\"\"\"You are a STRICT tool-calling assistant. Follow these rules EXACTLY.\n",
    "\n",
    "Available tools:\n",
    "{tools_info}\n",
    "\n",
    "MANDATORY WORKFLOW FOR THIS QUERY:\n",
    "The user wants data analyzed AND formatted. You MUST:\n",
    "1. First call data_statistics to get the actual statistics\n",
    "2. Then call format_data to format those statistics  \n",
    "3. Finally provide ANSWER with the formatted result\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. Call ONLY ONE tool per response\n",
    "2. WAIT for the tool result before proceeding\n",
    "3. Do NOT call multiple tools in one response\n",
    "4. Do NOT provide ANSWER until BOTH tools have been called\n",
    "5. Do NOT guess or calculate - use the actual tool results\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "TOOL: <exactly one tool name>\n",
    "ARGS: {{\"param\": \"value\"}}\n",
    "\n",
    "ABSOLUTELY FORBIDDEN:\n",
    "‚ùå Calling 2+ tools in one response\n",
    "‚ùå Including ANSWER before all tools are called\n",
    "‚ùå Guessing results without calling tools\n",
    "‚ùå Providing answers based on predicted results\n",
    "\n",
    "WORKFLOW EXAMPLE:\n",
    "Turn 1: \n",
    "TOOL: data_statistics\n",
    "ARGS: {{\"numbers\": \"...\"}}\n",
    "\n",
    "Turn 2 (after seeing result):\n",
    "TOOL: format_data\n",
    "ARGS: {{\"data\": \"<actual result from step 1>\", \"format_type\": \"markdown\"}}\n",
    "\n",
    "Turn 3 (after seeing formatted result):\n",
    "ANSWER: <the formatted markdown result>\n",
    "\n",
    "Remember: ONE tool per turn. WAIT for results. NO ANSWER until BOTH tools called.\"\"\"\n",
    "\n",
    "    agent = Agent(MODEL_NAME, system_prompt=system_prompt)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"User Query: {user_query}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    conversation_context = user_query\n",
    "    tool_results = []\n",
    "    tools_called = set()  # Track which tools have been called\n",
    "    \n",
    "    # Step 4: Agent loop with strict enforcement\n",
    "    for iteration in range(max_iterations):\n",
    "        print(f\"\\n--- Iteration {iteration + 1} ---\")\n",
    "        \n",
    "        # Get agent response\n",
    "        result = await agent.run(conversation_context)\n",
    "        \n",
    "        # Extract response text\n",
    "        if hasattr(result, 'data'):\n",
    "            response = result.data\n",
    "        elif hasattr(result, 'output'):\n",
    "            response = result.output\n",
    "        elif hasattr(result, 'message'):\n",
    "            response = result.message\n",
    "        else:\n",
    "            response = str(result)\n",
    "        \n",
    "        print(f\"Agent response:\\n{response}\\n\")\n",
    "        \n",
    "        # Step 5: Strict enforcement - block premature ANSWER\n",
    "        if \"ANSWER:\" in response:\n",
    "            # Check if both required tools were called\n",
    "            required_tools = {'data_statistics', 'format_data'}\n",
    "            if not required_tools.issubset(tools_called):\n",
    "                missing = required_tools - tools_called\n",
    "                print(f\"‚ö†Ô∏è  Agent tried to provide ANSWER too early!\")\n",
    "                print(f\"   Missing tool calls: {missing}\")\n",
    "                print(f\"   Forcing agent to call missing tools first...\\n\")\n",
    "                \n",
    "                # Force agent to call the missing tool\n",
    "                next_tool = list(missing)[0]\n",
    "                if next_tool == 'data_statistics':\n",
    "                    conversation_context = f\"\"\"You MUST call data_statistics first to get the actual statistics.\n",
    "\n",
    "Do NOT provide an answer yet. Respond with ONLY:\n",
    "TOOL: data_statistics\n",
    "ARGS: {{\"numbers\": \"120, 450, 380, 290, 510\"}}\"\"\"\n",
    "                else:\n",
    "                    conversation_context = f\"\"\"You have the statistics. Now you MUST call format_data to format them.\n",
    "\n",
    "Use the ACTUAL result from data_statistics:\n",
    "{tool_results[-1]['result'] if tool_results else ''}\n",
    "\n",
    "Respond with ONLY:\n",
    "TOOL: format_data\n",
    "ARGS: {{\"data\": \"<actual statistics>\", \"format_type\": \"markdown\"}}\"\"\"\n",
    "                continue\n",
    "            \n",
    "            # Both tools called - extract answer\n",
    "            answer_part = response.split(\"ANSWER:\", 1)[1]\n",
    "            if \"TOOL:\" in answer_part:\n",
    "                answer_part = answer_part.split(\"TOOL:\")[0]\n",
    "            \n",
    "            answer = answer_part.strip()\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"‚úÖ FINAL ANSWER:\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(answer)\n",
    "            break\n",
    "        \n",
    "        # Step 6: Parse tool call\n",
    "        elif \"TOOL:\" in response and \"ARGS:\" in response:\n",
    "            tool_name = None\n",
    "            args_json = None\n",
    "            \n",
    "            lines = response.split('\\n')\n",
    "            tool_line_idx = None\n",
    "            \n",
    "            # Find first TOOL: line\n",
    "            for idx, line in enumerate(lines):\n",
    "                line = line.strip()\n",
    "                if line.startswith(\"TOOL:\"):\n",
    "                    tool_name = line.replace(\"TOOL:\", \"\").strip()\n",
    "                    tool_line_idx = idx\n",
    "                    break\n",
    "            \n",
    "            # Find ARGS: after the TOOL: line\n",
    "            if tool_line_idx is not None:\n",
    "                remaining_lines = lines[tool_line_idx:]\n",
    "                response_segment = '\\n'.join(remaining_lines)\n",
    "                \n",
    "                # Look for next TOOL: to limit search\n",
    "                next_tool_idx = response_segment.find('\\nTOOL:', 1)\n",
    "                if next_tool_idx > 0:\n",
    "                    response_segment = response_segment[:next_tool_idx]\n",
    "                \n",
    "                # Find ARGS in this segment\n",
    "                if \"ARGS:\" in response_segment:\n",
    "                    args_start = response_segment.find(\"ARGS:\")\n",
    "                    after_args = response_segment[args_start + 5:].strip()\n",
    "                    \n",
    "                    # Extract JSON (handle multi-line)\n",
    "                    if after_args.startswith('{'):\n",
    "                        brace_count = 0\n",
    "                        json_end = 0\n",
    "                        for i, char in enumerate(after_args):\n",
    "                            if char == '{':\n",
    "                                brace_count += 1\n",
    "                            elif char == '}':\n",
    "                                brace_count -= 1\n",
    "                                if brace_count == 0:\n",
    "                                    json_end = i + 1\n",
    "                                    break\n",
    "                        \n",
    "                        if json_end > 0:\n",
    "                            args_json = after_args[:json_end]\n",
    "            \n",
    "            if tool_name and args_json:\n",
    "                # Check if agent tried multiple tools\n",
    "                tool_count = response.count('TOOL:')\n",
    "                if tool_count > 1:\n",
    "                    print(f\"‚ÑπÔ∏è  Agent mentioned {tool_count} tools, executing first: {tool_name}\\n\")\n",
    "                \n",
    "                try:\n",
    "                    args = json.loads(args_json)\n",
    "                    print(f\"üîß Calling tool: {tool_name}\")\n",
    "                    print(f\"üìã Arguments: {json.dumps(args, indent=2)}\")\n",
    "                    \n",
    "                    # Execute tool\n",
    "                    tool_result = await mcp_manager.call_tool(tool_name, args)\n",
    "                    print(f\"\\n‚úÖ Tool Result:\\n{tool_result}\\n\")\n",
    "                    \n",
    "                    tool_results.append({\n",
    "                        \"tool\": tool_name,\n",
    "                        \"result\": tool_result\n",
    "                    })\n",
    "                    tools_called.add(tool_name)\n",
    "                    \n",
    "                    # Build follow-up with specific instructions\n",
    "                    if 'data_statistics' in tools_called and 'format_data' not in tools_called:\n",
    "                        # Just called data_statistics - MUST call format_data next\n",
    "                        conversation_context = f\"\"\"You called data_statistics and got this result:\n",
    "{tool_result}\n",
    "\n",
    "Now you MUST call format_data to format these statistics as markdown.\n",
    "\n",
    "Respond with ONLY (use the ACTUAL result above):\n",
    "TOOL: format_data\n",
    "ARGS: {{\"data\": \"<paste the statistics result here>\", \"format_type\": \"markdown\"}}\n",
    "\n",
    "Do NOT provide ANSWER yet. Call format_data first.\"\"\"\n",
    "                    \n",
    "                    elif 'format_data' in tools_called:\n",
    "                        # Just called format_data - now can provide answer\n",
    "                        conversation_context = f\"\"\"You have called both required tools:\n",
    "\n",
    "1. data_statistics result:\n",
    "{tool_results[0]['result']}\n",
    "\n",
    "2. format_data result:\n",
    "{tool_results[1]['result']}\n",
    "\n",
    "Now provide the final answer using ONLY:\n",
    "ANSWER: <the formatted markdown result>\"\"\"\n",
    "                    \n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"‚ùå Failed to parse JSON: {e}\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Tool execution error: {e}\")\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"‚ùå Could not extract tool name or arguments\")\n",
    "                break\n",
    "        \n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Invalid response format\")\n",
    "            break\n",
    "    \n",
    "    # Cleanup\n",
    "    await mcp_manager.close()\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n\\nüìä Execution Summary:\")\n",
    "    print(f\"   - Model: {GRANITE_MODEL}\")\n",
    "    print(f\"   - Iterations: {iteration + 1}/{max_iterations}\")\n",
    "    print(f\"   - Tools called: {len(tool_results)}\")\n",
    "    for i, r in enumerate(tool_results, 1):\n",
    "        print(f\"     {i}. {r['tool']}\")\n",
    "\n",
    "# Test the advanced agent\n",
    "print(\"\\nüéØ Testing the advanced agent:\\n\")\n",
    "await run_advanced_agent(complex_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, you learned how to build production-grade AI agents by combining three powerful technologies: Pydantic AI for type-safe agent development, the Model Context Protocol (MCP) for standardized tool integration, and Ollama for local model deployment with IBM Granite models.\n",
    "\n",
    "You explored two key patterns:\n",
    "1. **Single-tool workflows** - Agents that call one tool at a time (calculator, text analyzer, weather)\n",
    "2. **Multi-tool workflows** - Advanced agents that coordinate multiple tools sequentially (data analysis + formatting)\n",
    "\n",
    "These patterns form the foundation for building complex agentic systems that can research information, process data pipelines, and execute multi-step workflows.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Now that you understand the basics, you can:\n",
    "- **Build custom MCP servers** - Add your own tools (database queries, API calls, file operations)\n",
    "- **Create specialized agents** - Develop agents for specific domains (research, data analysis, customer service)\n",
    "- **Scale to multi-agent systems** - Coordinate multiple agents working together on complex tasks\n",
    "- **Integrate with watsonx.ai** - Connect to IBM's cloud platform for enterprise-grade deployments\n",
    "\n",
    "### Related Resources\n",
    "\n",
    "Explore more AI agent patterns in the Granite Cookbook:\n",
    "- [AI Agents Recipes](https://github.com/ibm-granite-community/granite-snack-cookbook/tree/main/recipes/AI-Agents) - ReAct agents, Agentic RAG, and specialized agents\n",
    "- [Structured Response Recipe](https://github.com/ibm-granite-community/granite-snack-cookbook/tree/main/recipes/Structured_Response) - Type-safe outputs with Pydantic\n",
    "- [Function Calling Recipe](https://github.com/ibm-granite-community/granite-snack-cookbook/tree/main/recipes/Function-Calling) - Native function calling with Granite models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Official Documentation\n",
    "- [Pydantic AI Documentation](https://ai.pydantic.dev) - Complete guide to Pydantic AI framework\n",
    "- [Model Context Protocol (MCP)](https://modelcontextprotocol.io) - Official MCP specification and documentation\n",
    "- [Ollama](https://ollama.ai) - Run large language models locally\n",
    "- [FastMCP](https://github.com/jlowin/fastmcp) - Python framework for building MCP servers\n",
    "- [IBM Granite Models](https://www.ibm.com/granite) - IBM's family of open-source foundation models\n",
    "\n",
    "### Related Tutorials\n",
    "- [Pydantic AI Getting Started](https://ai.pydantic.dev/getting-started/) - Quick start guide for Pydantic AI\n",
    "- [MCP Servers Repository](https://github.com/modelcontextprotocol/servers) - Collection of example MCP servers\n",
    "- [Ollama Model Library](https://ollama.ai/library) - Browse available models for Ollama"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
