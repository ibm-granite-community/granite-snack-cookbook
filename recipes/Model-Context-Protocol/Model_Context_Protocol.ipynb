{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Model Context Protocol\n",
    "\n",
    "Modern AI applications face a fundamental challenge: how to securely and efficiently connect language models to the vast ecosystem of external data sources, APIs, and tools they need to be truly useful. Previously, developers would often build custom integrations for each external service, creating a fragmented landscape that is difficult to maintain, scale, and standardize. This fragmentation limits the potential of agentic AI systems, which require access to diverse external resources to perform complex tasks effectively.\n",
    "\n",
    "[Model Context Protocol (MCP)](https://modelcontextprotocol.io) addresses this challenge by providing a universal standard for AI-to-service communication. Rather than building point-to-point integrations, MCP establishes a common language that allows any AI model to connect to any compatible service through a standardized interface. This transforms agentic AI workflows by enabling models to dynamically discover and utilize external capabilitiesâ€”from databases and file systems to specialized APIs and knowledge graphsâ€”without requiring custom integration code for each service. The result is more flexible, maintainable, and powerful AI applications that can adapt to new tools and data sources as they become available.\n",
    "\n",
    "In this notebook, we will be exploring how to use connect [Granite models](https://www.ibm.com/granite) to MCP servers, enabling advanced agentic workflows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## MCP with Granite\n",
    "\n",
    "The **Model Context Protocol (MCP)** is an open standard that enables AI applications to securely connect to external data sources and tools. It consists of two main components:\n",
    "\n",
    "### MCP Server\n",
    "The **MCP Server** exposes resources, tools, and prompts that LLMs can access. Servers can provide access to databases, APIs, file systems, or any external service through a standardized interface.\n",
    "\n",
    "### MCP Client  \n",
    "The **MCP Client** connects to MCP servers and facilitates communication between your application and the server's capabilities. It handles:\n",
    "- Authentication and connection management\n",
    "- Resource discovery (what tools/resources are available)\n",
    "- Tool execution and data retrieval\n",
    "- Protocol-level communication with the server\n",
    "\n",
    "Together, these components will allow your application to connect Granite LLMs to external resources (servers) using a standardized MCP interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "First we will install the requisite packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/ibm-granite-community/utils \\\n",
    "    \"mcp[cli]\" \\\n",
    "    fastmcp \\\n",
    "    replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict, Any, List, Optional\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "import replicate\n",
    "import datetime\n",
    "from ibm_granite_community.notebook_utils import get_env_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Creating the MCP Client\n",
    "\n",
    "Let's start by building the MCP client. Our client needs a few main methods: \n",
    "\n",
    "- start(): Connects the LLM to the MCP server\n",
    "- close(): Cleans up server connections\n",
    "- create_system_prompt(): Provide the LLM access to the server's tools, resources, and prompts\n",
    "- parse_model_response(): Parse LLMs response and execute tool calls when provided\n",
    "\n",
    "We also provide additional chat method, which allows the model to call multiple tools in series to answer a users prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraniteMCPClient:\n",
    "    def __init__(self, replicate_model: str, server_command: str, server_args: Optional[List[str]] = None):\n",
    "        \"\"\"\n",
    "        Initialize the client with persistent MCP connection.\n",
    "        \n",
    "        Args:\n",
    "            replicate_model: The Replicate model identifier\n",
    "            server_command: Command to run MCP server (e.g., \"npx\")\n",
    "            server_args: Arguments for the server command\n",
    "        \"\"\"\n",
    "        self.model = replicate_model\n",
    "        self.replicate_client = replicate.Client(api_token=get_env_var(\"REPLICATE_API_TOKEN\"))\n",
    "        self.server_params = StdioServerParameters(\n",
    "            command=server_command,\n",
    "            args=server_args or [],\n",
    "            env=os.environ.copy()\n",
    "        )\n",
    "        self.session = None\n",
    "        self.read_stream = None\n",
    "        self.write_stream = None\n",
    "        self._client_context = None\n",
    "\n",
    "    async def start(self):\n",
    "        \"\"\"Initialize the persistent MCP connection\"\"\"\n",
    "        self._client_context = stdio_client(self.server_params)\n",
    "        self.read_stream, self.write_stream = await self._client_context.__aenter__()\n",
    "        \n",
    "        self.session = ClientSession(self.read_stream, self.write_stream)\n",
    "        await self.session.__aenter__()\n",
    "        await self.session.initialize()\n",
    "        \n",
    "        # Get and log available tools\n",
    "        tools_result = await self.session.list_tools()\n",
    "        available_tools = tools_result.tools\n",
    "        print(f\"ðŸ”§ Connected to MCP server. Available tools: {[tool.name for tool in available_tools]}\")\n",
    "\n",
    "    async def close(self):\n",
    "        \"\"\"Clean up MCP connections\"\"\"\n",
    "        if self.session:\n",
    "            await self.session.__aexit__(None, None, None)\n",
    "        if self._client_context:\n",
    "            await self._client_context.__aexit__(None, None, None)\n",
    "\n",
    "    def create_system_prompt(self, custom_system_prompt: Optional[str], tools: List) -> str:\n",
    "        \"\"\"Create system prompt combining custom prompt with tool information.\"\"\"\n",
    "        # Default system prompt if none provided\n",
    "        if custom_system_prompt is None:\n",
    "            custom_system_prompt = \"You are a helpful assistant.\"\n",
    "        \n",
    "        if not tools:\n",
    "            return f\"<|start_of_role|>system<|end_of_role|>{custom_system_prompt}<|end_of_text|>\"\n",
    "        \n",
    "        # Convert MCP tool definitions to the expected format\n",
    "        tools_info = []\n",
    "        for tool in tools:\n",
    "            # Convert MCP inputSchema to the expected arguments format\n",
    "            arguments = {}\n",
    "            if tool.inputSchema and 'properties' in tool.inputSchema:\n",
    "                for prop_name, prop_info in tool.inputSchema['properties'].items():\n",
    "                    arguments[prop_name] = {\n",
    "                        \"description\": prop_info.get('description', f\"The {prop_name} parameter\")\n",
    "                    }\n",
    "            \n",
    "            tool_info = {\n",
    "                \"name\": tool.name,\n",
    "                \"description\": tool.description,\n",
    "                \"arguments\": arguments\n",
    "            }\n",
    "            tools_info.append(tool_info)\n",
    "        \n",
    "        # Create the system prompt combining custom prompt with tool information\n",
    "        tools_json = json.dumps(tools_info, indent=4)\n",
    "        \n",
    "        prompt = f\"\"\"<|start_of_role|>system<|end_of_role|>Knowledge Cutoff Date: April 2024.\n",
    "Today's Date: {datetime.date.today().strftime(\"%B %d %Y\")}. {custom_system_prompt} You have access to the following tools. When a tool is required to answer the user's query, respond only with <|tool_call|> followed by a JSON list of tools used. If a tool does not exist in the provided list of tools, notify the user that you do not have the ability to fulfill the request. <|end_of_text|>\n",
    "<|start_of_role|>available_tools<|end_of_role|>{tools_json}<|end_of_text|>\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "\n",
    "    def parse_model_response(self, response: str) -> Optional[List[Dict[str, Any]]]:\n",
    "        \"\"\"Parse model response to extract tool calls\"\"\"\n",
    "        try:\n",
    "            response = response.strip()\n",
    "            \n",
    "            # Check for <|tool_call|> format first\n",
    "            if '<|tool_call|>' in response:\n",
    "                tool_call_start = response.find('<|tool_call|>') + len('<|tool_call|>')\n",
    "                json_part = response[tool_call_start:].strip()\n",
    "                if '<|end_of_text|>' in json_part:\n",
    "                    json_part = json_part[:json_part.find('<|end_of_text|>')].strip()\n",
    "            # If no marker, check if the entire response is a JSON array\n",
    "            elif response.startswith('[') and response.endswith(']'):\n",
    "                json_part = response\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "            json_part = json_part.replace('\\\\\"', '\"')\n",
    "\n",
    "            tool_calls = json.loads(json_part)\n",
    "            if not isinstance(tool_calls, list):\n",
    "                tool_calls = [tool_calls]\n",
    "            return tool_calls\n",
    "            \n",
    "        except (json.JSONDecodeError, ValueError):\n",
    "            return None\n",
    "\n",
    "    async def chat(self, user_message: str, system_prompt: Optional[str] = None) -> str:\n",
    "        \"\"\"\n",
    "        Chat with the model using the persistent MCP connection with agentic tool calling loop\n",
    "        \n",
    "        Args:\n",
    "            user_message: The user's message\n",
    "            system_prompt: Custom system prompt to use (optional)\n",
    "            \n",
    "        Returns:\n",
    "            The final response from the model\n",
    "        \"\"\"\n",
    "        if not self.session:\n",
    "            raise RuntimeError(\"Client not started. Call start() first.\")\n",
    "        \n",
    "        try:\n",
    "            # Get available tools from the persistent session\n",
    "            tools_result = await self.session.list_tools()\n",
    "            available_tools = tools_result.tools\n",
    "            \n",
    "            # Build the initial prompt with system instructions and user message\n",
    "            system_prompt_text = self.create_system_prompt(system_prompt, available_tools)\n",
    "            current_prompt = f\"{system_prompt_text}\\n<|start_of_role|>user<|end_of_role|>{user_message}<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>\"\n",
    "            \n",
    "            max_tool_executions = 5\n",
    "            tool_execution_count = 0\n",
    "            last_response = \"\"\n",
    "            \n",
    "            # Agentic tool calling loop\n",
    "            while tool_execution_count < max_tool_executions:\n",
    "                # Call the model\n",
    "                response = self.replicate_client.run(\n",
    "                    self.model,\n",
    "                    input={\n",
    "                        \"prompt\": current_prompt,\n",
    "                        \"max_tokens\": 4096,\n",
    "                        \"temperature\": 0.1,\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Handle streaming responses\n",
    "                if hasattr(response, '__iter__') and not isinstance(response, str):\n",
    "                    response_text = \"\".join(str(chunk) for chunk in response)\n",
    "                else:\n",
    "                    response_text = str(response)\n",
    "                \n",
    "                print(response_text)\n",
    "                last_response = response_text\n",
    "                \n",
    "                # Check if the model wants to call tools\n",
    "                tool_calls = self.parse_model_response(response_text)\n",
    "                \n",
    "                if not tool_calls:\n",
    "                    # No tool calls, exit the loop\n",
    "                    break\n",
    "                    \n",
    "                print(f\"ðŸ§  Model wants to use {len(tool_calls)} tool(s)\")\n",
    "                \n",
    "                # Execute all tool calls\n",
    "                tool_results = []\n",
    "                for tool_call in tool_calls:\n",
    "                    tool_name = tool_call['name']\n",
    "                    tool_args = tool_call.get('arguments', {})\n",
    "                    \n",
    "                    print(f\"   â€¢ Calling {tool_name}({tool_args})\")\n",
    "                    \n",
    "                    # Call the tool via the persistent session\n",
    "                    result = await self.session.call_tool(tool_name, tool_args)\n",
    "                    print(result)\n",
    "                    \n",
    "                    # Extract result text\n",
    "                    if result.content:\n",
    "                        if isinstance(result.content, list):\n",
    "                            result_text = \"\\n\".join(\n",
    "                                item.text if hasattr(item, 'text') else str(item)\n",
    "                                for item in result.content\n",
    "                            )\n",
    "                        else:\n",
    "                            result_text = str(result.content)\n",
    "                    else:\n",
    "                        result_text = \"Tool executed successfully\"\n",
    "                    \n",
    "                    tool_results.append(result_text)\n",
    "                \n",
    "                # Build tool response and append to context\n",
    "                tool_response_text = json.dumps(tool_results)\n",
    "                current_prompt = f\"{current_prompt}{response_text}\\n<|start_of_role|>tool_response<|end_of_role|>{tool_response_text}<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>\"\n",
    "                \n",
    "                tool_execution_count += 1\n",
    "            \n",
    "            # If the last response contained tool calls (hit max executions), give one final chance\n",
    "            if self.parse_model_response(last_response):\n",
    "                print(\"ðŸ”„ Max tool executions reached, giving final response opportunity\")\n",
    "                \n",
    "                final_response = self.replicate_client.run(\n",
    "                    self.model,\n",
    "                    input={\n",
    "                        \"prompt\": current_prompt,\n",
    "                        \"max_tokens\": 1024,\n",
    "                        \"temperature\": 0.7,\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                if hasattr(final_response, '__iter__') and not isinstance(final_response, str):\n",
    "                    return \"\".join(str(chunk) for chunk in final_response)\n",
    "                else:\n",
    "                    return str(final_response)\n",
    "            \n",
    "            # Return the last response (which had no tool calls)\n",
    "            return last_response\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Connecting MCP Servers\n",
    "Now that we have built an MCP client, we can now connect our LLM to any MCP server of our choice! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Memory\n",
    "\n",
    "Let's connect our model to an external memory base, that allows it to store and retrieve information about the user, even if it was stated in a previous conversation it no longer has access to.\n",
    "\n",
    "We will be using the [Official MCP Filesystem Server](\"https://github.com/modelcontextprotocol/servers/tree/main/src/filesystem\"). This enables our model to create, read, edit, and remove files in a specified directory. Let's first connect our server to the MCP Client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "replicate_model = \"ibm-granite/granite-3.3-8b-instruct\"\n",
    "server_command = \"npx\"\n",
    "server_args = [\"-y\", \"@modelcontextprotocol/server-filesystem\", \".\"]\n",
    "\n",
    "client = GraniteMCPClient(replicate_model, server_command, server_args)\n",
    "\n",
    "await client.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Now let's write some instructions to tell the model how to store, edit and retrieve memories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You are an AI assistant with persistent memory capabilities. You can store and retrieve information about users across conversations using a memory.txt file.\n",
    "\n",
    "ALWAYS start every response by using read_file with path \"memory.txt\" to check for relevant user information. If the read_file call fails because memory.txt doesn't exist, immediately use write_file with path \"memory.txt\" and content starting with \"1. \" to create it.\n",
    "\n",
    "WHENEVER the user shares information about themselves, use read_file first to get current contents, then use write_file with path \"memory.txt\" to save the updated memory list. When adding new information, append it as a new numbered entry to the existing list - do not overwrite existing entries.\n",
    "\n",
    "When the user provides new information, use read_file to get the current memory contents, then use write_file to save the complete updated numbered list with the new information added to the end.\n",
    "\n",
    "If the user disputes information or asks to delete a memory, use read_file to get current memories, remove that specific numbered entry, renumber the remaining list, then use write_file to save the updated list. If they provide information that conflicts with existing memories, use read_file first, then use write_file to replace the old memory with the new one in the complete updated list.\n",
    "\n",
    "Always use read_file on memory.txt first when the user sends a message and reference relevant memories naturally in your responses when they provide helpful context. Don't mention irrelevant memories and don't announce when you're reading or writing to memory unless specifically asked. When the user says \"forget that\" or \"delete that memory\" use read_file then write_file to remove the specified memory. When they ask \"what do you remember about me\" use read_file to get memories and summarize them. Be natural and conversational when using stored information to provide more personalized responses.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Now we can start a conversation with our memory-powered AI assistant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"Hello Granite, I am a AI engineer at IBM research. I have a dog named Sunday. She is a border collie and she is 13 years old.\"\n",
    "\n",
    "prompt = f\"{instructions} Users message: {message}\"\n",
    "\n",
    "response = await client.chat(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "The LLM will store this information in it's memory bank. Now we can open up a new chat, and ask it a question about this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"What is my dog's name?\"\n",
    "\n",
    "prompt = f\"{instructions} Users message: {message}\"\n",
    "\n",
    "response = await client.chat(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how MCP bridges the gap between language models and external resources, creating a foundation for more sophisticated agentic AI systems. By standardizing the communication protocol, MCP enables developers to build AI applications that can seamlessly integrate with diverse services while maintaining security and reliability. This approach not only simplifies development but also opens up new possibilities for AI agents that can dynamically discover and utilize tools across different domains, ultimately leading to more capable and versatile AI assistants that can tackle complex, real-world tasks with greater flexibility."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
