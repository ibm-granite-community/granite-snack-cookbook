{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Model Context Protocol\n",
    "\n",
    "Modern AI applications face a fundamental challenge: how to securely and efficiently connect language models to the vast ecosystem of external data sources, APIs, and tools they need to be truly useful. Previously, developers would often build custom integrations for each external service, creating a fragmented landscape that is difficult to maintain, scale, and standardize. This fragmentation limits the potential of agentic AI systems, which require access to diverse external resources to perform complex tasks effectively.\n",
    "\n",
    "[Model Context Protocol (MCP)](https://modelcontextprotocol.io) addresses this challenge by providing a universal standard for AI-to-service communication. Rather than building point-to-point integrations, MCP establishes a common language that allows any AI model to connect to any compatible service through a standardized interface. This transforms agentic AI workflows by enabling models to dynamically discover and utilize external capabilities, without requiring custom integration code for each service. The result is more flexible, maintainable, and powerful AI applications that can adapt to new tools and data sources as they become available.\n",
    "\n",
    "In this notebook, we will be exploring how to connect [Granite models](https://www.ibm.com/granite) to MCP servers, enabling advanced agentic workflows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## MCP with Granite\n",
    "\n",
    "The **Model Context Protocol (MCP)** is an open standard that enables AI applications to securely connect to external data sources and tools. It consists of two main components:\n",
    "\n",
    "### MCP Server\n",
    "The **MCP Server** exposes resources, tools, and prompts that LLMs can access. Servers can provide access to databases, APIs, file systems, or any external service through a standardized interface.\n",
    "\n",
    "### MCP Client  \n",
    "The **MCP Client** connects to MCP servers and facilitates communication between your application and the server's capabilities. It handles:\n",
    "- Authentication and connection management\n",
    "- Resource discovery (what tools/resources are available)\n",
    "- Tool execution and data retrieval\n",
    "- Protocol-level communication with the server\n",
    "\n",
    "Together, these components will allow your application to connect Granite LLMs to external resources (servers) using a standardized MCP interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "First we will install the requisite packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/ibm-granite-community/utils langchain langgraph langchain-mcp-adapters langchain_core langchain-community replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Replicate\n",
    "from langchain_core.messages import AIMessage, ToolMessage, ToolCall\n",
    "from langchain_core.prompts import (\n",
    "    MessagesPlaceholder,\n",
    "    HumanMessagePromptTemplate,\n",
    "    PromptTemplate,\n",
    ")\n",
    "from langchain_core.utils.json import parse_json_markdown\n",
    "from ibm_granite_community.langchain import TokenizerChatPromptTemplate\n",
    "from transformers import AutoTokenizer\n",
    "import os, datetime\n",
    "from typing import Dict, Any, List, Optional\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Creating the MCP Client\n",
    "\n",
    "Let's start by building the MCP client. Our client needs a few main methods: \n",
    "\n",
    "- start(): Connects the LLM to the MCP server\n",
    "- close(): Cleans up server connections\n",
    "- create_tool_prompt(): Provide the LLM access to the server's tools, resources, and prompts\n",
    "- parse_model_response(): Parse LLMs response and execute tool calls when provided\n",
    "\n",
    "We also provide additional chat method, which allows the model to call multiple tools in series to answer a users prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraniteMCPClient:\n",
    "    def __init__(self, replicate_model: str, server_command: str, server_args: Optional[List[str]] = None):\n",
    "        \"\"\"\n",
    "        Initialize the client with persistent MCP connection.\n",
    "        \n",
    "        Args:\n",
    "            replicate_model: The Replicate model identifier\n",
    "            server_command: Command to run MCP server (e.g., \"npx\")\n",
    "            server_args: Arguments for the server command\n",
    "        \"\"\"\n",
    "        self.model = replicate_model\n",
    "\n",
    "        self.llm = Replicate(\n",
    "            model=replicate_model,\n",
    "            model_kwargs={\"max_tokens\": 4096, \"temperature\": 0.1},\n",
    "        )\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(replicate_model, trust_remote_code=True)\n",
    "\n",
    "        self.server_params = StdioServerParameters(\n",
    "            command=server_command,\n",
    "            args=server_args or [],\n",
    "            env=os.environ.copy(),\n",
    "        )\n",
    "        self.session = self.read_stream = self.write_stream = self._client_context = None\n",
    "\n",
    "    async def start(self):\n",
    "        \"\"\"Initialize the persistent MCP connection\"\"\"\n",
    "        self._client_context = stdio_client(self.server_params)\n",
    "        self.read_stream, self.write_stream = await self._client_context.__aenter__()\n",
    "        \n",
    "        self.session = ClientSession(self.read_stream, self.write_stream)\n",
    "        await self.session.__aenter__()\n",
    "        await self.session.initialize()\n",
    "        \n",
    "        # Get and log available tools\n",
    "        tools_result = await self.session.list_tools()\n",
    "        available_tools = tools_result.tools\n",
    "        print(f\"ðŸ”§ Connected to MCP server. Available tools: {[tool.name for tool in available_tools]}\")\n",
    "\n",
    "    async def close(self):\n",
    "        \"\"\"Clean up MCP connections\"\"\"\n",
    "        if self.session:\n",
    "            await self.session.__aexit__(None, None, None)\n",
    "        if self._client_context:\n",
    "            await self._client_context.__aexit__(None, None, None)\n",
    "\n",
    "    def create_tool_prompt(\n",
    "        self,\n",
    "        tools: List,\n",
    "    ) -> str:\n",
    "        \"\"\"Return plain-text JSON block that enumerates the MCP tools.\n",
    "\n",
    "        The calling code will insert this string into its own\n",
    "        TokenizerChatPromptTemplate (`{tools_block}` placeholder),\n",
    "        so we deliberately include *nothing* except the JSON.\n",
    "        \"\"\"\n",
    "        if not tools:\n",
    "            return \"[]\"\n",
    "\n",
    "        tools_info = []\n",
    "        for tool in tools:\n",
    "            arguments = {}\n",
    "            if tool.inputSchema and \"properties\" in tool.inputSchema:\n",
    "                for name, meta in tool.inputSchema[\"properties\"].items():\n",
    "                    arguments[name] = {\n",
    "                        \"description\": meta.get(\"description\", f\"The {name} parameter\")\n",
    "                    }\n",
    "\n",
    "            tools_info.append(\n",
    "                {\n",
    "                    \"name\": tool.name,\n",
    "                    \"description\": tool.description,\n",
    "                    \"arguments\": arguments,\n",
    "                }\n",
    "            )\n",
    "            \n",
    "        return tools_info\n",
    "\n",
    "    def parse_model_response(self, response: str) -> Optional[List[Dict[str, Any]]]:\n",
    "        \"\"\"Parse model response to extract tool calls\"\"\"\n",
    "        try:\n",
    "            # If Granite used its special tag, isolate the JSON segment first\n",
    "            if \"<|tool_call|>\" in response:\n",
    "                json_blob = response.split(\"<|tool_call|>\", 1)[1]\n",
    "                # Trim any trailing role tags\n",
    "                if \"<|\" in json_blob:\n",
    "                    json_blob = json_blob.split(\"<|\", 1)[0]\n",
    "            else:\n",
    "                json_blob = response\n",
    "\n",
    "            tool_calls = parse_json_markdown(json_blob)\n",
    "            if tool_calls is None:\n",
    "                return None\n",
    "            if isinstance(tool_calls, dict):\n",
    "                tool_calls = [tool_calls]\n",
    "            return tool_calls\n",
    "        except Exception:\n",
    "            return None\n",
    "        \n",
    "    async def chat(     \n",
    "        self,\n",
    "        user_message: str,\n",
    "        system_prompt: str | None = None,\n",
    "        *,\n",
    "        max_tool_executions: int = 5,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Agent loop for Granite + MCP.\n",
    "\n",
    "        â€£ Prints the full prompt, raw model output, parsed tool calls,\n",
    "          and tool responses each turn.\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.session:\n",
    "            raise RuntimeError(\"Client not started. Call start() first.\")\n",
    "        \n",
    "        if system_prompt is None:\n",
    "            system_prompt = (\n",
    "                f\"Knowledge Cutoff Date: April 2024.\\n\"\n",
    "                f\"Today's Date: {datetime.date.today():%B %d, %Y}.\\n\"\n",
    "                \"You are a helpful assistant. \"\n",
    "                \"When a tool is required, reply ONLY with <|tool_call|> \"\n",
    "                \"followed by a JSON array.\"\n",
    "            )\n",
    "\n",
    "        prompt_template = TokenizerChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", \"{sys}\"),\n",
    "                HumanMessagePromptTemplate(\n",
    "                    prompt=PromptTemplate.from_template(\"{query}\")\n",
    "                ),\n",
    "                MessagesPlaceholder(\"tool_results\", optional=True),\n",
    "            ],\n",
    "            tokenizer=self.tokenizer,\n",
    "        )\n",
    "\n",
    "        tools_schema = self.create_tool_prompt(\n",
    "            (await self.session.list_tools()).tools\n",
    "        )  # list[dict] JSON schema objects\n",
    "\n",
    "        tool_results_msgs: list = []             # running AI / Tool messages\n",
    "        query_text = user_message                # first human turn\n",
    "        turn = 0\n",
    "\n",
    "        while turn < max_tool_executions:\n",
    "            turn += 1\n",
    "            prompt_text = prompt_template.format_prompt(\n",
    "                sys=system_prompt,\n",
    "                query=query_text,\n",
    "                tools=tools_schema,\n",
    "                tool_results=tool_results_msgs,\n",
    "            ).text\n",
    "\n",
    "            # Call Granite via LangChain-Replicate wrapper\n",
    "            response_text = str(self.llm.invoke(prompt_text))\n",
    "\n",
    "            print(\"\\n========== MODEL RAW RESPONSE ==========\")\n",
    "            print(response_text)\n",
    "            print(\"========================================\")\n",
    "\n",
    "            tool_calls = self.parse_model_response(response_text)\n",
    "\n",
    "            print(\"\\nParsed tool calls:\", tool_calls)\n",
    "\n",
    "            # Natural-language answer â†’ exit loop\n",
    "            if not tool_calls:\n",
    "                return response_text\n",
    "\n",
    "            # Execute each tool call\n",
    "            for idx, call_dict in enumerate(tool_calls, start=1):\n",
    "                name = call_dict[\"name\"]\n",
    "                args_dict = call_dict.get(\"arguments\", {})\n",
    "                print(f\"\\n>>> Executing tool {name} {args_dict}\")\n",
    "\n",
    "                result = await self.session.call_tool(name, args_dict)\n",
    "\n",
    "                # Flatten MCP result to string\n",
    "                if result.content:\n",
    "                    if isinstance(result.content, list):\n",
    "                        result_text = \"\\n\".join(\n",
    "                            x.text if hasattr(x, \"text\") else str(x)\n",
    "                            for x in result.content\n",
    "                        )\n",
    "                    else:\n",
    "                        result_text = str(result.content)\n",
    "                else:\n",
    "                    result_text = \"Tool executed successfully\"\n",
    "\n",
    "                print(\"Tool output -->\", result_text[:300], \"...\\n\")\n",
    "\n",
    "                # Record messages for next prompt\n",
    "                tc_obj = ToolCall(name=name, args=args_dict, id=str(idx))\n",
    "                tool_results_msgs.append(\n",
    "                    AIMessage(content=response_text, tool_calls=[tc_obj])\n",
    "                )\n",
    "                tool_results_msgs.append(\n",
    "                    ToolMessage(content=result_text, tool_call_id=idx)\n",
    "                )\n",
    "\n",
    "            # After first round the human has no new text; keep placeholder only\n",
    "            query_text = \" \"\n",
    "\n",
    "        # If we hit max_tool_executions without a final answer\n",
    "        prompt_text = prompt_template.format_prompt(\n",
    "            sys=system_prompt,\n",
    "            query=\" \",                       # no new user input\n",
    "            tools=tools_schema,\n",
    "            tool_results=tool_results_msgs,  # full history of tool use\n",
    "        ).text\n",
    "\n",
    "        final_answer = str(self.llm.invoke(prompt_text))\n",
    "\n",
    "        print(final_answer)\n",
    "\n",
    "        return final_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Connecting MCP Servers\n",
    "Now that we have built an MCP client, we can now connect our LLM to any MCP server of our choice! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Memory\n",
    "\n",
    "Let's connect our model to an external memory base, that allows it to store and retrieve information about the user, even if it was stated in a previous conversation it no longer has access to.\n",
    "\n",
    "We will be using the [Official MCP Filesystem Server](\"https://github.com/modelcontextprotocol/servers/tree/main/src/filesystem\"). This enables our model to create, read, edit, and remove files in a specified directory. Let's first connect our server to the MCP Client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "replicate_model = \"ibm-granite/granite-3.3-8b-instruct\"\n",
    "server_command = \"npx\"\n",
    "server_args = [\"-y\", \"@modelcontextprotocol/server-filesystem\", \".\"]\n",
    "\n",
    "client = GraniteMCPClient(replicate_model, server_command, server_args)\n",
    "\n",
    "await client.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Now let's write some instructions to tell the model how to store, edit and retrieve memories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You are an AI assistant with persistent memory stored in 'memory.txt'.\n",
    "\n",
    "At the start of each response:\n",
    "- Always call read_file with path 'memory.txt' to get the current memory.\n",
    "\n",
    "If 'memory.txt' does not exist:\n",
    "- Create it by calling write_file with content starting with '1. '.\n",
    "\n",
    "When the user shares new personal information:\n",
    "- Read the current memory.\n",
    "- Append the new fact as a new numbered line.\n",
    "- Save the updated list with write_file.\n",
    "\n",
    "When the user asks to remove or replace information:\n",
    "- Read the current memory.\n",
    "- Update the list accordingly (remove or replace).\n",
    "- Renumber the entries starting from 1.\n",
    "- Save the updated list with write_file.\n",
    "\n",
    "When the user asks something:\n",
    "- Read 'memory.txt' and use this information to answer naturally.\n",
    "\n",
    "When you answer, always:\n",
    "- Start by performing tool calls first.\n",
    "- Then give a clear, final natural language response to the user.\n",
    "\n",
    "Never read or edit any file except 'memory.txt'.\n",
    "you are strictly forbidden from using the following tools: list_directory, get_file_info\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Now we can start a conversation with our memory-powered AI assistant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_text = (\n",
    "    \"Hello Granite, I am an AI engineer at IBM Research. \"\n",
    "    \"I have a dog named Sunday. She is a border collie and she is 13 years old.\"\n",
    ")\n",
    "\n",
    "# prepend memory instructions, separated by a line of dashes\n",
    "combined_prompt = f\"{instructions}\\n\\n---\\n\\n{user_text}\"\n",
    "\n",
    "response = await client.chat(combined_prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "The LLM will store this information in it's memory bank. Now we can open up a new chat, and ask it a question about this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_text = (\n",
    "    \"what kind of dog do I have?\"\n",
    ")\n",
    "\n",
    "prompt = f\"{instructions}\\n\\n---\\n\\n{user_text}\"\n",
    "\n",
    "response = await client.chat(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how MCP bridges the gap between language models and external resources, creating a foundation for more sophisticated agentic AI systems. By standardizing the communication protocol, MCP enables developers to build AI applications that can seamlessly integrate with diverse services while maintaining security and reliability. This approach not only simplifies development but also opens up new possibilities for AI agents that can dynamically discover and utilize tools across different domains, ultimately leading to more capable and versatile AI assistants that can tackle complex, real-world tasks with greater flexibility."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
