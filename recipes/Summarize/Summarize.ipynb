{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6rko_ANX0EC"
   },
   "source": [
    "# Document Summarization\n",
    "\n",
    "This notebook demonstrates an application of long document summarization techniques to a work of literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwS1CzAbaFzq"
   },
   "source": [
    "## Install Dependencies\n",
    "\n",
    "Granite Kitchen comes with a bundle of dependencies that are required for notebooks. See the list of packages in its [`setup.py`](https://github.com/ibm-granite-community/granite-kitchen/blob/main/setup.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2zUHQD71qgqf"
   },
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/ibm-granite-community/utils \\\n",
    "    \"langchain_community<0.3.0\" \\\n",
    "    \"transformers>=4.45.2\" \\\n",
    "    langchain-huggingface \\\n",
    "    replicate \\\n",
    "    torch \\\n",
    "    tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydrVWz7EYHh9"
   },
   "source": [
    "## Select your model\n",
    "\n",
    "Select a Granite model from the [`ibm-granite`](https://replicate.com/ibm-granite) org on Replicate. Here we use the Replicate Langchain client to connect to the model.\n",
    "\n",
    "To get set up with Replicate, see [Getting Started with Replicate](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started/Getting_Started_with_Replicate.ipynb).\n",
    "\n",
    "To connect to a model on a provider other than Replicate, substitute this code cell with one from the [LLM component recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_LLMs.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TSkiGBY4qo32"
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import Replicate\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "\n",
    "model = Replicate(\n",
    "    model=\"ibm-granite/granite-3.0-8b-instruct\",\n",
    "    replicate_api_token=get_env_var('REPLICATE_API_TOKEN'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d0sWaZ7YLHN"
   },
   "source": [
    "## Download a book\n",
    "\n",
    "Here we fetch H.D. Thoreau's \"Walden\" from [Project Gutenberg](https://www.gutenberg.org/) for summarization.\n",
    "\n",
    "We have to trim it down so that it will fit in the 128k-token context window of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JFi40LArpIa"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from time import sleep\n",
    "\n",
    "# The following URL contains a text version of H.D. Thoreau's \"Walden\"\n",
    "url = \"https://www.gutenberg.org/cache/epub/205/pg205.txt\"\n",
    "\n",
    "# Get the contents\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "full_contents = response.text\n",
    "\n",
    "# Extract the text of the book, leaving out the gutenberg boilerplate.\n",
    "start_str = \"*** START OF THE PROJECT GUTENBERG EBOOK WALDEN, AND ON THE DUTY OF CIVIL DISOBEDIENCE ***\"\n",
    "start_index = full_contents.index(start_str) + len(start_str)\n",
    "end_str = \"*** END OF THE PROJECT GUTENBERG EBOOK WALDEN, AND ON THE DUTY OF CIVIL DISOBEDIENCE ***\"\n",
    "end_index = full_contents.index(end_str)\n",
    "book_contents = full_contents[start_index:end_index]\n",
    "print(\"Length of book text: {} chars\".format(len(book_contents)))\n",
    "\n",
    "# We limit the text to 200k characters, which is about 57k tokens. (400k chars is ~114k tokens; 300k chars is ~86k tokens; 350k chars is ~100k tokens).\n",
    "char_limit = 10000\n",
    "contents = book_contents[:char_limit]\n",
    "print(\"Length of text for summarization: {} chars\".format(len(contents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYuQmgRJY0n5"
   },
   "source": [
    "## Count the tokens\n",
    "\n",
    "Before sending our code to the AI model, it's crucial to understand how much of the model's capacity we're using. Language models typically have a limit on the number of tokens they can process in a single request.\n",
    "\n",
    "Key points:\n",
    "- We're using the [`granite-3.0-8b-instruct`](https://huggingface.co/ibm-granite/granite-3.0-8b-instruct) model, which has a context window of 4,000 tokens.\n",
    "- Tokenization can vary between models, so we use the specific tokenizer for our chosen model.\n",
    "\n",
    "Understanding token count helps us optimize our prompts and ensure we're using the model efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7JqmvTqbWPgl"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = \"ibm-granite/granite-3.0-8b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "print(\"Your model uses the tokenizer \" + type(tokenizer).__name__)\n",
    "\n",
    "print(f\"Your document has {len(tokenizer.tokenize(contents))} tokens. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygNmITWQZAZ8"
   },
   "source": [
    "## Summarize the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this optimial question-answer format according to the Granite Prompting Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_guide_template = \"\"\"\\\n",
    "<|start_of_role|>user<|end_of_role|>{prompt}<|end_of_text|>\n",
    "<|start_of_role|>assistant<|end_of_role|>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consruct our prompt and send it to the AI model on Replicate for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yu4HeuqWqvOj"
   },
   "outputs": [],
   "source": [
    "prompt = prompt_guide_template.format(prompt = f\"\"\"\n",
    "Summarize the following text:\n",
    "{contents}\n",
    "\"\"\")\n",
    "\n",
    "output = model.invoke(\n",
    "    prompt,\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 10000, # Set the maximum number of tokens to generate as output.\n",
    "        \"min_tokens\": 200, # Set the minimum number of tokens to generate as output.\n",
    "        \"temperature\": 0.75,\n",
    "        \"system_prompt\": \"You are a helpful assistant.\",\n",
    "        \"presence_penalty\": 0,\n",
    "        \"frequency_penalty\": 0\n",
    "    }\n",
    "    )\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Summaries\n",
    "\n",
    "Here we use a  hierarchical abstractive summarization technique to adapt to the context length of the model. Our approach is na√Øve, in that it takes equal-width chunks and groups of chunks from the document. A more sophisticated approach would be to create a document hierarchical structure that accounts for the document's structure and features, and groups text passages by topic or section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk the text\n",
    "\n",
    "Divide the full text into smaller passages for separate processing. The `chunk_size` (given in tokens) must account for the size of both the messages (input) and the completions (output). The resulting chunk size may sometimes exceed the `chunk_size` provided, so we give it additional headroom. \n",
    "\n",
    "The `chunk_overlap` parameter allows us to overlap chunks by a certain number of tokens, to help preserve coherence between chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Use entire book, or truncate if testing.\n",
    "text = book_contents\n",
    "if get_env_var('GRANITE_TESTING', 'false') == 'true':\n",
    "    text = book_contents[:20000]\n",
    "print(f\"The text is {len(tokenizer.tokenize(text))} tokens.\")\n",
    "\n",
    "# Split the documents into chunks\n",
    "chunk_token_limit = 3000  # In tokens: 3000 message + 512 completion + ~350 padding < 4000 context length\n",
    "text_splitter = TokenTextSplitter.from_huggingface_tokenizer(tokenizer=tokenizer, chunk_size=chunk_token_limit, chunk_overlap=0)\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "print(\"Chunk count: \" + str(len(chunks)))\n",
    "print(\"Max chunk length: \" + str(max([len(tokenizer.tokenize(chunk)) for chunk in chunks])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the chunks\n",
    "\n",
    "Here we create a separate summary of each passage. This can take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(texts, prompt_template, min, max):\n",
    "    summaries = []\n",
    "    for i, text in enumerate(texts):\n",
    "        print(f\"{i + 1}. Input size: {len(tokenizer.tokenize(text))} tokens\")\n",
    "        prompt = prompt_template.format(text=text)\n",
    "        output = model.invoke(\n",
    "            prompt,\n",
    "            model_kwargs={\n",
    "                \"max_tokens\": 2000, # Set the maximum number of tokens to generate as output.\n",
    "                \"min_tokens\": 200, # Set the minimum number of tokens to generate as output.\n",
    "                \"temperature\": 0.75,\n",
    "                \"system_prompt\": \"You are a helpful assistant.\",\n",
    "                \"presence_penalty\": 0,\n",
    "                \"frequency_penalty\": 0\n",
    "            }\n",
    "        )\n",
    "        print(f\"{i + 1}. Output size: {len(tokenizer.tokenize(output))} tokens\")\n",
    "        summary = f\"Summary {i+1}:\\n{output}\\n\\n\"\n",
    "        summaries.append(summary)\n",
    "        print(summary)\n",
    "\n",
    "    print(\"Summary count: \" + str(len(summaries)))\n",
    "    summary_contents = \"\\n\\n\".join(summaries)\n",
    "    print(f\"Total: {len(tokenizer.tokenize(summary_contents))} tokens\")\n",
    "\n",
    "    return summaries\n",
    "\n",
    "\n",
    "prompt = prompt_guide_template.format(prompt = \"\"\"\n",
    "    Summarize the following text using only the information found in the text:\n",
    "    {text}\n",
    "    \"\"\")\n",
    "\n",
    "summaries_lvl_1 = summarize(chunks, prompt, 200, 2000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the Summaries\n",
    "\n",
    "We signal to the model that it is receiving separate summaries of passages from an original text, and to create a unified summary of that text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a method for aggregating groups of summaries.\n",
    "def group_array(arr, n):\n",
    "    # Calculate the size of each chunk\n",
    "    avg_len = len(arr) // n\n",
    "    remainder = len(arr) % n\n",
    "    result = []\n",
    "    start = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        # Distribute the remainder elements across the first chunks\n",
    "        end = start + avg_len + (1 if i < remainder else 0)\n",
    "        result.append(arr[start:end])\n",
    "        start = end\n",
    "\n",
    "    return result\n",
    "\n",
    "# Aggregate groups of summaries for further summarization.\n",
    "# Summaries are <=512k tokens, so we want at most 6 summaries (<=3072 tokens) per group.\n",
    "num_groups = (len(chunks) // 6) + (1 if len(chunks) % 6 else 0)\n",
    "summary_groups = group_array(summaries_lvl_1, num_groups)\n",
    "texts_lvl_2 = [\"\\n\\n\".join(summary_group)[:3500] for summary_group in summary_groups]\n",
    "\n",
    "prompt = prompt_guide_template.format(prompt = \"\"\"\\\n",
    "A text was summarized in separate passages; those passage summaries are provided below. \n",
    "\n",
    "{text}\n",
    "\n",
    "From these summaries alone, compose a single, unified summary of the text.\n",
    "\"\"\")\n",
    "\n",
    "summaries_lvl_2 = summarize(texts_lvl_2, prompt, 500, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Final Summary\n",
    "\n",
    "Generate a single summary from the passage summaries generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_lvl_3 = [\"\\n\\n\".join(summaries_lvl_2)]\n",
    "final_summary = summarize(texts_lvl_3, prompt, 500, 1000)[0]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
