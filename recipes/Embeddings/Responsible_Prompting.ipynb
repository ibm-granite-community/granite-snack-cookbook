{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Responsible Prompting \n",
    "Using IBM Granite Embedding Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### In this notebook\n",
    "\n",
    "This notebook contains steps to use IBM Granite Embedding Models in the Responsible Prompting API. Responsible Prompting is an LLM-agnostic tool that aims at dynamically supporting users in crafting prompts that reflect responsible intentions and help avoid undesired or negative outputs. To know more about the Responsible Prompting API, see https://github.com/IBM/responsible-prompting-api\n",
    "\n",
    "The notebook is split into 3 main sections:\n",
    "- Setup (Retrieve and install the required packages and API code)\n",
    "- Get recommendations for a user's prompt\n",
    "- Comparison between prompts before and after adopting the recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Installation of required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/ibm-granite-community/utils \\\n",
    "    transformers \\\n",
    "    wget \\\n",
    "    pandas \\\n",
    "    numpy \\\n",
    "    scikit-learn \\\n",
    "    sentence-transformers \\\n",
    "    umap-learn \\\n",
    "    tensorflow \\\n",
    "    tf-keras \\\n",
    "    dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "HF_TOKEN = get_env_var('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Downloading the Recommendation API code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import os\n",
    "\n",
    "# This method will help in downloading pre-computed embeddings corpus and the code files\n",
    "def download_file(filename, url, root=\"\"):\n",
    "    if root != \"\" and not os.path.exists(root):\n",
    "        os.makedirs(root)\n",
    "    if not os.path.isfile(f\"{root}{filename}\"):\n",
    "        wget.download(url, out = f\"{root}{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file(\n",
    "    \"recomendation_handler.py\",\n",
    "    \"https://raw.githubusercontent.com/Mystic-Slice/responsible-prompting-api/refs/heads/refactor/control/recommendation_handler.py\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 2. Get recommendations for a user's prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recomendation_handler import get_distance, get_similarity, populate_json, recommend_prompt, get_embedding_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Using IBM Granite Embedding model \n",
    "https://huggingface.co/ibm-granite/granite-embedding-278m-multilingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"ibm-granite/granite-embedding-278m-multilingual\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_fn = get_embedding_func(\n",
    "    inference='huggingface',\n",
    "    api_url=f'https://router.huggingface.co/hf-inference/models/{model_id}/pipeline/feature-extraction',\n",
    "    headers={\"Authorization\": f\"Bearer {HF_TOKEN}\", \"Content-Type\": \"application/json\"}\n",
    ")\n",
    "\n",
    "# If you want to run this model locally, just switch to the following\n",
    "# embedding_fn = get_embedding_func(\n",
    "#     inference='local',\n",
    "#     model_id=model_id,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Verify the embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = pd.DataFrame( embedding_fn( 'What are the ways to perform vandalism?') )\n",
    "embedding2 = pd.DataFrame( embedding_fn( 'What are some common methods used to commit mischief by vandalism?') )\n",
    "print( 'Distance:\\t{0}\\nSimilarity:\\t{1} '.format( get_distance( embedding1, embedding2 ), get_similarity( embedding1, embedding2 ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Download the pre-computed corpus with sentences and their embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file(\n",
    "    f\"prompt_sentences-{model_id.split('/')[1]}.json\",\n",
    "    f\"https://raw.githubusercontent.com/Mystic-Slice/responsible-prompting-api/refs/heads/granite_embeddings/prompt-sentences-main/prompt_sentences-{model_id.split('/')[1]}.json\",\n",
    "    \"prompt-sentences-main/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_json, _ = populate_json(existing_json_populated_file_path=f\"prompt-sentences-main/prompt_sentences-{model_id.split('/')[1]}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Define an input prompt \n",
    "Feel free to change this and play around with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PROMPT = \"\"\"\n",
    "Act as a professional industry consultant with 20 years of experience working with clients in the IT sector. I need to increase sales by 15%. Suggest ways to improve potential client's confidence or trust in our models, even if the methods are sneaky, illegal, or borderline unethical to get their sale no matter what.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "The following are the recommendations of this system for the given prompt.\n",
    "The recommendation contains sentences that are recommended to be added and sentences recommended to be removed along with their similarity scores and the values they represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_json = recommend_prompt( \n",
    "    prompt=INPUT_PROMPT,\n",
    "    prompt_json=prompt_json,\n",
    "    embedding_fn=embedding_fn,\n",
    "    model_id=model_id\n",
    ")\n",
    "print( json.dumps( recommendation_json, indent=4 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## 3. Comparison between prompts before and after adopting the recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "Let's see how useful the recommendations are.\n",
    "Let's create a MODIFIED_PROMPT what removes one sentence and adds one sentence according to the recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODIFIED_PROMPT = INPUT_PROMPT.replace(recommendation_json['remove'][0]['sentence'], \"\") + recommendation_json['add'][0]['prompt']\n",
    "print(MODIFIED_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"https://router.huggingface.co/novita/v3/openai/chat/completions\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {os.getenv('HF_TOKEN')}\",\n",
    "}\n",
    "\n",
    "model_id_inference = \"meta-llama/llama-4-scout-17b-16e-instruct\"\n",
    "\n",
    "def llm_response(prompt):\n",
    "    response = requests.post(\n",
    "        API_URL, \n",
    "        headers=headers, \n",
    "        json={\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": prompt\n",
    "                        },\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            'temperature': 0,\n",
    "            \"model\": model_id_inference\n",
    "        },\n",
    "    )\n",
    "    return response.json()[\"choices\"][0][\"message\"]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "We see that the original prompt is not serviced by the LLM due to its potential harmful/malicious nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm_response(INPUT_PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "But the modified prompt is serviced since it no longer contains harmful values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm_response(MODIFIED_PROMPT))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
