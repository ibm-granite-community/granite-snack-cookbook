{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Model Context Protocol\n",
    "\n",
    "Modern AI applications face a fundamental challenge: how to securely and efficiently connect language models to the vast ecosystem of external data sources, APIs, and tools they need to be truly useful. Previously, developers would often build custom integrations for each external service, creating a fragmented landscape that is difficult to maintain, scale, and standardize. This fragmentation limits the potential of agentic AI systems, which require access to diverse external resources to perform complex tasks effectively.\n",
    "\n",
    "[Model Context Protocol (MCP)](https://modelcontextprotocol.io) addresses this challenge by providing a universal standard for AI-to-service communication. Rather than building point-to-point integrations, MCP establishes a common language that allows any AI model to connect to any compatible service through a standardized interface. This transforms agentic AI workflows by enabling models to dynamically discover and utilize external capabilities, without requiring custom integration code for each service. The result is more flexible, maintainable, and powerful AI applications that can adapt to new tools and data sources as they become available.\n",
    "\n",
    "In this notebook, we will be exploring how to connect [Granite models](https://www.ibm.com/granite) to MCP servers, enabling advanced agentic workflows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## MCP with Granite\n",
    "\n",
    "The **Model Context Protocol (MCP)** is an open standard that enables AI applications to securely connect to external data sources and tools. It consists of two main components:\n",
    "\n",
    "### MCP Server\n",
    "The **MCP Server** exposes resources, tools, and prompts that LLMs can access. Servers can provide access to databases, APIs, file systems, or any external service through a standardized interface.\n",
    "\n",
    "### MCP Client  \n",
    "The **MCP Client** connects to MCP servers and facilitates communication between your application and the server's capabilities. It handles:\n",
    "- Authentication and connection management\n",
    "- Resource discovery (what tools/resources are available)\n",
    "- Tool execution and data retrieval\n",
    "- Protocol-level communication with the server\n",
    "\n",
    "Together, these components will allow your application to connect Granite LLMs to external resources (servers) using a standardized MCP interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "First we will install the requisite packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo \"::group::Install Dependencies\"\n",
    "%pip install uv\n",
    "! uv pip install git+https://github.com/ibm-granite-community/utils.git \\\n",
    "    langchain_core \\\n",
    "    'langchain_replicate @ git+https://github.com/ibm-granite-community/langchain-replicate.git' \\\n",
    "    transformers \\\n",
    "    mcp \\\n",
    "    mcp-server-fetch\n",
    "! echo \"::endgroup::\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_replicate import Replicate\n",
    "from langchain_core.messages import AIMessage, ToolMessage, ToolCall\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.utils.json import parse_json_markdown\n",
    "from ibm_granite_community.langchain import TokenizerChatPromptTemplate\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "from transformers import AutoTokenizer\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "from contextlib import AbstractAsyncContextManager, AsyncExitStack\n",
    "from typing import Any, Self\n",
    "from mcp import ClientSession, StdioServerParameters, stdio_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Creating the MCP Client\n",
    "\n",
    "Let's start by building the MCP client. Our client needs a few methods: \n",
    "\n",
    "- `__aenter__`: Connects the LLM to the MCP server\n",
    "- `__aexit__`: Cleans up server connections\n",
    "- `create_tool_prompt`: Provide the LLM access to the server's tools, resources, and prompts\n",
    "- `parse_model_response`: Parse LLMs response and execute tool calls when provided\n",
    "\n",
    "We also provide a `chat` method, which allows the model to call multiple tools in series to answer a users prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraniteMCPClient(AbstractAsyncContextManager):\n",
    "    def __init__(self, replicate_model: str, server_command: str, server_args: list[str] | None = None):\n",
    "        \"\"\"\n",
    "        Initialize the client with persistent MCP connection.\n",
    "\n",
    "        Args:\n",
    "            replicate_model: The Replicate model identifier\n",
    "            server_command: Command to run MCP server (e.g., \"npx\")\n",
    "            server_args: Arguments for the server command\n",
    "        \"\"\"\n",
    "        self.model = replicate_model\n",
    "\n",
    "        self.llm = Replicate(\n",
    "            model=replicate_model,\n",
    "            model_kwargs={\"max_tokens\": 4096, \"temperature\": 0},\n",
    "            replicate_api_token=get_env_var('REPLICATE_API_TOKEN'),\n",
    "        )\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(replicate_model, trust_remote_code=True)\n",
    "\n",
    "        self.server_params = StdioServerParameters(\n",
    "            command=server_command,\n",
    "            args=server_args or [],\n",
    "            env=os.environ.copy(),\n",
    "        )\n",
    "        self.session = None\n",
    "        self._exit_stack = AsyncExitStack()\n",
    "\n",
    "    async def __aenter__(self) -> Self:\n",
    "        \"\"\"Initialize the persistent MCP connection\"\"\"\n",
    "        client_context = stdio_client(self.server_params)\n",
    "        read_stream, write_stream = await self._exit_stack.enter_async_context(client_context)\n",
    "\n",
    "        self.session = ClientSession(read_stream, write_stream)\n",
    "        await self._exit_stack.enter_async_context(self.session)\n",
    "        await self.session.initialize()\n",
    "\n",
    "        # Get and log available tools\n",
    "        tools_result = await self.session.list_tools()\n",
    "        available_tools = tools_result.tools\n",
    "        print(\"ðŸ”§ Connected to MCP server.\")\n",
    "        print(f\"Available tools: {[tool.name for tool in available_tools]}\")\n",
    "        return self\n",
    "\n",
    "    async def __aexit__(self, *exc_details):\n",
    "        \"\"\"Clean up MCP connections\"\"\"\n",
    "        await self._exit_stack.aclose()\n",
    "        print(\"ðŸ”§ Disconnected from MCP server.\")\n",
    "\n",
    "    def create_tool_prompt(\n",
    "        self,\n",
    "        tools: list,\n",
    "    ) -> list[dict[str, Any]]:\n",
    "        \"\"\"Return JSON block that enumerates the MCP tools.\n",
    "        \"\"\"\n",
    "        if not tools:\n",
    "            return []\n",
    "\n",
    "        tools_info = []\n",
    "        for tool in tools:\n",
    "            arguments = {}\n",
    "            if tool.inputSchema and \"properties\" in tool.inputSchema:\n",
    "                for name, meta in tool.inputSchema[\"properties\"].items():\n",
    "                    arguments[name] = {\n",
    "                        \"description\": meta.get(\"description\", f\"The {name} parameter\")\n",
    "                    }\n",
    "\n",
    "            tools_info.append(\n",
    "                {\n",
    "                    \"name\": tool.name,\n",
    "                    \"description\": tool.description,\n",
    "                    \"arguments\": arguments,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return tools_info\n",
    "\n",
    "    def parse_model_response(self, response: str) -> list[dict[str, Any]] | None:\n",
    "        \"\"\"Parse model response to extract tool calls\"\"\"\n",
    "        try:\n",
    "            # If Granite used its special tag, isolate the JSON segment first\n",
    "            if \"<tool_call>\" in response:\n",
    "                json_blob = response.split(\"<tool_call>\", 1)[1]\n",
    "                # Trim any trailing role tags\n",
    "                if \"</tool_call>\" in json_blob:\n",
    "                    json_blob = json_blob.split(\"</tool_call>\", 1)[0]\n",
    "            else:\n",
    "                json_blob = response\n",
    "\n",
    "            tool_calls = parse_json_markdown(json_blob)\n",
    "            if tool_calls is None:\n",
    "                return None\n",
    "            if isinstance(tool_calls, dict):\n",
    "                tool_calls = [tool_calls]\n",
    "            return tool_calls\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    async def chat(\n",
    "        self,\n",
    "        user_message: str,\n",
    "        *,\n",
    "        max_tool_executions: int = 5,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Agent loop for Granite + MCP.\n",
    "\n",
    "        â€£ Prints the full prompt, raw model output, parsed tool calls,\n",
    "          and tool responses each turn.\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.session:\n",
    "            raise RuntimeError(\"Client not started.\")\n",
    "\n",
    "        prompt_template = TokenizerChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "                MessagesPlaceholder(\"tool_results\", optional=True),\n",
    "            ],\n",
    "            tokenizer=self.tokenizer,\n",
    "        )\n",
    "\n",
    "        tools_schema = self.create_tool_prompt(\n",
    "            (await self.session.list_tools()).tools\n",
    "        )  # list[dict] JSON schema objects\n",
    "\n",
    "        tool_results_msgs: list = []             # running AI / Tool messages\n",
    "        query_text = user_message                # first human turn\n",
    "        turn = 0\n",
    "\n",
    "        while turn < max_tool_executions:\n",
    "            turn += 1\n",
    "            prompt_text = prompt_template.format(\n",
    "                query=query_text,\n",
    "                tools=tools_schema,\n",
    "                tool_results=tool_results_msgs,\n",
    "            )\n",
    "\n",
    "            # Call Granite via LangChain-Replicate wrapper\n",
    "            response_text = str(self.llm.invoke(prompt_text))\n",
    "\n",
    "            print(\"\\n========== MODEL RAW RESPONSE ==========\")\n",
    "            print(response_text)\n",
    "            print(\"========================================\")\n",
    "\n",
    "            tool_calls = self.parse_model_response(response_text)\n",
    "\n",
    "            print(\"\\nParsed tool calls:\", tool_calls)\n",
    "\n",
    "            # Natural-language answer â†’ exit loop\n",
    "            if not tool_calls:\n",
    "                return response_text\n",
    "\n",
    "            # Execute each tool call\n",
    "            for idx, call_dict in enumerate(tool_calls, start=1):\n",
    "                name = call_dict[\"name\"]\n",
    "                args_dict = call_dict.get(\"arguments\", {})\n",
    "                # Check if arguments quoted string\n",
    "                if isinstance(args_dict, str) and len(args_dict) >= 2:\n",
    "                    quote = args_dict[0]\n",
    "                    if quote in \"\\\"'\" and args_dict[-1] == quote:  # Remove surrounding quotes\n",
    "                        args_dict = args_dict[1:-1].encode().decode(\"unicode_escape\")\n",
    "                    # Check if valid JSON\n",
    "                    try:\n",
    "                        args_dict = json.loads(args_dict)\n",
    "                    except json.JSONDecodeError:\n",
    "                        # Not valid JSON; assume dict repr\n",
    "                        args_dict = ast.literal_eval(args_dict)\n",
    "\n",
    "                print(f\"\\n>>> Executing tool {name} {args_dict}\")\n",
    "\n",
    "                result = await self.session.call_tool(name, args_dict) # type: ignore\n",
    "\n",
    "                # Flatten MCP result to string\n",
    "                if result.content:\n",
    "                    if isinstance(result.content, list):\n",
    "                        result_text = \"\\n\".join(\n",
    "                            x.text if hasattr(x, \"text\") else str(x)\n",
    "                            for x in result.content\n",
    "                        )\n",
    "                    else:\n",
    "                        result_text = str(result.content)\n",
    "                else:\n",
    "                    result_text = \"Tool executed successfully\"\n",
    "\n",
    "                print(\"Tool output -->\", result_text[:300], \"...\\n\")\n",
    "\n",
    "                # Record messages for next prompt\n",
    "                tc_obj = ToolCall(name=name, args=args_dict, id=str(idx))\n",
    "                tool_results_msgs.append(\n",
    "                    AIMessage(content=response_text, tool_calls=[tc_obj])\n",
    "                )\n",
    "                tool_results_msgs.append(\n",
    "                    ToolMessage(content=result_text, tool_call_id=str(idx))\n",
    "                )\n",
    "\n",
    "            # After first round the human has no new text; keep placeholder only\n",
    "            query_text = \" \"\n",
    "\n",
    "        # If we hit max_tool_executions without a final answer\n",
    "        prompt_text = prompt_template.format(\n",
    "            query=\"Directly answer user's original message\",                       # no new user input\n",
    "            tools=tools_schema,\n",
    "            tool_results=tool_results_msgs,  # full history of tool use\n",
    "        )\n",
    "\n",
    "        final_answer = str(self.llm.invoke(prompt_text))\n",
    "\n",
    "        return final_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Connecting MCP Servers\n",
    "Now that we have built an MCP client, we can now connect our LLM to any MCP server of our choice! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Web Search\n",
    "\n",
    "Let's connect our model to an external web content fetch server, which allows it to retrieve and process information from web pages in real time. \n",
    "\n",
    "We will be using the [Official MCP Fetch Server](https://github.com/modelcontextprotocol/servers/tree/main/src/fetch). This enables our model to fetch, read, and summarize content from any URL, making it possible to answer questions using up-to-date information from the internet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "replicate_model = \"ibm-granite/granite-4.0-h-small\"\n",
    "server_command = \"python\"\n",
    "server_args = [\"-m\", \"mcp_server_fetch\"]\n",
    "\n",
    "async with GraniteMCPClient(replicate_model, server_command, server_args) as client:\n",
    "    instructions = \"\"\"\n",
    "    You are a helpful assistant with access to a fetch, a tool that helps you read webpages given a url. You can use this to help answer user queries. ALWAYS answer the user's query DIRECTLY regardless of whether you use a tool call or not. Do not include a max_length parameter in your calls to fetch. The output will never be truncated. You are provided all of it. Do not tell the user you cannot answer their request because the output is truncated.\n",
    "    \"\"\"\n",
    "\n",
    "    user_text = (\n",
    "        \"Read a maximum of 10000 characters from this website 'https://research.ibm.com/blog/granite-vision-ocr-leaderboard' and then summarize the content.\"\n",
    "    )\n",
    "\n",
    "    combined_prompt = f\"{instructions}\\n\\n---\\n\\n{user_text}\"\n",
    "\n",
    "    response = await client.chat(combined_prompt, max_tool_executions = 1)\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how MCP bridges the gap between language models and external resources, creating a foundation for more sophisticated agentic AI systems. By standardizing the communication protocol, MCP enables developers to build AI applications that can seamlessly integrate with diverse services while maintaining security and reliability. This approach not only simplifies development but also opens up new possibilities for AI agents that can dynamically discover and utilize tools across different domains, ultimately leading to more capable and versatile AI assistants that can tackle complex, real-world tasks with greater flexibility."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
