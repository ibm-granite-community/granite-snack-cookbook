{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6rko_ANX0EC"
   },
   "source": [
    "# Document Summarization\n",
    "\n",
    "This notebook demonstrates an innovative application of long document summarization techniques to automatically generate documentation for Python code. By treating a codebase as a \"long document,\" we leverage AI-powered language models to comprehend, distill, and explain complex code structures.\n",
    "\n",
    "Key concepts:\n",
    "\n",
    "1. Document preprocessing: We fetch a document from a URLand format code from a GitHub repository, similar to how one might prepare a long text document for summarization.\n",
    "2. Chunking and tokenization: We analyze the token count of our code \"document\" to ensure it fits within the model's context window, a crucial step in long document processing.\n",
    "3. Prompt engineering: We craft a specialized prompt that guides the AI to focus on key aspects of the code, much like how summarization prompts direct models to capture essential information.\n",
    "4. AI-powered analysis: Using the Replicate API, we access a large language model capable of understanding code semantics and generating human-readable explanations.\n",
    "5. Structured output: We instruct the model to produce documentation in a consistent format, analogous to generating structured summaries from lengthy texts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwS1CzAbaFzq"
   },
   "source": [
    "## Install Dependencies\n",
    "\n",
    "Before we begin, we need to install the required Python packages. We'll be using:\n",
    "\n",
    "- `replicate`: To interact with the Replicate API for accessing AI models\n",
    "- `transformers`: For tokenization and working with language models\n",
    "\n",
    "These packages will be installed using pip, Python's package installer. If you're running this notebook in a fresh environment, make sure you have pip installed and updated (if you are in Colab, this is done for you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2zUHQD71qgqf"
   },
   "outputs": [],
   "source": [
    "!pip install replicate transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydrVWz7EYHh9"
   },
   "source": [
    "## Set Replicate Token\n",
    "\n",
    "To use the Replicate API, we need to authenticate our requests. This is done using an API token.\n",
    "\n",
    "For security reasons, it's best to store this token as an environment variable rather than hardcoding it into our script. In this notebook, we're using Google Colab's `userdata` feature to securely store and retrieve the token.  (If you are not using Colab, change this cell to\n",
    "\n",
    "```\n",
    "os.environ['REPLICATE_API_TOKEN'] = \"your-token\"\n",
    "```\n",
    "\n",
    "Remember to never share your API tokens publicly or commit them to version control systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TSkiGBY4qo32"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.environ.get('REPLICATE_API_TOKEN') is None:\n",
    "    \"\"\"Replicate API token not set, we're probably in Colab. Let's try to fetch it.\"\"\"\n",
    "    from google.colab import userdata\n",
    "    userdata = userdata.get(\"replicate-api-token\")\n",
    "    os.environ['REPLICATE_API_TOKEN'] = userdata.get('REPLICATE_API_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d0sWaZ7YLHN"
   },
   "source": [
    "## Download a book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JFi40LArpIa"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from time import sleep\n",
    "\n",
    "# The following URL contains a text version of H.D. Thoreau's \"Walden\"\n",
    "url = \"https://www.gutenberg.org/cache/epub/205/pg205.txt\"\n",
    "\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "\n",
    "contents = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYuQmgRJY0n5"
   },
   "source": [
    "## Count the tokens\n",
    "\n",
    "Before sending our code to the AI model, it's crucial to understand how much of the model's capacity we're using. Language models typically have a limit on the number of tokens they can process in a single request.\n",
    "\n",
    "Key points:\n",
    "- We're using the `granite-8B-Code-instruct-128k` model, which has a context window of 128,000 tokens\n",
    "- The context window includes both the input (our code) and the output (the generated documentation)\n",
    "- Tokenization can vary between models, so we use the specific tokenizer for our chosen model\n",
    "- If our input is too large, we may need to split it into smaller chunks or summarize it\n",
    "\n",
    "Understanding token count helps us optimize our prompts and ensure we're using the model efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7JqmvTqbWPgl"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = \"ibm-granite/granite-8B-Code-instruct-128k\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "print(f\"Your document has has {len(tokenizer(contents, return_tensors='pt')['input_ids'][0])} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygNmITWQZAZ8"
   },
   "source": [
    "### Create our prompt and call the model in Replicate\n",
    "\n",
    "This is where we construct our final prompt and send it to the AI model for processing.\n",
    "\n",
    "Our approach involves:\n",
    "1. Combining the code we fetched with specific instructions for documentation\n",
    "2. Using a template to guide the model's output format\n",
    "3. Calling the Replicate API with our constructed prompt and additional parameters\n",
    "\n",
    "Key considerations:\n",
    "- The prompt includes both the code and instructions for how to document it\n",
    "- We use a response template to ensure consistent formatting across functions\n",
    "- Parameters like `max_tokens`, `temperature`, and `system_prompt` can be adjusted to fine-tune the model's behavior\n",
    "- The output is streamed, allowing for real-time display of the generated documentation\n",
    "\n",
    "This step is where the magic happens - transforming our code into human-readable documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yu4HeuqWqvOj"
   },
   "outputs": [],
   "source": [
    "import replicate\n",
    "\n",
    "prompt = \"\"\"\n",
    "\n",
    "Provide detailed developer documentation for each function provided above.\n",
    "\n",
    "Response Template:\n",
    "## `function_name`\n",
    "\n",
    "* _param1_: (type) description\"\n",
    "\n",
    "Synopsis of the function\n",
    "\n",
    "_**returns**_:\n",
    "\"\"\"\n",
    "\n",
    "output = replicate.run(\n",
    "    \"ibm-granite/granite-8b-code-instruct-128k\",\n",
    "    input={\n",
    "\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 10000,\n",
    "        \"min_tokens\": 0,\n",
    "        \"temperature\": 0.75,\n",
    "        \"system_prompt\": \"You are a helpful assistant.\",\n",
    "        \"presence_penalty\": 0,\n",
    "        \"frequency_penalty\": 0\n",
    "    })\n",
    "\n",
    "\n",
    "print(\"\".join(output))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
