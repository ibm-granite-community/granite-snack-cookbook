{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microsoft Semantic Kernel With IBM Granite 3.3 LLM\n",
    "\n",
    "> **⚠️ Important Note**: This notebook is **not compatible with Google Colab** as it requires a local Ollama installation running a Granite model. Please run this notebook in a local environment with Ollama properly configured and running on port 11434.\n",
    "\n",
    "## What is Microsoft Semantic Kernel?\n",
    "\n",
    "**Microsoft Semantic Kernel (SK)** is an open-source SDK that allows developers to integrate AI capabilities into their applications through a unified framework. It acts as a middleware layer that orchestrates AI models, plugins, and traditional programming logic.\n",
    "\n",
    "## Key Features & Differentiators\n",
    "\n",
    "### 🔌 **Plugin Architecture**\n",
    "- **Semantic Kernel**: Uses a sophisticated plugin system where AI functions can be chained together with traditional code\n",
    "- **Other Frameworks**: Often focus on single model interactions or require custom integration work\n",
    "\n",
    "### 🧠 **AI Orchestration** \n",
    "- **Semantic Kernel**: Built-in planning capabilities that can automatically sequence multiple AI operations\n",
    "- **LangChain/LlamaIndex**: Primarily focused on chaining operations manually\n",
    "- **OpenAI API**: Direct model access without orchestration features\n",
    "\n",
    "### 🔄 **Multi-Model Support**\n",
    "- **Semantic Kernel**: Vendor-agnostic - works with OpenAI, Azure OpenAI, Hugging Face, Ollama, and more\n",
    "- **Other Frameworks**: Often tied to specific providers or require extensive configuration\n",
    "\n",
    "### 🎯 **Enterprise-Ready**\n",
    "- **Semantic Kernel**: Built with enterprise needs in mind - security, scalability, and integration with Microsoft ecosystem\n",
    "- **Other Frameworks**: May require additional tooling for enterprise deployment\n",
    "\n",
    "## About Granite 3.3\n",
    "\n",
    "**IBM Granite 3.3** models feature enhanced reasoning capabilities and support for Fill-in-the-Middle (FIM) code completion. Key highlights include:\n",
    "\n",
    "- **Enhanced Architecture**: New dense architecture trained with 12 trillion tokens across 12 languages and 116 programming languages\n",
    "- **128K Context Length**: Extended context for complex tasks\n",
    "- **Strong RAG Performance**: Excellent retrieval-augmented generation capabilities\n",
    "- **Function Calling**: Native support for tool/function calling\n",
    "- **Response Controls**: Built-in length and originality controls\n",
    "- **Apache 2.0 License**: Fully open-source\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "\n",
    "1. **Ollama installed locally**: Download and install from [https://ollama.ai](https://ollama.ai)\n",
    "2. **Granite 3.3 8B model downloaded**: Run `ollama pull granite3.3:8b` in your terminal\n",
    "3. **Ollama running**: Start Ollama service (usually `ollama serve` or it starts automatically)\n",
    "4. **Python 3.10, 3.11, or 3.12**: This notebook requires a recent Python version\n",
    "\n",
    "## This Demo\n",
    "\n",
    "This notebook demonstrates a simple chatbot using Semantic Kernel with:\n",
    "- **Ollama** as the local AI provider\n",
    "- **Granite 3.3 8B** model for responses  \n",
    "- **Granite 3.3 prompt template** following official guidelines\n",
    "- **Async execution** for responsive interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Install granite community utils and check Python version compatibility\n",
    "%pip install git+https://github.com/ibm-granite-community/utils\n",
    "\n",
    "import sys\n",
    "assert sys.version_info >= (3, 10) and sys.version_info < (3, 13), \\\n",
    "    f\"Python 3.10, 3.11, or 3.12 is required. Current version: {sys.version_info.major}.{sys.version_info.minor}\"\n",
    "\n",
    "print(f\"✅ Python version {sys.version_info.major}.{sys.version_info.minor} is compatible.\")\n",
    "\n",
    "# Install required packages\n",
    "%pip install semantic-kernel requests ipywidgets ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup Kernel and Granite 3.3 8B Model for Chatbot\n",
    "import requests\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.prompt_template import PromptTemplateConfig\n",
    "from semantic_kernel.connectors.ai.ollama import OllamaChatCompletion\n",
    "from semantic_kernel.connectors.ai.ollama.ollama_prompt_execution_settings import OllamaChatPromptExecutionSettings\n",
    "\n",
    "# Initialize kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "# Configure execution settings\n",
    "execution_settings = OllamaChatPromptExecutionSettings()\n",
    "\n",
    "# Validate Ollama is running and has the Granite model\n",
    "ollama_host = \"http://localhost:11434\"\n",
    "granite_model = \"granite3.3:8b\"\n",
    "\n",
    "try:\n",
    "    # Check if Ollama is running\n",
    "    response = requests.get(f\"{ollama_host}/api/tags\", timeout=5)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Check if Granite model is available\n",
    "    models = response.json()\n",
    "    available_models = [model['name'] for model in models['models']]\n",
    "    \n",
    "    if granite_model not in available_models:\n",
    "        print(f\"❌ Granite model '{granite_model}' not found in Ollama.\")\n",
    "        print(f\"Available models: {available_models}\")\n",
    "        print(f\"Please run: ollama pull {granite_model}\")\n",
    "        raise ValueError(f\"Required model {granite_model} not available\")\n",
    "    \n",
    "    print(f\"✅ Ollama is running and {granite_model} model is available\")\n",
    "    \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"❌ Cannot connect to Ollama at {ollama_host}\")\n",
    "    print(\"Please ensure Ollama is installed and running:\")\n",
    "    print(\"1. Install Ollama from https://ollama.ai\")\n",
    "    print(\"2. Run 'ollama serve' in terminal\")\n",
    "    print(f\"3. Run 'ollama pull {granite_model}' to download the model\")\n",
    "    raise ConnectionError(f\"Ollama connection failed: {e}\")\n",
    "\n",
    "# Initialize Semantic Kernel with Ollama service\n",
    "service_id = \"ollama\"\n",
    "kernel.add_service(\n",
    "    OllamaChatCompletion(\n",
    "        service_id=service_id,\n",
    "        host=ollama_host,\n",
    "        ai_model_id=granite_model,\n",
    "    )\n",
    ")\n",
    "print(\"✅ Semantic Kernel initialized successfully with Granite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define Chat Template using Jinja2 format with official Granite 3.3 template\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Install transformers to access the tokenizer\n",
    "%pip install transformers\n",
    "\n",
    "# Get current date for the system prompt\n",
    "current_date = datetime.now().strftime(\"%B %d, %Y\")\n",
    "\n",
    "# Load the Granite 3.3 tokenizer to get its official chat template\n",
    "print(\"📥 Loading Granite 3.3 tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ibm-granite/granite-3.3-8b-instruct\")\n",
    "\n",
    "# Get the chat template from the tokenizer\n",
    "chat_template = tokenizer.chat_template\n",
    "print(\"✅ Retrieved official chat template from Granite 3.3 tokenizer\")\n",
    "\n",
    "# Create the function configuration using jinja2 format\n",
    "prompt_template_config = PromptTemplateConfig(\n",
    "    template=chat_template, \n",
    "    name=\"chat\", \n",
    "    template_format=\"jinja2\"\n",
    ")\n",
    "\n",
    "# Create the chat function\n",
    "try:\n",
    "    function = kernel.add_function(\n",
    "        function_name=\"chat_function\",\n",
    "        plugin_name=\"chat_plugin\",\n",
    "        prompt_template_config=prompt_template_config,\n",
    "        prompt_execution_settings=execution_settings,\n",
    "    )\n",
    "    print(\"✅ Chat function configured with jinja2 template format\")\n",
    "    print(\"✅ Using official Granite 3.3 chat template from tokenizer\")\n",
    "    print(f\"✅ System prompt includes current date: {current_date}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Function creation failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define chat function to interact with Granite using the template\n",
    "import textwrap\n",
    "import time\n",
    "\n",
    "async def chat_with_granite(user_message):\n",
    "    \"\"\"\n",
    "    Chat function that processes a single user input and returns the response.\n",
    "    Uses jinja2 template format with official Granite 3.3 chat template.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if function is None:\n",
    "            raise RuntimeError(\"Function not initialized - check previous cells for errors\")\n",
    "        \n",
    "        print(f\"🤔 Thinking... (User: {user_message[:50]}{'...' if len(user_message) > 50 else ''})\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # The Granite chat template expects a messages format\n",
    "        # Create proper messages array for the jinja2 template\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": f\"Knowledge Cutoff Date: April 2024.\\nToday's Date: {current_date}. You are Granite, developed by IBM. You are a helpful AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "        \n",
    "        # Pass messages and required template variables\n",
    "        result = await kernel.invoke(function, \n",
    "                                   messages=messages,\n",
    "                                   add_generation_prompt=True)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        response_time = end_time - start_time\n",
    "        \n",
    "        # Get the response text\n",
    "        response_text = str(result)\n",
    "        \n",
    "        # Format the output with line wrapping for display\n",
    "        wrapped_text = textwrap.fill(response_text, width=80, break_long_words=False, break_on_hyphens=False)\n",
    "        \n",
    "        print(f\"\\n🤖 Assistant (responded in {response_time:.2f}s):\")\n",
    "        print(\"=\" * 50)\n",
    "        print(wrapped_text)\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        return response_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error occurred: {e}\")\n",
    "        print(\"\\n🔧 Troubleshooting:\")\n",
    "        print(\"1. Ensure Ollama is running: ollama serve\")\n",
    "        print(\"2. Ensure Granite model is available: ollama pull granite3.3:8b\")\n",
    "        print(\"3. Check if Ollama is accessible at http://localhost:11434\")\n",
    "        print(\"4. Try restarting the notebook kernel\")\n",
    "        raise\n",
    "\n",
    "def interactive_chat():\n",
    "    \"\"\"\n",
    "    Interactive chat mode - prompts user for input via terminal and returns response.\n",
    "    \"\"\"\n",
    "    print(\"💬 Chat with Granite 3.3 8B via Semantic Kernel (Jinja2 Template)!\")\n",
    "    print(\"Example prompts:\")\n",
    "    print(\"- 'What are the benefits of using Semantic Kernel?'\")\n",
    "    print(\"- 'Explain quantum computing in simple terms'\")\n",
    "    print(\"- 'Write a Python function to calculate fibonacci numbers'\")\n",
    "    print()\n",
    "    \n",
    "    user_input = input(\"Enter your message: \")\n",
    "    \n",
    "    if user_input and user_input.strip():\n",
    "        return user_input\n",
    "    else:\n",
    "        print(\"❌ Please enter a message to continue\")\n",
    "        return None\n",
    "\n",
    "print(\"✅ Chat functions initialized!\")\n",
    "print(\"To start chatting, run the next cell!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send your message to Granite\n",
    "\n",
    "message = interactive_chat()\n",
    "\n",
    "if message:\n",
    "    response = await chat_with_granite(message)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "118d2b9a2f754c97bb09201d53657bcd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_33a6a2361f3e49f6a7d93ffabfc587d7",
       "IPY_MODEL_40cf7af969994bd1b82549c5a6bd58db"
      ],
      "layout": "IPY_MODEL_3ffb6def52844c8c8aa079ad738da328"
     }
    },
    "2f759b5620ce4ea5beb42d934da81376": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": "1px solid gray",
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": "300px",
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": "auto",
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33a6a2361f3e49f6a7d93ffabfc587d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "TextModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "TextModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "TextView",
      "continuous_update": true,
      "description": "",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_f6342b101a5e4fb987c7924c49bdf97c",
      "placeholder": "Type a message...",
      "style": "IPY_MODEL_431c63a263f14a0e9132b135a2c0465f",
      "value": ""
     }
    },
    "3ffb6def52844c8c8aa079ad738da328": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40cf7af969994bd1b82549c5a6bd58db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "primary",
      "description": "Send",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_adfc081dfd8547b9865a1ed2b899d213",
      "style": "IPY_MODEL_69289e1cf7cd4ee1b2379a062e5795a8",
      "tooltip": ""
     }
    },
    "431c63a263f14a0e9132b135a2c0465f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "69289e1cf7cd4ee1b2379a062e5795a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "9f1a82bfd90b4918964b9c253b248153": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_2f759b5620ce4ea5beb42d934da81376",
      "msg_id": "",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "\u001b[1mYou:\u001b[0m Hello\n"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "\u001b[1mGranite:\u001b[0m [Granite not running – using mock reply]\n",
         "\n"
        ]
       }
      ]
     }
    },
    "adfc081dfd8547b9865a1ed2b899d213": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f6342b101a5e4fb987c7924c49bdf97c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "80%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
