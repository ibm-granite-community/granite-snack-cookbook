{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh8090CYsCPi"
      },
      "source": [
        "# Improving our multimodal RAG with Inference Scaling\n",
        "*Using IBM Granite, Docling, and Langchain*\n",
        "\n",
        "Welcome to this Granite recipe. In this recipe, you'll learn how to harness the power of inference scaling to improve the multimodal RAG pipeline created in a [previous recipe](https://github.com/ibm-granite-community/granite-snack-cookbook/blob/main/recipes/RAG/Granite_Multimodal_RAG.ipynb). This tutorial will guide you through the following processes:\n",
        "\n",
        "- **Document preprocessing:** Learn how to handle documents from various sources, parse and transform them into usable formats and store them in vector databases by using Docling. You will use a Granite MLLM to generate image descriptions of images in the documents.\n",
        "- **RAG:** Understand how to connect LLMs such as Granite with external knowledge bases to enhance query responses and generate valuable insights.\n",
        "- **Implementing DRAG and IterDRAG:** Apply the inference scaling techniques from the paper to significantly improve RAG performance when working with long context.\n",
        "- **LangChain for workflow integration:** Discover how to use LangChain to streamline and orchestrate document processing and retrieval workflows, enabling seamless interaction between different components of the system.\n",
        "\n",
        "And uses three cutting-edge technologies:\n",
        "\n",
        "1. **[Docling](https://docling-project.github.io/docling/):** An open-source toolkit used to parse and convert documents.\n",
        "2. **[Granite](https://www.ibm.com/granite/docs/models/granite/):** A state-of-the-art family of LLMs that provide robust natural language capabilities and a vision language model that provides image to text generation.\n",
        "3. **[LangChain](https://github.com/langchain-ai/langchain):** A powerful framework used to build applications powered by language models, designed to simplify complex workflows and integrate external tools seamlessly.\n",
        "\n",
        "By the end of this recipe, you will accomplish the following:\n",
        "- Gain proficiency in document preprocessing, chunking and image understanding.\n",
        "- Integrate vector databases to enhance retrieval capabilities.\n",
        "- Implement DRAG and IterDRAG to perform efficient and accurate data retrieval with inference scaling.\n",
        "- Experience firsthand how scaling inference compute can lead to almost linear improvements in RAG performance.\n",
        "\n",
        "This recipe is designed for AI developers, researchers and enthusiasts looking to enhance their knowledge of document management and advanced natural language processing (NLP) techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Inference Scaling Methods: DRAG and IterDRAG\n",
        "\n",
        "This notebook implements two advanced inference scaling techniques from the research paper [\"Inference Scaling for Long-Context Retrieval Augmented Generation\"](https://arxiv.org/html/2410.04343v1):\n",
        "\n",
        "1. **DRAG (Demonstration-based RAG)**: This method leverages in-context learning to improve RAG performance. By including multiple RAG examples as demonstrations, DRAG helps models learn to locate relevant information in long contexts. Unlike standard RAG which might plateau with more documents, DRAG shows linear improvements with increased context length.\n",
        "\n",
        "2. **IterDRAG (Iterative Demonstration-based RAG)**: An extension of DRAG that addresses complex multi-hop queries by decomposing them into simpler sub-queries. IterDRAG interleaves retrieval and generation steps, creating reasoning chains that bridge compositional gaps. This approach is particularly effective for handling complex queries across long contexts.\n",
        "\n",
        "These methods show that scaling inference computation can improve RAG performance almost linearly when optimally allocated, allowing RAG systems to make better use of long-context capabilities of modern LLMs.\n",
        "\n",
        "For this implementation, we'll use an IBM Granite model capable of processing different modalities. You will create an AI system to answer real-time user queries from unstructured data, applying the principles from the paper.\n",
        "\n",
        "[Retrieval-augmented generation (RAG)](https://www.ibm.com/think/topics/retrieval-augmented-generation) is a technique used with large language models (LLMs) to connect the model with a knowledge base of information outside the data the LLM has been trained on without having to perform [fine-tuning](https://www.ibm.com/think/topics/rag-vs-fine-tuning). Traditional RAG is limited to text-based use cases such as text summarization and chatbots.\n",
        "\n",
        "Multimodal RAG can use [multimodal](https://www.ibm.com/think/topics/multimodal-ai) LLMs (MLLM) to process information from multiple types of data to be included as part of the external knowledge base used in RAG. Multimodal data can include text, images, audio, video or other forms. In this recipe we use IBM's latest multimodal vision model, granite 3.2 vision.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Familiarity with Python programming.\n",
        "- Basic understanding of LLMs, NLP concepts and computer vision."
      ],
      "metadata": {
        "id": "FuAkjwhFKUlK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vooxv7ltEZBf"
      },
      "source": [
        "## Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRjU2pzWsCPj"
      },
      "source": [
        "- Familiarity with Python programming.\n",
        "- Basic understanding of LLMs, NLP concepts and computer vision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN2mK175_JRH",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Step 1: Setting up the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wfg8kTVr_JRH"
      },
      "source": [
        "Ensure you are running Python 3.10, 3.11 or 3.12 in a freshly created virtual environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEoM938B_JRH"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "assert sys.version_info >= (3, 10) and sys.version_info < (3, 13), \"Use Python 3.10, 3.11, or 3.12 to run this notebook.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p_2cX1-_JRI",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Step 2: Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfMWUUSs_JRI",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "! pip install \"git+https://github.com/ibm-granite-community/utils.git\" \\\n",
        "    transformers \\\n",
        "    pillow \\\n",
        "    langchain_community \\\n",
        "    langchain_huggingface \\\n",
        "    langchain_milvus \\\n",
        "    docling \\\n",
        "    replicate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gu-Oeay_JRJ"
      },
      "source": [
        "## Step 3: Selecting the AI models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65eDG9U3sCPk"
      },
      "source": [
        "### Logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsyITy60sCPk"
      },
      "source": [
        "To see some logging information, we can configure INFO log level.\n",
        "\n",
        "NOTE: It is okay to skip running this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zpBLqg_sCPk"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Piwy8BABsCPk"
      },
      "source": [
        "### Load the Granite models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFuZBhG-_JRJ"
      },
      "source": [
        "Specify the embeddings model to use for generating text embedding vectors. Here we will use one of the [Granite Embeddings models](https://huggingface.co/collections/ibm-granite/granite-embedding-models-6750b30c802c1926a35550bb)\n",
        "\n",
        "To use a different embeddings model, replace this code cell with one from [this Embeddings Model recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_Embeddings_Models.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvztNZly_JRJ"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "embeddings_model_path = \"ibm-granite/granite-embedding-30m-english\"\n",
        "embeddings_model = HuggingFaceEmbeddings(\n",
        "    model_name=embeddings_model_path,\n",
        ")\n",
        "embeddings_tokenizer = AutoTokenizer.from_pretrained(embeddings_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW-axzJAsCPk"
      },
      "source": [
        "Specify the MLLM to use for image understanding. We will use the Granite vision model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGhwKYh-sCPk"
      },
      "outputs": [],
      "source": [
        "from ibm_granite_community.notebook_utils import get_env_var\n",
        "from langchain_community.llms import Replicate\n",
        "from transformers import AutoProcessor\n",
        "\n",
        "vision_model_path = \"ibm-granite/granite-vision-3.2-2b\"\n",
        "vision_model = Replicate(\n",
        "    model=vision_model_path,\n",
        "    replicate_api_token=get_env_var(\"REPLICATE_API_TOKEN\"),\n",
        "    model_kwargs={\n",
        "        \"max_tokens\": embeddings_tokenizer.max_len_single_sentence, # Set the maximum number of tokens to generate as output.\n",
        "        \"min_tokens\": 100, # Set the minimum number of tokens to generate as output.\n",
        "        \"temperature\": 0.01,\n",
        "    },\n",
        ")\n",
        "vision_processor = AutoProcessor.from_pretrained(vision_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma8eWR10_JRJ"
      },
      "source": [
        "Specify the language model to use for the RAG generation operation. Here we use the Replicate LangChain client to connect to a Granite model from the [`ibm-granite`](https://replicate.com/ibm-granite) org on Replicate.\n",
        "\n",
        "To get set up with Replicate, see [Getting Started with Replicate](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started/Getting_Started_with_Replicate.ipynb).\n",
        "\n",
        "To connect to a model on a provider other than Replicate, substitute this code cell with one from the [LLM component recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_LLMs.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ckyj7Zrh_JRK"
      },
      "outputs": [],
      "source": [
        "model_path = \"ibm-granite/granite-3.3-8b-instruct\"\n",
        "model = Replicate(\n",
        "    model=model_path,\n",
        "    replicate_api_token=get_env_var(\"REPLICATE_API_TOKEN\"),\n",
        "    model_kwargs={\n",
        "        \"max_tokens\": 1000, # Set the maximum number of tokens to generate as output.\n",
        "        \"min_tokens\": 100, # Set the minimum number of tokens to generate as output.\n",
        "        \"temperature\": 0.01\n",
        "    },\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nviHG3n7_JRK",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Step 4: Preparing the documents for the vector database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ7Guu7A_JRK"
      },
      "source": [
        "In this example, from a set of source documents, we use [Docling](https://docling-project.github.io/docling/) to convert the documents into text and images. The text is then split into chunks. The images are processed by the MLLM to generate image summaries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuB8kkzf_JRK"
      },
      "source": [
        "### Use Docling to download the documents and convert to text and images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se6So2yw_JRK"
      },
      "source": [
        "Docling will download the PDF documents and process them so we can obtain the text and images the documents contain. In the PDF, there are various data types, including text, tables, graphs and images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNGz_0gZ_JRK"
      },
      "outputs": [],
      "source": [
        "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
        "from docling.datamodel.base_models import InputFormat\n",
        "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
        "\n",
        "pdf_pipeline_options = PdfPipelineOptions(\n",
        "    do_ocr=False,\n",
        "    generate_picture_images=True,\n",
        ")\n",
        "format_options = {\n",
        "    InputFormat.PDF: PdfFormatOption(pipeline_options=pdf_pipeline_options),\n",
        "}\n",
        "converter = DocumentConverter(format_options=format_options)\n",
        "\n",
        "sources = [\n",
        "    \"https://midwestfoodbank.org/images/AR_2020_WEB2.pdf\",\n",
        "]\n",
        "conversions = { source: converter.convert(source=source).document for source in sources }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVBYwCwVsCPk"
      },
      "source": [
        "With the documents processed, we then further process the text elements in the documents. We chunk them into appropriate sizes for the embeddings model we are using. A list of LangChain documents are created from the text chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ1cIA5DsCPk"
      },
      "outputs": [],
      "source": [
        "from docling_core.transforms.chunker.hybrid_chunker import HybridChunker\n",
        "from docling_core.types.doc.document import TableItem\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "doc_id = 0\n",
        "texts: list[Document] = []\n",
        "for source, docling_document in conversions.items():\n",
        "    for chunk in HybridChunker(tokenizer=embeddings_tokenizer).chunk(docling_document):\n",
        "        items = chunk.meta.doc_items\n",
        "        if len(items) == 1 and isinstance(items[0], TableItem):\n",
        "            continue # we will process tables later\n",
        "        refs = \" \".join(map(lambda item: item.get_ref().cref, items))\n",
        "        print(refs)\n",
        "        text = chunk.text\n",
        "        document = Document(\n",
        "            page_content=text,\n",
        "            metadata={\n",
        "                \"doc_id\": (doc_id:=doc_id+1),\n",
        "                \"source\": source,\n",
        "                \"ref\": refs,\n",
        "            },\n",
        "        )\n",
        "        texts.append(document)\n",
        "\n",
        "print(f\"{len(texts)} text document chunks created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wje3H-q6sCPl"
      },
      "source": [
        "Next we process any tables in the documents. We convert the table data to markdown format for passing into the language model. A list of LangChain documents are created from the table's markdown renderings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJsFQM1psCPl"
      },
      "outputs": [],
      "source": [
        "from docling_core.types.doc.labels import DocItemLabel\n",
        "\n",
        "doc_id = len(texts)\n",
        "tables: list[Document] = []\n",
        "for source, docling_document in conversions.items():\n",
        "    for table in docling_document.tables:\n",
        "        if table.label in [DocItemLabel.TABLE]:\n",
        "            ref = table.get_ref().cref\n",
        "            print(ref)\n",
        "            text = table.export_to_markdown()\n",
        "            document = Document(\n",
        "                page_content=text,\n",
        "                metadata={\n",
        "                    \"doc_id\": (doc_id:=doc_id+1),\n",
        "                    \"source\": source,\n",
        "                    \"ref\": ref\n",
        "                },\n",
        "            )\n",
        "            tables.append(document)\n",
        "\n",
        "\n",
        "print(f\"{len(tables)} table documents created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dz5utAL1sCPl"
      },
      "source": [
        "Finally we process any images in the documents. Here we use the vision language model to understand the content of an image. In this example, we are interested in any textual information in the image. You might want to experiment with different prompt text to see how it might improve the results.\n",
        "\n",
        "NOTE: Processing the images can take a very long time depending upon the number of images and the service running the vision language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82Z5blH1sCPl"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import io\n",
        "import PIL.Image\n",
        "import PIL.ImageOps\n",
        "\n",
        "def encode_image(image: PIL.Image.Image, format: str = \"png\") -> str:\n",
        "    image = PIL.ImageOps.exif_transpose(image) or image\n",
        "    image = image.convert(\"RGB\")\n",
        "\n",
        "    buffer = io.BytesIO()\n",
        "    image.save(buffer, format)\n",
        "    encoding = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
        "    uri = f\"data:image/{format};base64,{encoding}\"\n",
        "    return uri\n",
        "\n",
        "# Feel free to experiment with this prompt\n",
        "image_prompt = \"Give a detailed description of what is depicted in the image\"\n",
        "conversation = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": image_prompt},\n",
        "        ],\n",
        "    },\n",
        "]\n",
        "vision_prompt = vision_processor.apply_chat_template(\n",
        "    conversation=conversation,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "pictures: list[Document] = []\n",
        "doc_id = len(texts) + len(tables)\n",
        "for source, docling_document in conversions.items():\n",
        "    for picture in docling_document.pictures:\n",
        "        ref = picture.get_ref().cref\n",
        "        print(ref)\n",
        "        image = picture.get_image(docling_document)\n",
        "        if image:\n",
        "            text = vision_model.invoke(vision_prompt, image=encode_image(image))\n",
        "            document = Document(\n",
        "                page_content=text,\n",
        "                metadata={\n",
        "                    \"doc_id\": (doc_id:=doc_id+1),\n",
        "                    \"source\": source,\n",
        "                    \"ref\": ref,\n",
        "                },\n",
        "            )\n",
        "            pictures.append(document)\n",
        "\n",
        "print(f\"{len(pictures)} image descriptions created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kdyu_TDEsCPl"
      },
      "source": [
        "We can then display the LangChain documents created from the input documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htIYVVjHPKSX"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "from docling_core.types.doc.document import RefItem\n",
        "from IPython.display import display\n",
        "\n",
        "# Print all created documents\n",
        "for document in itertools.chain(texts, tables):\n",
        "    print(f\"Document ID: {document.metadata['doc_id']}\")\n",
        "    print(f\"Source: {document.metadata['source']}\")\n",
        "    print(f\"Content:\\n{document.page_content}\")\n",
        "    print(\"=\" * 80)  # Separator for clarity\n",
        "\n",
        "for document in pictures:\n",
        "    print(f\"Document ID: {document.metadata['doc_id']}\")\n",
        "    source = document.metadata['source']\n",
        "    print(f\"Source: {source}\")\n",
        "    print(f\"Content:\\n{document.page_content}\")\n",
        "    docling_document = conversions[source]\n",
        "    ref = document.metadata['ref']\n",
        "    picture = RefItem(cref=ref).resolve(docling_document)\n",
        "    image = picture.get_image(docling_document)\n",
        "    print(\"Image:\")\n",
        "    display(image)\n",
        "    print(\"=\" * 80)  # Separator for clarity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Bjz1IR3_JRK",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Populate the vector database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKT1ZZoysCPl"
      },
      "source": [
        "Using the embedding model, we load the documents from the text chunks and generated image captioning into a vector database. Creating this vector database allows us to easily conduct a semantic similarity search across our documents.\n",
        "\n",
        "NOTE: Population of the vector database can take some time depending on your embedding model and service."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNjFYpNrsCPl"
      },
      "source": [
        "### Choose your vector database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe7g4WoasCPl"
      },
      "source": [
        "Specify the database to use for storing and retrieving embedding vectors.\n",
        "\n",
        "To connect to a vector database other than Milvus, replace this code cell with one from [this Vector Store recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_Vector_Stores.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nq9W6M_sCPo"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "from langchain_core.vectorstores import VectorStore\n",
        "from langchain_milvus import Milvus\n",
        "\n",
        "db_file = tempfile.NamedTemporaryFile(prefix=\"vectorstore_\", suffix=\".db\", delete=False).name\n",
        "print(f\"The vector database will be saved to {db_file}\")\n",
        "\n",
        "vector_db: VectorStore = Milvus(\n",
        "    embedding_function=embeddings_model,\n",
        "    connection_args={\"uri\": db_file},\n",
        "    auto_id=True,\n",
        "    enable_dynamic_field=True,\n",
        "    index_params={\"index_type\": \"AUTOINDEX\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT28uYDIsCPo"
      },
      "source": [
        "We now add all the LangChain documents for the text, tables and image descriptions to the vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSbVb6R4_JRK"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "documents = list(itertools.chain(texts, tables, pictures))\n",
        "ids = vector_db.add_documents(documents)\n",
        "print(f\"{len(ids)} documents added to the vector database\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq50gMAO_JRK"
      },
      "source": [
        "## Step 5: RAG with Granite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZOYJW0D_JRL"
      },
      "source": [
        "Now that we have successfully converted our documents and vectorized them, we can set up out RAG pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UC_KSxFk_JRL"
      },
      "source": [
        "### Retrieve relevant chunks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf8T8eZk_JRL"
      },
      "source": [
        "Here we test the vector database by searching for chunks with relevant information to our query in the vector space. We display the documents associated with the retrieved image description.\n",
        "\n",
        "Feel free to try different queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMtTSHhQ_JRL"
      },
      "outputs": [],
      "source": [
        "query = \"Analyze how Midwest Food Bank's financial efficiency changed during the pandemic by comparing their 2019 and 2020 performance metrics. What specific pandemic adaptations had the greatest impact on their operational capacity, and how did their volunteer management strategy evolve to maintain service levels despite COVID-19 restrictions? Provide specific statistics from the report to support your analysis.\"\n",
        "for doc in vector_db.as_retriever(search_kwargs={\"k\": 10}).invoke(query):\n",
        "    print(doc)\n",
        "    print(\"=\" * 80)  # Separator for clarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viFYsnxTS2OC"
      },
      "source": [
        "The returned document should be responsive to the query. Let's go ahead and construct our RAG pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxVuFY_A_JRL"
      },
      "source": [
        "### Create the RAG pipeline for Granite\n",
        "\n",
        "First we create the prompts for Granite to perform the RAG query. We use the Granite chat template and supply the placeholder values that the LangChain RAG pipeline will replace.\n",
        "\n",
        "`{context}` will hold the retrieved chunks, as shown in the previous search, and feeds this to the model as document context for answering our question.\n",
        "\n",
        "Next, we construct the RAG pipeline by using the Granite prompt templates previously created.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PB-CPPTo_JRL"
      },
      "outputs": [],
      "source": [
        "from ibm_granite_community.notebook_utils import escape_f_string\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "# Create a Granite prompt for question-answering with the retrieved context\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    conversation=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"{input}\",\n",
        "    }],\n",
        "    documents=[{\n",
        "        \"doc_id\": \"0\",\n",
        "        \"text\": \"{context}\",\n",
        "    }],\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=False,\n",
        ")\n",
        "\n",
        "# The Granite prompt can contain JSON strings, so we must escape them\n",
        "prompt_template = PromptTemplate.from_template(template=escape_f_string(prompt, \"input\", \"context\"))\n",
        "\n",
        "# Create a Granite document prompt template to wrap each retrieved document\n",
        "document_prompt_template = PromptTemplate.from_template(template=\"\"\"\\\n",
        "<|end_of_text|>\n",
        "<|start_of_role|>document {{\"document_id\": \"{doc_id}\"}}<|end_of_role|>\n",
        "{page_content}\"\"\")\n",
        "document_separator=\"\"\n",
        "\n",
        "# Assemble the retrieval-augmented generation chain\n",
        "combine_docs_chain = create_stuff_documents_chain(\n",
        "    llm=model,\n",
        "    prompt=prompt_template,\n",
        "    document_prompt=document_prompt_template,\n",
        "    document_separator=document_separator,\n",
        ")\n",
        "rag_chain = create_retrieval_chain(\n",
        "    retriever=vector_db.as_retriever(search_kwargs={\"k\": 10}),\n",
        "    combine_docs_chain=combine_docs_chain,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_NU_Yhl_JRQ",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Generate a retrieval-augmented response to a question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXQzDqAB_JRQ",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "The pipeline uses the query to locate documents from the vector database and use them as context for the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hdo9PXsU_JRQ",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "outputs = rag_chain.invoke({\"input\": query})\n",
        "\n",
        "print(outputs['answer'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_uyUdCu_JRQ"
      },
      "source": [
        "Awesome! We have created an AI application that can successfully leverage knowledge from the source documents' text and images. That being said, while the answer is good it isn't great. Let's try some Inference Scaling RAG techniques that we can find in [this](https://arxiv.org/html/2410.04343v1) paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTaX1-SPT6rU"
      },
      "source": [
        "## Enhanced RAG with DRAG (Demonstration-based RAG)\n",
        "\n",
        "Now we'll implement the DRAG technique from the research paper \"Inference Scaling for Long-Context Retrieval Augmented Generation\" to enhance our RAG system.\n",
        "\n",
        "DRAG uses in-context examples to demonstrate to the model how to extract and use information from documents, improving performance for long-context scenarios.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Create sample in-context demonstrations\n",
        "These would typically come from a curated dataset of high-quality QA pairs\n",
        "For this example, we'll create some synthetic examples that match the expected domain"
      ],
      "metadata": {
        "id": "I8cm8_F1ErGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "def create_enhanced_drag_demonstrations(vector_db, num_examples=3, k_docs_per_example=5):\n",
        "    \"\"\"Create high-quality demonstrations for DRAG technique that showcase effective document analysis\"\"\"\n",
        "\n",
        "    example_queries = [\n",
        "        \"How did the COVID-19 pandemic impact Midwest Food Bank's operations in 2020?\",\n",
        "        \"What role did volunteers play at Midwest Food Bank during 2020, and how were they affected by the pandemic?\",\n",
        "        \"How did Midwest Food Bank's international programs perform during 2020, particularly in Haiti and East Africa?\"\n",
        "    ]\n",
        "\n",
        "    example_answers = [\n",
        "        \"The COVID-19 pandemic significantly impacted Midwest Food Bank's operations in 2020. Despite challenges, MFB remained open and responsive to increased needs. They implemented safety protocols, reduced volunteer numbers for social distancing, and altered their distribution model to allow partner agencies to receive food safely. The pandemic created unprecedented food insecurity, with many people seeking assistance for the first time. MFB distributed 37% more food than in 2019, with a record 179 semi-loads of Disaster Relief family food boxes sent nationwide. The organization also faced supply chain disruptions and food procurement challenges in the early months but continued to find and distribute food. Community, business, and donor support helped fund operations and food purchases. Additionally, MFB began participating in the USDA Farmers to Families Food Box program in May 2020, distributing over $52 million worth of nutritious produce, protein, and dairy products.\",\n",
        "\n",
        "        \"Volunteers were described as 'the life-blood of the organization' in the 2020 annual report. Despite the pandemic creating safety challenges, volunteers demonstrated courage and dedication by increasing their hours to meet growing needs. MFB implemented safety protocols at each location and limited volunteer group sizes to allow for social distancing. This created a challenge as food needs increased while fewer volunteers were available to help. To address this gap, multiple MFB locations received assistance from the National Guard, who filled vital volunteer positions driving trucks, operating forklifts, and helping with food distributions. In 2020, 17,930 individuals volunteered 300,898 hours of service, equivalent to 150 full-time employees. The volunteer-to-staff ratio was remarkable with 450 volunteers for every 1 paid MFB staff member, highlighting the volunteer-driven nature of the organization during the crisis.\",\n",
        "\n",
        "        \"In 2020, Midwest Food Bank's international operations in East Africa and Haiti faced unique challenges but continued to serve communities. In East Africa (operated as Kapu Africa), strict lockdowns led to mass hunger, especially in slum areas. Kapu Africa distributed 7.2 million Tender Mercies meals, working with partner ministries to share food in food-insecure slums. A notable outcome was a spiritual awakening among recipients, with many asking why they were receiving help. In Haiti, the pandemic added to existing challenges, closing airports, seaports, factories, and schools. MFB Haiti more than doubled its food shipments to Haiti, delivering over 160 tons of food relief, nearly three-quarters being Tender Mercies meals. As Haitian children primarily receive nourishment from school lunches, MFB Haiti distributed Tender Mercies through faith-based schools and also partnered with over 20 feeding centers serving approximately 1,100 children daily. Nearly 1 million Tender Mercies meals were distributed in Haiti during 2020.\"\n",
        "    ]\n",
        "\n",
        "    demonstrations = []\n",
        "    for i in range(min(num_examples, len(example_queries))):\n",
        "        # For each example, retrieve relevant documents\n",
        "        retrieved_docs = vector_db.as_retriever(search_kwargs={\"k\": k_docs_per_example}).invoke(example_queries[i])\n",
        "\n",
        "        # Create a demonstration example\n",
        "        demonstration = {\n",
        "            \"query\": example_queries[i],\n",
        "            \"documents\": retrieved_docs,\n",
        "            \"answer\": example_answers[i]\n",
        "        }\n",
        "        demonstrations.append(demonstration)\n",
        "\n",
        "    return demonstrations"
      ],
      "metadata": {
        "id": "HIBeRyrts9R4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Format the demonstrations for inclusion in the prompt\n"
      ],
      "metadata": {
        "id": "jSFxp2aAEmax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_demonstration(demo, document_prompt_template, document_separator):\n",
        "    \"\"\"Format a single demonstration for inclusion in the prompt\"\"\"\n",
        "    # Format documents for this demonstration\n",
        "    formatted_docs = []\n",
        "    for i, doc in enumerate(demo[\"documents\"]):\n",
        "        # Apply document prompt template to each document\n",
        "        formatted_doc = document_prompt_template.format(\n",
        "            doc_id=f\"example-{i+1}\",\n",
        "            page_content=doc.page_content\n",
        "        )\n",
        "        formatted_docs.append(formatted_doc)\n",
        "\n",
        "    # Join documents with separator\n",
        "    context = document_separator.join(formatted_docs)\n",
        "\n",
        "    # Format the full demonstration\n",
        "    return f\"\"\"\n",
        "Example:\n",
        "Question: {demo[\"query\"]}\n",
        "Context:\n",
        "{context}\n",
        "Answer: {demo[\"answer\"]}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rAxk4TAMpgLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Create demonstrations for DRAG\n"
      ],
      "metadata": {
        "id": "qjdO1fZnEkhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_demonstrations = 3  # You can adjust this value\n",
        "docs_per_demonstration = 5  # Number of documents per demonstration example\n",
        "demonstrations = create_enhanced_drag_demonstrations(vector_db, num_demonstrations, docs_per_demonstration)\n",
        "\n",
        "# Format all demonstrations together\n",
        "formatted_demonstrations = \"\\n\\n\".join([\n",
        "    format_demonstration(demo, document_prompt_template, document_separator)\n",
        "    for demo in demonstrations\n",
        "])"
      ],
      "metadata": {
        "id": "-hOKJVJwpl2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we create the prompt template\n",
        "\n"
      ],
      "metadata": {
        "id": "6VjaYCV8EhCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drag_prompt = tokenizer.apply_chat_template(\n",
        "    conversation=[{\n",
        "        \"role\": \"system\",\n",
        "        \"content\": f\"\"\"You are Granite, developed by IBM. Write the response to the user's input by strictly aligning with the facts in the provided documents.\n",
        "        If the information needed to answer the question is not available in the documents, inform the user that the question cannot be answered based on the available data.\n",
        "        Here are examples of effectively extracting information from documents to answer questions:\n",
        "\n",
        "        {formatted_demonstrations}\n",
        "\n",
        "        Follow these examples when answering the user's question.\"\"\"\n",
        "    }, {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"{input}\",\n",
        "    }],\n",
        "    documents=[{\n",
        "        \"doc_id\": \"0\",\n",
        "        \"text\": \"{context}\",\n",
        "    }],\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=False,\n",
        ")\n",
        "\n",
        "# The Granite prompt can contain JSON strings, so we must escape them\n",
        "drag_prompt_template = PromptTemplate.from_template(template=escape_f_string(drag_prompt, \"input\", \"context\"))"
      ],
      "metadata": {
        "id": "iCHKDMvhp1IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Create a custom retriever that reorders documents (higher ranked closer to query)\n"
      ],
      "metadata": {
        "id": "UG7ZDOboFUVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReorderingRetriever:\n",
        "    def __init__(self, base_retriever):\n",
        "        self.base_retriever = base_retriever\n",
        "\n",
        "    def invoke(self, query):\n",
        "        docs = self.base_retriever.invoke(query)\n",
        "        return list(reversed(docs))  # Reverse the order so higher-ranked docs are closer to query\n",
        "\n",
        "# Create the base retriever with increased document count\n",
        "base_retriever = vector_db.as_retriever(search_kwargs={\"k\": 10})  # Retrieve more documents\n",
        "reordering_retriever = ReorderingRetriever(base_retriever)"
      ],
      "metadata": {
        "id": "cKCktZX6abVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Create DRAG pipeline\n"
      ],
      "metadata": {
        "id": "ogy765ACFZev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drag_combine_docs_chain = create_stuff_documents_chain(\n",
        "    llm=model,\n",
        "    prompt=drag_prompt_template,\n",
        "    document_prompt=document_prompt_template,\n",
        "    document_separator=document_separator,\n",
        ")\n",
        "\n",
        "drag_chain = create_retrieval_chain(\n",
        "    retriever=vector_db.as_retriever(search_kwargs={\"k\": 10}),\n",
        "    combine_docs_chain=drag_combine_docs_chain,\n",
        ")"
      ],
      "metadata": {
        "id": "__deMUohp6bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Generate a DRAG-enhanced response to a question\n"
      ],
      "metadata": {
        "id": "MU0Gb-ZOFdPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drag_outputs = drag_chain.invoke({\"input\": query})\n",
        "print(\"\\n=== DRAG-Enhanced Answer ===\")\n",
        "print(drag_outputs['answer'])"
      ],
      "metadata": {
        "id": "7yg43AXImf4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, looks like we got some improvements in the answer by giving it some examples. Let's try an even more thorough RAG technique next!"
      ],
      "metadata": {
        "id": "YImI-AF7rLGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing IterDRAG (Iterative Demonstration-based RAG)\n",
        "\n",
        "IterDRAG extends DRAG by decomposing complex queries into simpler sub-queries and performing interleaved retrieval. This is particularly effective for complex multi-hop questions.\n"
      ],
      "metadata": {
        "id": "s23R6wdBzt64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "import re"
      ],
      "metadata": {
        "id": "SBkHsH7vG-ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Create a query decomposition chain"
      ],
      "metadata": {
        "id": "N4lVxi1FHByl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decompose_prompt = tokenizer.apply_chat_template(\n",
        "    conversation=[{\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"You are a helpful assistant that breaks down complex questions into simpler sub-questions.\n",
        "        For multi-part or complex questions, generate 1-3 sub-questions that would help answer the main question.\"\"\"\n",
        "    }, {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"Please break down the following question into simpler sub-questions. Format your response as:\n",
        "        Follow up: [sub-question]\n",
        "\n",
        "        If the question is simple enough already, just respond with \"No follow-up needed.\"\n",
        "\n",
        "        Question: {question}\"\"\"\n",
        "    }],\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=False,\n",
        ")\n",
        "\n",
        "decompose_prompt_template = PromptTemplate.from_template(template=decompose_prompt)\n",
        "decompose_chain = LLMChain(llm=model, prompt=decompose_prompt_template)"
      ],
      "metadata": {
        "id": "lwFSrNakHE8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Create a sub-query answering chain"
      ],
      "metadata": {
        "id": "Ry9BUgWVHPBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub_answer_prompt = tokenizer.apply_chat_template(\n",
        "    conversation=[{\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"You are a helpful assistant that answers specific questions based on the provided context.\n",
        "        Focus only on the sub-question and provide a concise intermediate answer.\"\"\"\n",
        "    }, {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"Please answer the following sub-question based on the provided context.\n",
        "        Format your response as:\n",
        "        Intermediate answer: [your concise answer to the sub-question]\n",
        "\n",
        "        Sub-question: {sub_question}\n",
        "\n",
        "        Context:\n",
        "        {context}\"\"\"\n",
        "    }],\n",
        "    documents=[{\n",
        "        \"Context\": \"{context}\",\n",
        "    }],\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=False,\n",
        ")\n",
        "\n",
        "sub_answer_prompt_template = PromptTemplate.from_template(template=sub_answer_prompt)\n",
        "sub_answer_chain = create_stuff_documents_chain(\n",
        "    llm=model,\n",
        "    prompt=sub_answer_prompt_template,\n",
        "    document_prompt=document_prompt_template,\n",
        "    document_separator=document_separator,\n",
        ")"
      ],
      "metadata": {
        "id": "UeqL0ZjRHSKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Create a final answer generation chain"
      ],
      "metadata": {
        "id": "WR2BWbyNHVvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_answer_prompt = tokenizer.apply_chat_template(\n",
        "    conversation=[{\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"You are a helpful assistant that provides comprehensive answers to questions.\n",
        "        Use the intermediate answers to sub-questions to formulate a complete final answer.\"\"\"\n",
        "    }, {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"Please provide a final answer to the main question based on the intermediate answers to sub-questions.\n",
        "        Format your response as:\n",
        "        So the final answer is: [your comprehensive answer to the main question]\n",
        "\n",
        "        Main question: {main_question}\n",
        "\n",
        "        Sub-questions and intermediate answers:\n",
        "        {sub_qa_pairs}\"\"\"\n",
        "    }],\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=False,\n",
        ")\n",
        "\n",
        "final_answer_prompt_template = PromptTemplate.from_template(template=final_answer_prompt)\n",
        "final_answer_chain = LLMChain(llm=model, prompt=final_answer_prompt_template)"
      ],
      "metadata": {
        "id": "_ZbRz6MHHajC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Create example demonstrations for IterDRAG"
      ],
      "metadata": {
        "id": "2ZeEDURDHdi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_iterdrag_demonstrations(num_examples=2):\n",
        "    \"\"\"Create examples showing how to decompose and answer complex questions\"\"\"\n",
        "\n",
        "    demonstrations = [\n",
        "        {\n",
        "            \"main_question\": \"What impact did the pandemic have on the food bank's operations and distribution?\",\n",
        "            \"sub_questions\": [\n",
        "                \"How did food distribution volume change during the pandemic?\",\n",
        "                \"What operational challenges did the food bank face during the pandemic?\",\n",
        "                \"What new programs were implemented in response to the pandemic?\"\n",
        "            ],\n",
        "            \"intermediate_answers\": [\n",
        "                \"Food distribution volume increased by 60% during the pandemic, rising from approximately 62 million pounds in 2019 to over 100 million pounds in 2020.\",\n",
        "                \"The food bank faced challenges including supply chain disruptions, volunteer shortages due to social distancing requirements, and the need to implement new safety protocols for food handling and distribution.\",\n",
        "                \"New programs included contactless distribution methods, expanded mobile pantry operations, emergency food boxes for vulnerable populations, and virtual nutrition education classes.\"\n",
        "            ],\n",
        "            \"final_answer\": \"The pandemic had a profound impact on food bank operations and distribution. Distribution volume increased by 60% to over 100 million pounds of food in 2020. Operationally, the food bank faced supply chain disruptions, volunteer shortages, and safety protocol challenges. In response, they implemented contactless distribution, expanded mobile pantries, created emergency food boxes for vulnerable populations, and developed virtual nutrition education. Despite these challenges, they successfully scaled operations to meet the unprecedented community need during the crisis.\"\n",
        "        },\n",
        "        {\n",
        "            \"main_question\": \"How does the food bank's financial management compare to industry standards for non-profits?\",\n",
        "            \"sub_questions\": [\n",
        "                \"What percentage of the food bank's budget goes to program services versus administrative costs?\",\n",
        "                \"What are the industry standards for program spending versus overhead for food banks?\"\n",
        "            ],\n",
        "            \"intermediate_answers\": [\n",
        "                \"94% of the food bank's budget goes directly to program services, with only 6% allocated to administrative and fundraising costs.\",\n",
        "                \"Industry standards suggest that well-run food banks typically allocate 85-90% of their budget to program services, with 10-15% for administrative and fundraising expenses.\"\n",
        "            ],\n",
        "            \"final_answer\": \"The food bank demonstrates excellent financial management compared to industry standards. With 94% of its budget allocated to program services and only 6% to administrative and fundraising costs, it exceeds the industry benchmark of 85-90% for program spending. This financial efficiency places the food bank among the top-performing non-profits in terms of maximizing donor impact and minimizing overhead expenses.\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Format demonstrations for the prompt\n",
        "    formatted_demos = \"\"\n",
        "    for demo in demonstrations[:num_examples]:\n",
        "        formatted_demo = f\"\"\"\n",
        "        Example:\n",
        "        Main question: {demo['main_question']}\n",
        "\n",
        "        Step 1: Decompose into sub-questions\n",
        "        \"\"\"\n",
        "        for i, sq in enumerate(demo['sub_questions']):\n",
        "            formatted_demo += f\"Follow up: {sq}\\n\"\n",
        "\n",
        "        formatted_demo += \"\\nStep 2: Answer each sub-question\\n\"\n",
        "        for i, (sq, ans) in enumerate(zip(demo['sub_questions'], demo['intermediate_answers'])):\n",
        "            formatted_demo += f\"Sub-question: {sq}\\nIntermediate answer: {ans}\\n\\n\"\n",
        "\n",
        "        formatted_demo += f\"Step 3: Generate final answer\\nSo the final answer is: {demo['final_answer']}\"\n",
        "        formatted_demos += formatted_demo + \"\\n\\n\"\n",
        "\n",
        "    return formatted_demos\n",
        "\n",
        "# Add demonstrations to decompose prompt\n",
        "iterdrag_demonstrations = create_iterdrag_demonstrations()\n",
        "decompose_prompt_with_demos = tokenizer.apply_chat_template(\n",
        "    conversation=[{\n",
        "        \"role\": \"system\",\n",
        "        \"content\": f\"\"\"\n",
        "\n",
        "        You are a helpful assistant that breaks down complex questions into simpler sub-questions.\n",
        "        For multi-part or complex questions, generate 1-3 sub-questions that would help answer the main question.\n",
        "\n",
        "        Here are examples of how to decompose complex questions:\n",
        "        {iterdrag_demonstrations}\n",
        "\n",
        "        Follow these examples when breaking down the user's question.\"\"\"\n",
        "    }, {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"\n",
        "\n",
        "        Please break down the following question into simpler sub-questions. Format your response as:\n",
        "        Follow up: [sub-question]\n",
        "\n",
        "        If the question is simple enough already, just respond with \"No follow-up needed.\"\n",
        "\n",
        "        Question: {{question}}\"\"\"\n",
        "    }],\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=False,\n",
        ")\n",
        "\n",
        "decompose_prompt_template = PromptTemplate.from_template(template=decompose_prompt_with_demos)\n",
        "decompose_chain = LLMChain(llm=model, prompt=decompose_prompt_template)"
      ],
      "metadata": {
        "id": "f7TGXp43HhT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Implement the IterDRAG function"
      ],
      "metadata": {
        "id": "PDnvF2tkHrEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def iterative_drag(query, vector_db, max_iterations=3):\n",
        "    \"\"\"\n",
        "    Implements IterDRAG: decomposing queries, retrieving documents for sub-queries,\n",
        "    and generating a final answer based on intermediate answers.\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== Processing query with IterDRAG: '{query}' ===\")\n",
        "\n",
        "    # Store the main question\n",
        "    main_question = query\n",
        "    #Decompose the main question into sub-questions\n",
        "    print(\"Step 1: Decomposing the query into sub-questions...\")\n",
        "    decompose_result = decompose_chain.invoke({\"question\": main_question})\n",
        "    decompose_output = decompose_result[\"text\"]\n",
        "\n",
        "    # Check if no decomposition is needed\n",
        "    if \"No follow-up needed\" in decompose_output:\n",
        "        print(\"No decomposition needed. Using standard DRAG approach.\")\n",
        "        return drag_chain.invoke({\"input\": main_question})\n",
        "\n",
        "    # Extract sub-questions using regex\n",
        "    sub_questions = re.findall(r\"Follow up: (.*?)(?=Follow up:|$)\", decompose_output, re.DOTALL)\n",
        "    sub_questions = [sq.strip() for sq in sub_questions if sq.strip()]\n",
        "\n",
        "    if not sub_questions:\n",
        "        print(\"Could not extract sub-questions. Using standard DRAG approach.\")\n",
        "        return drag_chain.invoke({\"input\": main_question})\n",
        "\n",
        "    print(f\"Decomposed into {len(sub_questions)} sub-questions:\")\n",
        "    for i, sq in enumerate(sub_questions):\n",
        "        print(f\"  {i+1}. {sq}\")\n",
        "\n",
        "    # Answer each sub-question\n",
        "    sub_qa_pairs = []\n",
        "    for i, sub_question in enumerate(sub_questions[:max_iterations]):\n",
        "        print(f\"\\nStep 2.{i+1}: Processing sub-question: {sub_question}\")\n",
        "\n",
        "        # Retrieve documents relevant to this sub-question\n",
        "        retrieved_docs = reordering_retriever.invoke(sub_question)\n",
        "        print(f\"Retrieved {len(retrieved_docs)} documents for this sub-question\")\n",
        "\n",
        "        # Generate answer for this sub-question\n",
        "        sub_answer_result = sub_answer_chain.invoke({\n",
        "            \"sub_question\": sub_question,\n",
        "            \"context\": retrieved_docs\n",
        "        })\n",
        "\n",
        "        # Extract intermediate answer using regex\n",
        "        intermediate_answer_match = re.search(r\"Intermediate answer: (.*?)$\", sub_answer_result, re.DOTALL)\n",
        "        if intermediate_answer_match:\n",
        "            intermediate_answer = intermediate_answer_match.group(1).strip()\n",
        "        else:\n",
        "            intermediate_answer = sub_answer_result\n",
        "\n",
        "        print(f\"Generated intermediate answer: {intermediate_answer[:100]}...\")\n",
        "\n",
        "        # Store the sub-question and its answer\n",
        "        sub_qa_pairs.append({\n",
        "            \"sub_question\": sub_question,\n",
        "            \"intermediate_answer\": intermediate_answer\n",
        "        })\n",
        "\n",
        "    # Generate the final answer based on sub-question answers\n",
        "    print(\"\\nStep 3: Generating final answer based on intermediate answers...\")\n",
        "    sub_qa_formatted = \"\\n\\n\".join([\n",
        "        f\"Sub-question: {pair['sub_question']}\\nIntermediate answer: {pair['intermediate_answer']}\"\n",
        "        for pair in sub_qa_pairs\n",
        "    ])\n",
        "\n",
        "    final_answer_result = final_answer_chain.invoke({\n",
        "        \"main_question\": main_question,\n",
        "        \"sub_qa_pairs\": sub_qa_formatted\n",
        "    })\n",
        "\n",
        "    # Extract final answer\n",
        "    final_answer_match = re.search(r\"So the final answer is: (.*?)$\", final_answer_result[\"text\"], re.DOTALL)\n",
        "    if final_answer_match:\n",
        "        final_answer = final_answer_match.group(1).strip()\n",
        "    else:\n",
        "        final_answer = final_answer_result[\"text\"]\n",
        "\n",
        "    return {\"answer\": final_answer, \"sub_qa_pairs\": sub_qa_pairs}\n"
      ],
      "metadata": {
        "id": "yQYyEg8yzuuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing RAG Approaches\n",
        "\n",
        "Now that we have all three RAG approaches set up, let's compare them using the same query, this time much more complex, to see the differences."
      ],
      "metadata": {
        "id": "s1HbAsV52bdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run all approaches on the same complex query\n",
        "comparison_query = \"What was the full impact chain of the National Guard's assistance during the pandemic? Specifically, how did their involvement affect volunteer operations, what specific tasks did they perform, and how did this ultimately translate to community impact in terms of food distribution capabilities and reach?\"\n",
        "\n",
        "print(\"\\n=== Standard RAG ===\")\n",
        "standard_result = rag_chain.invoke({\"input\": comparison_query})\n",
        "print(standard_result[\"answer\"])\n",
        "\n",
        "print(\"\\n=== DRAG ===\")\n",
        "drag_result = drag_chain.invoke({\"input\": comparison_query})\n",
        "print(drag_result[\"answer\"])\n",
        "\n",
        "print(\"\\n=== IterDRAG ===\")\n",
        "iterdrag_result = iterative_drag(comparison_query, vector_db)\n",
        "print(iterdrag_result[\"answer\"])"
      ],
      "metadata": {
        "id": "-FFg5Kf80FLZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}