{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microsoft Semantic Kernel With IBM Granite 4 Micro\n",
    "\n",
    "> **‚ö†Ô∏è Important Note**: This notebook is **not compatible with Google Colab** as it requires a local Ollama installation running a Granite model. Please run this notebook in a local environment with Ollama properly configured and running on port 11434.\n",
    "\n",
    "## What is Microsoft Semantic Kernel?\n",
    "\n",
    "**Microsoft Semantic Kernel (SK)** is an open-source SDK that allows developers to integrate AI capabilities into their applications through a unified framework. It acts as a middleware layer that orchestrates AI models, plugins, and traditional programming logic.\n",
    "\n",
    "## Key Features & Differentiators\n",
    "\n",
    "### üîå **Plugin Architecture**\n",
    "- **Semantic Kernel**: Uses a sophisticated plugin system where AI functions can be chained together with traditional code\n",
    "- **Other Frameworks**: Often focus on single model interactions or require custom integration work\n",
    "\n",
    "### üß† **AI Orchestration** \n",
    "- **Semantic Kernel**: Built-in planning capabilities that can automatically sequence multiple AI operations\n",
    "- **LangChain/LlamaIndex**: Primarily focused on chaining operations manually\n",
    "- **OpenAI API**: Direct model access without orchestration features\n",
    "\n",
    "### üîÑ **Multi-Model Support**\n",
    "- **Semantic Kernel**: Vendor-agnostic - works with OpenAI, Azure OpenAI, Hugging Face, Ollama, and more\n",
    "- **Other Frameworks**: Often tied to specific providers or require extensive configuration\n",
    "\n",
    "### üéØ **Enterprise-Ready**\n",
    "- **Semantic Kernel**: Built with enterprise needs in mind - security, scalability, and integration with Microsoft ecosystem\n",
    "- **Other Frameworks**: May require additional tooling for enterprise deployment\n",
    "\n",
    "## About Granite 4 Micro\n",
    "\n",
    "**IBM Granite 4 Micro** model highlights include:\n",
    "\n",
    "- **Enhanced Architecture**: New dense architecture trained with 12 trillion tokens across 12 languages and 116 programming languages\n",
    "- **128K Context Length**: Extended context for complex tasks\n",
    "- **Strong RAG Performance**: Excellent retrieval-augmented generation capabilities\n",
    "- **Function Calling**: Native support for tool/function calling\n",
    "- **Response Controls**: Built-in length and originality controls\n",
    "- **Apache 2.0 License**: Fully open-source\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "\n",
    "1. **Ollama installed locally**: Download and install from [https://ollama.ai](https://ollama.ai)\n",
    "2. **Granite 4 Micro model downloaded**: Run `ollama pull ibm/granite4:micro` in your terminal\n",
    "3. **Ollama running**: Start Ollama service (usually `ollama serve` or it starts automatically)\n",
    "4. **Python 3.11, or 3.12**: This notebook requires a recent Python version\n",
    "\n",
    "## This Demo\n",
    "\n",
    "This notebook demonstrates a simple chatbot using Semantic Kernel with:\n",
    "- **Ollama** as the local AI provider\n",
    "- **Granite 4 Micro** model for responses  \n",
    "- **Granite prompt template** following official guidelines\n",
    "- **Async execution** for responsive interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Install granite utils\n",
    "! echo \"::group::Install Dependencies\"\n",
    "%pip install uv\n",
    "! uv pip install \"git+https://github.com/ibm-granite-community/utils.git\"\n",
    "\n",
    "# Install required packages\n",
    "! uv pip install semantic-kernel requests ipywidgets ollama\n",
    "\n",
    "! echo \"::endgroup::\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup Kernel and Granite Model for Chatbot\n",
    "import requests\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.prompt_template import PromptTemplateConfig\n",
    "from semantic_kernel.connectors.ai.ollama import OllamaChatCompletion\n",
    "from semantic_kernel.connectors.ai.ollama.ollama_prompt_execution_settings import OllamaChatPromptExecutionSettings\n",
    "\n",
    "# Initialize kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "# Configure execution settings\n",
    "execution_settings = OllamaChatPromptExecutionSettings()\n",
    "\n",
    "# Validate Ollama is running and has the Granite model\n",
    "ollama_host = \"http://localhost:11434\"\n",
    "granite_model = \"ibm/granite4:micro\"\n",
    "\n",
    "try:\n",
    "    # Check if Ollama is running\n",
    "    response = requests.get(f\"{ollama_host}/api/tags\", timeout=5)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Check if Granite model is available\n",
    "    models = response.json()\n",
    "    available_models = [model['name'] for model in models['models']]\n",
    "\n",
    "    if granite_model not in available_models:\n",
    "        print(f\"‚ùå Granite model '{granite_model}' not found in Ollama.\")\n",
    "        print(f\"Available models: {available_models}\")\n",
    "        print(f\"Please run: ollama pull {granite_model}\")\n",
    "        raise ValueError(f\"Required model {granite_model} not available\")\n",
    "\n",
    "    print(f\"‚úÖ Ollama is running and {granite_model} model is available\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"‚ùå Cannot connect to Ollama at {ollama_host}\")\n",
    "    print(\"Please ensure Ollama is installed and running:\")\n",
    "    print(\"1. Install Ollama from https://ollama.ai\")\n",
    "    print(\"2. Run 'ollama serve' in terminal\")\n",
    "    print(f\"3. Run 'ollama pull {granite_model}' to download the model\")\n",
    "    raise ConnectionError(f\"Ollama connection failed: {e}\")\n",
    "\n",
    "# Initialize Semantic Kernel with Ollama service\n",
    "service_id = \"ollama\"\n",
    "kernel.add_service(\n",
    "    OllamaChatCompletion(\n",
    "        service_id=service_id,\n",
    "        host=ollama_host,\n",
    "        ai_model_id=granite_model,\n",
    "    )\n",
    ")\n",
    "print(\"‚úÖ Semantic Kernel initialized successfully with Granite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define Chat Template using a simple approach that works with Semantic Kernel\n",
    "\n",
    "# Create a simple jinja2 template string that works with Semantic Kernel\n",
    "jinja2_template_string = \"\"\"{{ input }}\"\"\"\n",
    "\n",
    "# Create the prompt template configuration using jinja2 format\n",
    "prompt_config = PromptTemplateConfig(\n",
    "    template=jinja2_template_string,\n",
    "    name=\"chat\",\n",
    "    template_format=\"jinja2\"\n",
    ")\n",
    "\n",
    "# Create the chat function using the prompt configuration\n",
    "try:\n",
    "    chat_function = kernel.add_function(\n",
    "        function_name=\"chat_function\",\n",
    "        plugin_name=\"chat_plugin\",\n",
    "        prompt_template_config=prompt_config,\n",
    "        prompt_execution_settings=execution_settings,\n",
    "    )\n",
    "    print(\"‚úÖ Chat function configured successfully\")\n",
    "    print(\"‚úÖ Using simple input passthrough - Ollama will handle chat formatting\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Function creation failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define chat function to interact with Granite\n",
    "import textwrap\n",
    "import time\n",
    "\n",
    "async def chat_with_granite(user_message):\n",
    "    \"\"\"\n",
    "    Chat function that processes a single user input and returns the response.\n",
    "    Ollama will handle the chat formatting automatically.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if chat_function is None:\n",
    "            raise RuntimeError(\"Function not initialized - check previous cells for errors\")\n",
    "\n",
    "        print(f\"ü§î Thinking... (User: {user_message[:50]}{'...' if len(user_message) > 50 else ''})\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Pass the user message directly - Ollama will handle chat formatting\n",
    "        result = await kernel.invoke(chat_function, input=user_message)\n",
    "\n",
    "        end_time = time.time()\n",
    "        response_time = end_time - start_time\n",
    "\n",
    "        # Get the response text\n",
    "        response_text = str(result)\n",
    "\n",
    "        # Format the output with line wrapping for display\n",
    "        wrapped_text = textwrap.fill(response_text, width=80, break_long_words=False, break_on_hyphens=False)\n",
    "\n",
    "        print(f\"\\nü§ñ Assistant (responded in {response_time:.2f}s):\")\n",
    "        print(\"=\" * 50)\n",
    "        print(wrapped_text)\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        return response_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error occurred: {e}\")\n",
    "        print(\"\\nüîß Troubleshooting:\")\n",
    "        print(\"1. Ensure Ollama is running: ollama serve\")\n",
    "        print(\"2. Ensure Granite model is available: ollama pull ibm/granite4:micro\")\n",
    "        print(\"3. Check if Ollama is accessible at http://localhost:11434\")\n",
    "        print(\"4. Try restarting the notebook kernel\")\n",
    "        raise\n",
    "\n",
    "def interactive_chat():\n",
    "    \"\"\"\n",
    "    Interactive chat mode - prompts user for input via terminal and returns response.\n",
    "    \"\"\"\n",
    "    print(\"üí¨ Chat with Granite 4 Micro via Semantic Kernel!\")\n",
    "    print(\"Example prompts:\")\n",
    "    print(\"- 'What are the benefits of using Semantic Kernel?'\")\n",
    "    print(\"- 'Explain quantum computing in simple terms'\")\n",
    "    print(\"- 'Write a Python function to calculate fibonacci numbers'\")\n",
    "    print()\n",
    "\n",
    "    user_input = input(\"Enter your message: \")\n",
    "\n",
    "    if user_input and user_input.strip():\n",
    "        return user_input\n",
    "    else:\n",
    "        print(\"‚ùå Please enter a message to continue\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Chat functions initialized!\")\n",
    "print(\"To start chatting, run the next cell!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Send your message to Granite\n",
    "message = interactive_chat()\n",
    "\n",
    "if message:\n",
    "    response = await chat_with_granite(message)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
